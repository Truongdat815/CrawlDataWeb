import time
import json
import os
import re
import sys
from concurrent.futures import ThreadPoolExecutor, as_completed
from playwright.sync_api import sync_playwright
from src import config, utils

# Import MongoDB
try:
    from pymongo import MongoClient
    MONGODB_AVAILABLE = True
except ImportError:
    MONGODB_AVAILABLE = False

# Helper function Ä‘á»ƒ print an toÃ n vá»›i encoding UTF-8
def safe_print(*args, **kwargs):
    """Print function an toÃ n vá»›i encoding UTF-8 trÃªn Windows"""
    try:
        # Thá»­ print bÃ¬nh thÆ°á»ng
        print(*args, **kwargs)
    except UnicodeEncodeError:
        # Náº¿u lá»—i encoding, encode láº¡i thÃ nh ASCII-safe
        message = ' '.join(str(arg) for arg in args)
        # Thay tháº¿ emoji vÃ  kÃ½ tá»± Ä‘áº·c biá»‡t
        message = message.encode('ascii', 'replace').decode('ascii')
        print(message, **kwargs)

class RoyalRoadScraper:
    def __init__(self, max_workers=None):
        self.browser = None
        self.context = None
        self.page = None
        self.playwright = None
        self.max_workers = max_workers or config.MAX_WORKERS
        
        # Khá»Ÿi táº¡o MongoDB client náº¿u Ä‘Æ°á»£c báº­t
        self.mongo_client = None
        self.mongo_db = None
        # Khá»Ÿi táº¡o cÃ¡c collections riÃªng biá»‡t
        self.mongo_collections = {}
        if config.MONGODB_ENABLED and MONGODB_AVAILABLE:
            try:
                self.mongo_client = MongoClient(config.MONGODB_URI)
                self.mongo_db = self.mongo_client[config.MONGODB_DB_NAME]
                # Khá»Ÿi táº¡o táº¥t cáº£ cÃ¡c collections
                self.mongo_collections = {
                    "stories": self.mongo_db[config.MONGODB_COLLECTION_STORIES],
                    "chapters": self.mongo_db[config.MONGODB_COLLECTION_CHAPTERS],
                    "comments": self.mongo_db[config.MONGODB_COLLECTION_COMMENTS],
                    "reviews": self.mongo_db[config.MONGODB_COLLECTION_REVIEWS],
                    "scores": self.mongo_db[config.MONGODB_COLLECTION_SCORES],
                    "users": self.mongo_db[config.MONGODB_COLLECTION_USERS],
                }
                # Giá»¯ láº¡i collection cÅ© Ä‘á»ƒ tÆ°Æ¡ng thÃ­ch
                self.mongo_collection = self.mongo_db[config.MONGODB_COLLECTION_FICTIONS]
                safe_print("âœ… ÄÃ£ káº¿t ná»‘i MongoDB vá»›i cÃ¡c collections: stories, chapters, comments, reviews, scores, users")
            except Exception as e:
                safe_print(f"âš ï¸ KhÃ´ng thá»ƒ káº¿t ná»‘i MongoDB: {e}")
                safe_print("   Tiáº¿p tá»¥c lÆ°u vÃ o file JSON...")
                self.mongo_client = None

    def start(self):
        """Khá»Ÿi Ä‘á»™ng trÃ¬nh duyá»‡t"""
        self.playwright = sync_playwright().start()
        self.browser = self.playwright.chromium.launch(headless=config.HEADLESS)
        # ThÃªm user agent vÃ  viewport Ä‘á»ƒ trÃ¡nh bá»‹ cháº·n
        self.context = self.browser.new_context(
            user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            viewport={"width": 1920, "height": 1080}
        )
        self.page = self.context.new_page()
        safe_print("âœ… Bot Ä‘Ã£ khá»Ÿi Ä‘á»™ng!")

    def stop(self):
        """ÄÃ³ng trÃ¬nh duyá»‡t vÃ  MongoDB connection"""
        if self.browser:
            self.browser.close()
        if self.playwright:
            self.playwright.stop()
        if self.mongo_client:
            self.mongo_client.close()
            safe_print("âœ… ÄÃ£ Ä‘Ã³ng káº¿t ná»‘i MongoDB")
        safe_print("zzz Bot Ä‘Ã£ táº¯t.")

    def scrape_webnovel_fiction(self, fiction_url, max_chapters=None):
        """
        CÃ o má»™t bá»™ truyá»‡n Webnovel (single book URL)
        Args:
            fiction_url: URL cá»§a bá»™ truyá»‡n trÃªn Webnovel
            max_chapters: Sá»‘ chÆ°Æ¡ng tá»‘i Ä‘a muá»‘n cÃ o (None = láº¥y háº¿t)
        """
        safe_print(f"ğŸŒ Äang truy cáº­p truyá»‡n Webnovel: {fiction_url}")
        self.page.goto(fiction_url, timeout=config.TIMEOUT)
        # Äá»£i page load xong (wait cho networkidle)
        self.page.wait_for_load_state("networkidle")
        time.sleep(3)
        safe_print(f"    âœ… Page Ä‘Ã£ load xong, title: {self.page.title()}")
        
        # Láº¥y ID truyá»‡n tá»« URL (vÃ­ dá»¥: _34078380808505505)
        fiction_id = ""
        try:
            match = re.search(r"_(\d{10,})$", fiction_url)
            if match:
                fiction_id = match.group(1)
            else:
                match = re.search(r"(\d{10,})", fiction_url)
                if match:
                    fiction_id = match.group(1)
        except:
            fiction_id = "unknown"
        
        safe_print("... Äang láº¥y thÃ´ng tin chung")
        
        # Láº¥y title (Webnovel dÃ¹ng h1 hoáº·c h2 trong meta hoáº·c page title)
        title = ""
        try:
            # Thá»­ nhiá»u selector
            title_el = self.page.locator("h1").first
            if title_el.count() > 0:
                title = title_el.inner_text().strip()
            if not title:
                # Fallback: láº¥y tá»« page title
                title = self.page.title().split('|')[0].strip()
            safe_print(f"    âœ… Title: {title}")
        except Exception as e:
            safe_print(f"    âš ï¸ Lá»—i láº¥y title: {e}")
            title = "Unknown Title"
        
        # Láº¥y author (Webnovel cÃ³ link author vá»›i class/href profile)
        author = ""
        try:
            # TÃ¬m trong pháº§n tá»­ chá»©a "Author:"
            author_el = self.page.locator("a[href*='/profile/']").first
            if author_el.count() > 0:
                author = author_el.inner_text().strip()
            safe_print(f"    âœ… Author: {author}")
        except Exception as e:
            safe_print(f"    âš ï¸ Lá»—i láº¥y author: {e}")
            author = "Unknown Author"
        
        # Láº¥y cover image (Webnovel: img cÃ³ src chá»©a 'bookcover' hoáº·c 'book-pic')
        img_url_raw = None
        try:
            img_el = self.page.locator("img[src*='bookcover'], img[src*='book-pic'], img.book-cover").first
            if img_el.count() > 0:
                img_url_raw = img_el.get_attribute("src")
                # ThÃªm https: náº¿u URL báº¯t Ä‘áº§u báº±ng //
                if img_url_raw and img_url_raw.startswith("//"):
                    img_url_raw = "https:" + img_url_raw
                safe_print(f"    âœ… Cover: {img_url_raw[:80]}...")
        except Exception as e:
            safe_print(f"    âš ï¸ Lá»—i láº¥y cover: {e}")
        
        local_img_path = None
        if img_url_raw:
            local_img_path = utils.download_image(img_url_raw, fiction_id)
            if local_img_path:
                safe_print(f"    âœ… ÄÃ£ táº£i cover vá»: {local_img_path}")
        
        # Láº¥y genre (tá»« link category nhÆ° "Anime & Comics")
        genre = ""
        try:
            # TÃ¬m trong span cÃ³ icon book (thÆ°á»ng bÃªn cáº¡nh chapter count)
            # Pattern: <span>ğŸ“• Anime & Comics</span> hoáº·c link <a>Anime & Comics</a>
            genre_candidates = [
                "span._ml a",  # Link trong span._ml
                "a[href*='/category/']",  # Link cÃ³ /category/ trong href
                "span:has-text('Anime') a",  # Span chá»©a text "Anime" vÃ  cÃ³ link bÃªn trong
                ".det-info a[href*='/']"  # Fallback: link trong det-info
            ]
            
            for selector in genre_candidates:
                genre_el = self.page.locator(selector).first
                if genre_el.count() > 0:
                    genre_text = genre_el.inner_text().strip()
                    # Kiá»ƒm tra xem cÃ³ pháº£i genre há»£p lá»‡ khÃ´ng (khÃ´ng pháº£i number hoáº·c quÃ¡ ngáº¯n)
                    if genre_text and len(genre_text) > 2 and not genre_text.isdigit():
                        genre = genre_text
                        break
            
            if not genre:
                # Fallback: tÃ¬m text pattern "XXX Chapters" gáº§n Ä‘Ã³ vÃ  láº¥y text trÆ°á»›c Ä‘Ã³
                page_text = self.page.locator("body").inner_text()
                genre_match = re.search(r"([A-Za-z &]+)\s+\d+\s+Chapters", page_text)
                if genre_match:
                    genre = genre_match.group(1).strip()
            
            safe_print(f"    âœ… Genre: {genre}")
        except Exception as e:
            safe_print(f"    âš ï¸ Lá»—i láº¥y genre: {e}")
        
        # Láº¥y tags tá»« div.m-tags > p.m-tag (Webnovel structure)
        tags = []
        try:
            # TÃ¬m div.m-tags container
            tags_container = self.page.locator("div.m-tags").first
            if tags_container.count() > 0:
                # Láº¥y táº¥t cáº£ p.m-tag trong container
                tag_elements = tags_container.locator("p.m-tag").all()
                for tag_el in tag_elements:
                    tag_text = tag_el.inner_text().strip()
                    # Clean tag text (bá» # prefix náº¿u cÃ³)
                    if tag_text:
                        clean_tag = tag_text.lstrip('#').strip()
                        if clean_tag and clean_tag not in tags:
                            tags.append(clean_tag)
            safe_print(f"    âœ… Tags: {tags}")
        except Exception as e:
            safe_print(f"    âš ï¸ Lá»—i láº¥y tags: {e}")
        
        # Láº¥y description/synopsis (Webnovel: thÆ°á»ng trong section hoáº·c div cÃ³ text "Synopsis")
        description = ""
        try:
            # TÃ¬m cÃ¡c paragraph trong pháº§n synopsis (thÆ°á»ng cÃ³ class _synopsis hoáº·c náº±m trong section.j_synopsis)
            desc_paras = self.page.locator("div._synopsis p, section.j_synopsis p").all()
            if desc_paras:
                description = "\n".join([p.inner_text().strip() for p in desc_paras if p.inner_text().strip()])
                safe_print(f"    âœ… Description: {description[:100]}...")
            else:
                # Fallback: tÃ¬m pháº§n Synopsis
                desc_container = self.page.locator("text=Synopsis").locator('..').first
                if desc_container.count() > 0:
                    desc_text = desc_container.inner_text()
                    # Loáº¡i bá» chá»¯ "Synopsis" vÃ  láº¥y chá»‰ pháº§n Ä‘áº§u (trÆ°á»›c Tags/Fans)
                    lines = [line.strip() for line in desc_text.split('\n') if line.strip()]
                    # Lá»c bá» "Synopsis" vÃ  dá»«ng á»Ÿ "Tags", "Fans", "General Audiences", etc.
                    filtered_lines = []
                    for line in lines:
                        if line.lower() in ['synopsis', 'tags', 'fans', 'see all', 'general audiences', 'weekly power status']:
                            continue
                        if line.startswith('#') or 'Contributed' in line or 'Power' in line:
                            break
                        filtered_lines.append(line)
                    description = "\n".join(filtered_lines).strip()
                    safe_print(f"    âœ… Description: {description[:100]}...")
        except Exception as e:
            safe_print(f"    âš ï¸ Lá»—i láº¥y description: {e}")
        
        # Láº¥y stats (views, chapters count, etc.)
        views_text = ""
        chapters_count_text = ""
        try:
            # TÃ¬m text chá»©a "Views" hoáº·c sá»‘ lÆ°á»£t xem
            page_text = self.page.locator("body").inner_text()
            view_match = re.search(r"([\d,\.KMkm]+)\s*Views?", page_text, re.I)
            if view_match:
                views_text = view_match.group(1)
            
            # TÃ¬m text chá»©a "Chapters" hoáº·c sá»‘ chÆ°Æ¡ng
            chap_match = re.search(r"(\d+[\d,\.]*)\s*Chapters?", page_text, re.I)
            if chap_match:
                chapters_count_text = chap_match.group(1)
        except:
            pass
        
        # Láº¥y total reviews vÃ  ratings
        total_reviews = 0
        total_rating = 0.0
        try:
            # TÃ¬m rating chÃ­nh (4.87) - thÆ°á»ng hiá»ƒn thá»‹ vá»›i stars gáº§n title
            # Pattern 1: TÃ¬m sá»‘ tháº­p phÃ¢n cÃ³ 1-2 chá»¯ sá»‘ sau dáº¥u pháº©y, theo sau bá»Ÿi "(XXX ratings)"
            rating_match = re.search(r"(\d+\.\d{1,2})\s*\((\d+)\s*ratings?\)", page_text, re.I)
            if rating_match:
                total_rating = float(rating_match.group(1))
                total_reviews = int(rating_match.group(2))  # XXX ratings
            else:
                # Pattern 2: Fallback - tÃ¬m "XXX Reviews" riÃªng
                reviews_match = re.search(r"(\d+)\s*Reviews?", page_text, re.I)
                if reviews_match:
                    total_reviews = int(reviews_match.group(1))
                
                # TÃ¬m rating (sá»‘ tháº­p phÃ¢n)
                rating_match2 = re.search(r"(\d+\.\d+)", page_text)
                if rating_match2:
                    total_rating = float(rating_match2.group(1))
        except Exception as e:
            safe_print(f"    âš ï¸ Lá»—i láº¥y rating: {e}")
        
        # Láº¥y scores chi tiáº¿t (5 categories) tá»« review section
        scores = {
            "writing_quality": "",
            "stability_of_updates": "",
            "story_development": "",
            "character_design": "",
            "world_background": ""
        }
        try:
            # TÃ¬m review section cÃ³ 5 score categories
            score_items = self.page.locator("li:has(strong)").all()
            for item in score_items:
                try:
                    label = item.locator("strong").inner_text().strip().lower()
                    # Äáº¿m sá»‘ sao (svg vá»›i class _on)
                    stars = item.locator("svg.g_star._on, span.g_star svg._on").count()
                    
                    if "writing quality" in label:
                        scores["writing_quality"] = str(stars)
                    elif "stability" in label:
                        scores["stability_of_updates"] = str(stars)
                    elif "story" in label:
                        scores["story_development"] = str(stars)
                    elif "character" in label:
                        scores["character_design"] = str(stars)
                    elif "world" in label or "background" in label:
                        scores["world_background"] = str(stars)
                except:
                    continue
            safe_print(f"    âœ… Scores: {scores}")
            safe_print(f"    âœ… Reviews: {total_reviews}, Rating: {total_rating}")
        except Exception as e:
            safe_print(f"    âš ï¸ Lá»—i láº¥y scores: {e}")
        
        # Láº¥y story-level comments (reviews/paragraphs tá»« trang fiction)
        story_comments = []
        try:
            safe_print("    ğŸ’¬ Äang láº¥y story-level comments...")
            
            # Webnovel cÃ³ thá»ƒ cÃ³ tab "Reviews" cáº§n click Ä‘á»ƒ show
            try:
                # TÃ¬m vÃ  click vÃ o tab/button Reviews
                review_tab = self.page.locator("button:has-text('Review'), a:has-text('Review'), div:has-text('Reviews')").first
                if review_tab.count() > 0:
                    safe_print("    ğŸ”˜ Clicking Reviews tab...")
                    review_tab.click()
                    time.sleep(2)
            except:
                pass
            
            # Scroll xuá»‘ng review section
            try:
                reviews_heading = self.page.locator("h3:has-text('Review'), h2:has-text('Review')").first
                if reviews_heading.count() > 0:
                    reviews_heading.scroll_into_view_if_needed()
                    time.sleep(2)
            except:
                self.page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
                time.sleep(3)
            
            # Scroll nhiá»u láº§n Ä‘á»ƒ load Táº¤T Cáº¢ reviews (infinite scroll)
            safe_print("    â³ Äang scroll Ä‘á»ƒ load Táº¤T Cáº¢ reviews...")
            previous_height = 0
            no_change_count = 0
            max_scrolls = 50  # Giá»›i háº¡n tá»‘i Ä‘a Ä‘á»ƒ trÃ¡nh vÃ²ng láº·p vÃ´ háº¡n
            
            for scroll_attempt in range(max_scrolls):
                # Scroll xuá»‘ng
                self.page.evaluate("window.scrollBy(0, 500)")
                time.sleep(1.2)
                
                # Kiá»ƒm tra xem page cÃ³ tÄƒng chiá»u cao khÃ´ng
                current_height = self.page.evaluate("document.body.scrollHeight")
                if current_height == previous_height:
                    no_change_count += 1
                    if no_change_count >= 3:  # Náº¿u 3 láº§n liÃªn tiáº¿p khÃ´ng thay Ä‘á»•i -> Ä‘Ã£ háº¿t
                        safe_print(f"    âœ… ÄÃ£ scroll háº¿t reviews (sau {scroll_attempt + 1} láº§n scroll)")
                        break
                else:
                    no_change_count = 0  # Reset náº¿u cÃ³ thay Ä‘á»•i
                    previous_height = current_height
                    safe_print(f"    ğŸ“œ Scroll láº§n {scroll_attempt + 1}: PhÃ¡t hiá»‡n thÃªm content...")
            
            if scroll_attempt >= max_scrolls - 1:
                safe_print(f"    âš ï¸ ÄÃ£ scroll {max_scrolls} láº§n, cÃ³ thá»ƒ váº«n cÃ²n reviews nhÆ°ng dá»«ng Ä‘á»ƒ trÃ¡nh timeout")
            
            # Äá»£i reviews render
            safe_print("    â³ Äá»£i reviews render...")
            time.sleep(5)
            
            # Láº¥y reviews - approach Ä‘Æ¡n giáº£n: tÃ¬m text "Attention please" Ä‘á»ƒ xÃ¡c Ä‘á»‹nh review Ä‘áº§u tiÃªn
            # Sau Ä‘Ã³ tÃ¬m pattern: profile link + content
            
            # Debug: In ra page content Ä‘á»ƒ xem cÃ³ reviews khÃ´ng
            page_text = self.page.locator("body").inner_text()
            if "Attention please" in page_text:
                safe_print("    âœ… TÃ¬m tháº¥y text 'Attention please' trong page")
            else:
                safe_print("    âš ï¸ KHÃ”NG tÃ¬m tháº¥y 'Attention please' - reviews chÆ°a load")
            
            # TÃ¬m táº¥t cáº£ text nodes chá»©a "Attention" hoáº·c reviews dÃ i
            test_phrases = ["Attention please", "Its just peak", "Without a doubt", "HOOOOOOLY"]
            review_items = []
            
            for phrase in test_phrases:
                try:
                    # Thá»­ nhiá»u selector khÃ¡c nhau
                    selectors_with_phrase = [
                        f"p:has-text('{phrase}')",
                        f"div:has-text('{phrase}')",
                        f"li:has-text('{phrase}')",
                        f"*:has-text('{phrase}')"
                    ]
                    
                    for sel in selectors_with_phrase:
                        phrase_el = self.page.locator(sel).first
                        if phrase_el.count() > 0:
                            safe_print(f"    ğŸ” TÃ¬m tháº¥y element vá»›i selector: {sel}")
                            # Thá»­ nhiá»u loáº¡i ancestor
                            ancestor = None
                            ancestor_selectors = [
                                "xpath=ancestor::li[1]",
                                "xpath=ancestor::div[@class][1]",
                                "xpath=parent::*[1]",
                            ]
                            
                            for anc_sel in ancestor_selectors:
                                test_anc = phrase_el.locator(anc_sel).first
                                if test_anc.count() > 0:
                                    ancestor = test_anc
                                    safe_print(f"    âœ… TÃ¬m Ä‘Æ°á»£c ancestor vá»›i: {anc_sel}")
                                    break
                            
                            if ancestor:
                                review_items.append(ancestor)
                                safe_print(f"    âœ… TÃ¬m tháº¥y review chá»©a: '{phrase[:30]}...'")
                                break
                            else:
                                safe_print(f"    âš ï¸ KhÃ´ng tÃ¬m Ä‘Æ°á»£c ancestor")
                except Exception as ex:
                    safe_print(f"    âš ï¸ Error: {ex}")
                    continue
            
            if not review_items:
                safe_print(f"    âš ï¸ KhÃ´ng tÃ¬m tháº¥y reviews, skip comments")
                story_comments = []
            else:
                safe_print(f"    âœ… TÃ¬m Ä‘Æ°á»£c {len(review_items)} review items, báº¯t Ä‘áº§u parse...")
                
                for review_item in review_items:
                    try:
                        # Láº¥y toÃ n bá»™ text tá»« item Ä‘á»ƒ debug
                        full_item_text = review_item.inner_text().strip()
                        
                        # Láº¥y username tá»« link profile
                        username = ""
                        username_el = review_item.locator("a[href*='/profile/']").first
                        if username_el.count() > 0:
                            username = username_el.inner_text().strip()
                            # Loáº¡i bá» "LV X" prefix náº¿u cÃ³
                            username = re.sub(r'^LV\s*\d+\s*', '', username).strip()
                        
                        # Láº¥y comment content
                        # Strategy: Láº¥y táº¥t cáº£ paragraphs, filter ra nhá»¯ng cÃ¡i khÃ´ng pháº£i metadata
                        all_paragraphs = review_item.locator("p").all()
                        content_lines = []
                        
                        for p in all_paragraphs:
                            p_text = p.inner_text().strip()
                            # Skip metadata lines (LV, VIEW, short single words, numbers only)
                            if p_text and len(p_text) > 3:
                                # Skip náº¿u chá»‰ lÃ  metadata
                                if re.match(r'^(LV\s*\d+|VIEW|LIKE|\d+\s*(mth|d|h|m)|\d+$|Prev|Next)', p_text, re.I):
                                    continue
                                # Skip náº¿u chá»‰ lÃ  username
                                if p_text == username:
                                    continue
                                # Valid content
                                content_lines.append(p_text)
                        
                        content_text = "\n".join(content_lines).strip()
                        
                        # Fallback: náº¿u khÃ´ng cÃ³ content tá»« <p>, láº¥y toÃ n bá»™ vÃ  filter
                        if not content_text:
                            lines = full_item_text.split('\n')
                            filtered = []
                            for line in lines:
                                line = line.strip()
                                if not line or len(line) < 5:
                                    continue
                                # Skip metadata
                                if re.match(r'^(LV\s*\d+|VIEW|LIKE|\d+\s*(mth|d|h|m)|Prev|Next)', line, re.I):
                                    continue
                                if line == username:
                                    continue
                                filtered.append(line)
                            content_text = "\n".join(filtered[:20])  # Láº¥y tá»‘i Ä‘a 20 dÃ²ng Ä‘áº§u
                        
                        # Báº¯t buá»™c pháº£i cÃ³ content
                        if not content_text or len(content_text) < 15:
                            continue
                        
                        # Kiá»ƒm tra GIF images trong review
                        gif_imgs = review_item.locator("img[src*='.gif'], img[src*='giphy'], img[src*='tenor'], img[data-src*='.gif']").all()
                        gif_urls = []
                        for gif_el in gif_imgs:
                            gif_url = gif_el.get_attribute("src") or gif_el.get_attribute("data-src")
                            if gif_url:
                                if gif_url.startswith("//"):
                                    gif_url = "https:" + gif_url
                                gif_urls.append(gif_url)
                        
                        # ThÃªm GIF URLs vÃ o content
                        if gif_urls:
                            for gif_url in gif_urls:
                                content_text += f"\n[GIF: {gif_url}]"
                        
                        # Láº¥y time (1mth, 24d, etc.)
                        time_text = ""
                        time_patterns = [r'(\d+mth)', r'(\d+d)', r'(\d+h)', r'(\d+m)']
                        review_text = review_item.inner_text()
                        for pattern in time_patterns:
                            match = re.search(pattern, review_text)
                            if match:
                                time_text = match.group(1)
                                break
                        
                        # Láº¥y comment_id tá»« nhiá»u attributes
                        comment_id = (
                            review_item.get_attribute("id") or 
                            review_item.get_attribute("data-id") or 
                            review_item.get_attribute("data-comment-id") or
                            review_item.get_attribute("data-cid") or
                            ""
                        )
                        # Náº¿u váº«n khÃ´ng cÃ³, generate tá»« username + time
                        if not comment_id:
                            import hashlib
                            comment_id = hashlib.md5(f"{username}_{time_text}_{content_text[:20]}".encode()).hexdigest()[:12]
                        
                        # Táº¡o comment object theo schema má»›i
                        comment_data = {
                            "comment_id": comment_id,
                            "content_id": fiction_id,  # Story ID cho story-level comments
                            "comment_text": content_text,  # Äá»•i content â†’ comment_text
                            "time": time_text,
                            "user_id": username,
                            "parent_id": "",
                            "is_root": True,  # Story-level comments lÃ  root
                            "react": 0,  # TODO: scrape reactions/likes
                            "replies": []
                        }
                        
                        story_comments.append(comment_data)
                        
                    except Exception as ex:
                        safe_print(f"    âš ï¸ Lá»—i parse review item: {ex}")
                        continue
            
            safe_print(f"    âœ… Láº¥y Ä‘Æ°á»£c {len(story_comments)} story comments")
        except Exception as e:
            safe_print(f"    âš ï¸ Lá»—i láº¥y story comments: {e}")
        
        # Convert chapters count to integer
        total_chapters_int = 0
        try:
            total_chapters_int = int(chapters_count_text.replace(',', '').replace('.', ''))
        except:
            total_chapters_int = 0
        
        # Táº¡o fiction_data theo schema HOÃ€N CHá»ˆNH
        fiction_data = {
            "story_id": fiction_id,
            "story_name": title,
            "story_url": fiction_url,
            "cover_image": local_img_path,
            "author_id": author,  # Sáº½ link vá»›i Users collection
            "genre": genre,
            "status": "Unknown",  # TODO: scrape (Ongoing/Completed/Hiatus)
            "tags": tags,
            "description": description,
            "total_chapters": total_chapters_int,
            "total_views": views_text,
            "followers": 0,  # TODO: scrape
            "favorites": 0,  # TODO: scrape
            "ratings": total_reviews,
            "overall_score": total_rating,
            "style_score": float(scores.get("writing_quality", 0)) if scores.get("writing_quality") else 0,
            "story_score": float(scores.get("story_development", 0)) if scores.get("story_development") else 0,
            "character_score": float(scores.get("character_design", 0)) if scores.get("character_design") else 0,
            "world_background_score": float(scores.get("world_background", 0)) if scores.get("world_background") else 0,
            "stability_score": float(scores.get("stability_of_updates", 0)) if scores.get("stability_of_updates") else 0,
            "voted": 0,  # TODO: scrape power stones
            "time": "",  # TODO: scrape publish date
            "comments": story_comments,
            "chapter_list": []
        }
        
        # Láº¥y danh sÃ¡ch chapters tá»« catalog
        safe_print("... Äang tÃ¬m danh sÃ¡ch chÆ°Æ¡ng")
        chapter_urls = self._get_webnovel_chapter_urls(fiction_url, fiction_id)
        
        if not chapter_urls:
            safe_print("âš ï¸ KhÃ´ng tÃ¬m tháº¥y chÆ°Æ¡ng nÃ o!")
        else:
            if max_chapters:
                chapter_urls = chapter_urls[:max_chapters]
            safe_print(f"--> TÃ¬m tháº¥y {len(chapter_urls)} chÆ°Æ¡ng")
            
            # CÃ o chapters song song
            safe_print(f"ğŸš€ Báº¯t Ä‘áº§u cÃ o {len(chapter_urls)} chÆ°Æ¡ng vá»›i {self.max_workers} thread...")
            chapter_results = [None] * len(chapter_urls)
            future_to_index = {}
            
            with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
                for index, chap_url in enumerate(chapter_urls):
                    future = executor.submit(self._scrape_single_chapter_worker, chap_url, index)
                    future_to_index[future] = index
                
                completed = 0
                for future in as_completed(future_to_index):
                    index = future_to_index[future]
                    try:
                        chapter_data = future.result()
                        chapter_results[index] = chapter_data
                        completed += 1
                        status = "âœ…" if chapter_data else "âš ï¸"
                        safe_print(f"    {status} HoÃ n thÃ nh chÆ°Æ¡ng {index + 1}/{len(chapter_urls)} (Ä‘Ã£ xong {completed}/{len(chapter_urls)})")
                    except Exception as e:
                        safe_print(f"    âŒ Lá»—i khi cÃ o chÆ°Æ¡ng {index + 1}: {e}")
                        chapter_results[index] = None
            
            # ThÃªm vÃ o fiction_data theo Ä‘Ãºng thá»© tá»±
            for index in range(len(chapter_results)):
                chapter_data = chapter_results[index]
                if chapter_data:
                    fiction_data["chapter_list"].append(chapter_data)
        
        safe_print(f"âœ… ÄÃ£ hoÃ n thÃ nh {len(fiction_data['chapter_list'])} chÆ°Æ¡ng")
        
        # LÆ°u káº¿t quáº£
        self._save_to_json(fiction_data)

    def _get_webnovel_chapter_urls(self, fiction_url, fiction_id):
        """Láº¥y danh sÃ¡ch URL chapters tá»« Webnovel (workaround: láº¥y first chapter rá»“i navigate)"""
        chapter_urls = []
        
        # Chiáº¿n lÆ°á»£c má»›i: tÃ¬m nÃºt READ hoáº·c first chapter link trÃªn trang book
        safe_print(f"    ğŸ“– TÃ¬m first chapter tá»« trang book...")
        first_chapter_url = None
        
        try:
            # TÃ¬m nÃºt "READ" hoáº·c link chapter Ä‘áº§u tiÃªn
            read_button = self.page.locator("a:has-text('READ'), a.j_read_btn, a[class*='read']").first
            if read_button.count() > 0:
                first_chapter_url = read_button.get_attribute("href")
                if first_chapter_url:
                    if not first_chapter_url.startswith("http"):
                        first_chapter_url = "https://www.webnovel.com" + first_chapter_url
                    safe_print(f"    âœ… TÃ¬m tháº¥y first chapter: {first_chapter_url[:80]}...")
        except Exception as e:
            safe_print(f"    âš ï¸ KhÃ´ng tÃ¬m tháº¥y nÃºt READ: {e}")
        
        # Náº¿u khÃ´ng tÃ¬m Ä‘Æ°á»£c, thá»­ build first chapter URL (Webnovel format)
        if not first_chapter_url:
            # Webnovel first chapter thÆ°á»ng cÃ³ format: /book/<id>/<slug>_<chapter-id>
            # Ta cÃ³ thá»ƒ thá»­ guess hoáº·c láº¥y tá»« API
            safe_print(f"    âš ï¸ KhÃ´ng tÃ¬m tháº¥y first chapter link")
            safe_print(f"    ğŸ’¡ Workaround: Webnovel yÃªu cáº§u login hoáº·c block bot Ä‘á»ƒ xem catalog")
            safe_print(f"    ğŸ’¡ Báº¡n cÃ³ thá»ƒ:")
            safe_print(f"        1. Cháº¡y vá»›i HEADLESS=False trong config.py Ä‘á»ƒ xem browser")
            safe_print(f"        2. ThÃªm cookies/login vÃ o browser context")
            safe_print(f"        3. DÃ¹ng API Webnovel (náº¿u cÃ³)")
            return []
        
        # Náº¿u tÃ¬m Ä‘Æ°á»£c first chapter, ta cÃ³ thá»ƒ navigate qua chapters (prev/next)
        # NhÆ°ng giá»›i háº¡n Ä‘á»ƒ demo
        chapter_urls.append(first_chapter_url)
        safe_print(f"    âœ… Láº¥y Ä‘Æ°á»£c 1 chapter URL (demo mode)")
        
        return chapter_urls

    def scrape_best_rated_fictions(self, best_rated_url, num_fictions=10, start_from=0):
        """
        CÃ o nhiá»u bá»™ truyá»‡n tá»« trang web-novel
        Args:
            best_rated_url: URL trang web-novel
            num_fictions: Sá»‘ lÆ°á»£ng bá»™ truyá»‡n muá»‘n cÃ o (máº·c Ä‘á»‹nh 10)
            start_from: Báº¯t Ä‘áº§u tá»« vá»‹ trÃ­ thá»© máº¥y (0 = bá»™ Ä‘áº§u tiÃªn, 5 = bá» qua 5 bá»™ Ä‘áº§u)
        """
        safe_print(f"ğŸ“š Äang truy cáº­p trang web-novel: {best_rated_url}")
        self.page.goto(best_rated_url, timeout=config.TIMEOUT)
        time.sleep(2)
        
        # Láº¥y danh sÃ¡ch cÃ¡c bá»™ truyá»‡n tá»« trang web-novel
        if start_from > 0:
            safe_print(f"ğŸ” Äang láº¥y danh sÃ¡ch {num_fictions} bá»™ truyá»‡n (báº¯t Ä‘áº§u tá»« vá»‹ trÃ­ {start_from + 1})...")
        else:
            safe_print(f"ğŸ” Äang láº¥y danh sÃ¡ch {num_fictions} bá»™ truyá»‡n Ä‘áº§u tiÃªn...")
        fiction_urls = self._get_fiction_urls_from_best_rated(num_fictions, start_from)
        
        if not fiction_urls:
            safe_print("âŒ KhÃ´ng tÃ¬m tháº¥y bá»™ truyá»‡n nÃ o!")
            return
        
        safe_print(f"âœ… ÄÃ£ tÃ¬m tháº¥y {len(fiction_urls)} bá»™ truyá»‡n:")
        for i, url in enumerate(fiction_urls, 1):
            safe_print(f"   {i}. {url}")
        
        # CÃ o tá»«ng bá»™ truyá»‡n tuáº§n tá»±
        for index, fiction_url in enumerate(fiction_urls, 1):
            safe_print(f"\n{'='*60}")
            safe_print(f"ğŸ“– Báº¯t Ä‘áº§u cÃ o bá»™ truyá»‡n {index}/{len(fiction_urls)}")
            safe_print(f"{'='*60}")
            try:
                self.scrape_fiction(fiction_url)
                safe_print(f"âœ… HoÃ n thÃ nh bá»™ truyá»‡n {index}/{len(fiction_urls)}")
            except Exception as e:
                safe_print(f"âŒ Lá»—i khi cÃ o bá»™ truyá»‡n {index}: {e}")
                continue
            
            # Delay giá»¯a cÃ¡c bá»™ truyá»‡n
            if index < len(fiction_urls):
                safe_print(f"â³ Nghá»‰ {config.DELAY_BETWEEN_CHAPTERS * 2} giÃ¢y trÆ°á»›c khi cÃ o bá»™ tiáº¿p theo...")
                time.sleep(config.DELAY_BETWEEN_CHAPTERS * 2)
        
        safe_print(f"\n{'='*60}")
        safe_print(f"ğŸ‰ ÄÃ£ hoÃ n thÃ nh cÃ o {len(fiction_urls)} bá»™ truyá»‡n!")
        safe_print(f"{'='*60}")

    def _get_fiction_urls_from_best_rated(self, num_fictions=10, start_from=0):
        """
        Láº¥y danh sÃ¡ch URL cá»§a cÃ¡c bá»™ truyá»‡n tá»« trang web-novel
        Selector: h2.fiction-title a
        Args:
            num_fictions: Sá»‘ lÆ°á»£ng bá»™ truyá»‡n muá»‘n láº¥y
            start_from: Báº¯t Ä‘áº§u tá»« vá»‹ trÃ­ thá»© máº¥y (0 = bá»™ Ä‘áº§u tiÃªn)
        """
        fiction_urls = []
        
        try:
            # Scroll xuá»‘ng Ä‘á»ƒ load thÃªm ná»™i dung náº¿u cáº§n
            self.page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
            time.sleep(2)
            
            # Láº¥y táº¥t cáº£ cÃ¡c link truyá»‡n tá»« tháº» h2.fiction-title a
            fiction_links = self.page.locator("h2.fiction-title a").all()
            
            # TÃ­nh toÃ¡n vá»‹ trÃ­ báº¯t Ä‘áº§u vÃ  káº¿t thÃºc
            start_index = start_from
            end_index = start_from + num_fictions
            
            # Láº¥y cÃ¡c link tá»« vá»‹ trÃ­ start_from Ä‘áº¿n end_index
            for link in fiction_links[start_index:end_index]:
                try:
                    href = link.get_attribute("href")
                    if href:
                        # Táº¡o full URL
                        if href.startswith("/"):
                            full_url = config.BASE_URL + href
                        elif href.startswith("http"):
                            full_url = href
                        else:
                            full_url = config.BASE_URL + "/" + href
                        
                        if full_url not in fiction_urls:
                            fiction_urls.append(full_url)
                except Exception as e:
                    safe_print(f"âš ï¸ Lá»—i khi láº¥y URL truyá»‡n: {e}")
                    continue
            
            return fiction_urls
            
        except Exception as e:
            safe_print(f"âš ï¸ Lá»—i khi láº¥y danh sÃ¡ch truyá»‡n tá»« web-novel: {e}")
            return []

    def scrape_fiction(self, fiction_url):
        """
        HÃ m chÃ­nh Ä‘á»ƒ cÃ o toÃ n bá»™ 1 bá»™ truyá»‡n.
        Luá»“ng Ä‘i: VÃ o trang truyá»‡n -> Láº¥y Info -> Láº¥y List Chapter -> VÃ o tá»«ng Chapter -> Láº¥y Content.
        """
        safe_print(f"ğŸŒ Äang truy cáº­p truyá»‡n: {fiction_url}")
        self.page.goto(fiction_url, timeout=config.TIMEOUT)

        # 1. Láº¥y ID truyá»‡n tá»« URL (VÃ­ dá»¥: 21220)
        fiction_id = fiction_url.split("/")[4]

        # 2. Láº¥y thÃ´ng tin tá»•ng quan (Metadata)
        safe_print("... Äang láº¥y thÃ´ng tin chung")
        
        # Láº¥y title
        title = self.page.locator("h1").first.inner_text()
        
        # Láº¥y URL áº£nh bÃ¬a rá»“i táº£i vá» luÃ´n
        img_url_raw = self.page.locator(".cover-art-container img").get_attribute("src")
        local_img_path = utils.download_image(img_url_raw, fiction_id)

        # Láº¥y author
        author = self.page.locator(".fic-title h4 a").first.inner_text()

        # Láº¥y category
        category = self.page.locator(".fiction-info span").first.inner_text()

        # Láº¥y status
        status = self.page.locator(".fiction-info span:nth-child(2)").first.inner_text()

        #Láº¥y tags
        tags = self.page.locator(".tags a").all_inner_texts()

        #Láº¥y description - giá»¯ nguyÃªn Ä‘á»‹nh dáº¡ng nhÆ° trong UI
        description = ""
        try:
            desc_container = self.page.locator(".description").first
            if desc_container.count() > 0:
                # Láº¥y HTML Ä‘á»ƒ giá»¯ Ä‘á»‹nh dáº¡ng
                html_content = desc_container.inner_html()
                # Chuyá»ƒn HTML sang text vá»›i Ä‘á»‹nh dáº¡ng Ä‘Ãºng
                description = self._convert_html_to_formatted_text(html_content)
        except Exception as e:
            safe_print(f"âš ï¸ Lá»—i khi láº¥y description: {e}")
            description = ""

        #Láº¥y stats
        # stats = self.page.locator(".stats-content .list-item").all()
        # Container chÃ­nh: .stats-content ul.list-unstyled
        base_locator = ".stats-content ul.list-unstyled li:nth-child({}) span"

        # 1. Overall Score (Náº±m á»Ÿ vá»‹ trÃ­ con thá»© 2)
        overall_score = self.page.locator(base_locator.format(2)).inner_text()

        # 2. Style Score (Vá»‹ trÃ­ con thá»© 4)
        style_score = self.page.locator(base_locator.format(4)).inner_text()

        # 3. Story Score (Vá»‹ trÃ­ con thá»© 6)
        story_score = self.page.locator(base_locator.format(6)).inner_text()

        # 4. Grammar Score (Vá»‹ trÃ­ con thá»© 8)
        grammar_score = self.page.locator(base_locator.format(8)).inner_text()

        # 5. Character Score (Vá»‹ trÃ­ con thá»© 10)
        character_score = self.page.locator(base_locator.format(10)).inner_text()

        # 1. Äá»‹nh vá»‹ táº¥t cáº£ cÃ¡c tháº» <li> chá»©a GIÃ TRá»Š sá»‘ liá»‡u
        # Sá»­ dá»¥ng class Ä‘áº·c trÆ°ng (.font-red-sunglo) vÃ  giá»›i háº¡n trong khá»‘i stats bÃªn pháº£i (.col-sm-6)
        stats_values_locator = self.page.locator("div.col-sm-6 li.font-red-sunglo")
        
        # 2. Láº¥y giÃ¡ trá»‹ báº±ng cÃ¡ch dÃ¹ng chá»‰ má»¥c (index)
        
        # Láº¥y total_views (Index 0)
        total_views = stats_values_locator.nth(0).inner_text()
        
        # Láº¥y average_views (Index 1)
        average_views = stats_values_locator.nth(1).inner_text()
        
        # Láº¥y followers (Index 2)
        followers = stats_values_locator.nth(2).inner_text()
        
        # Láº¥y favorites (Index 3)
        favorites = stats_values_locator.nth(3).inner_text()
        
        # Láº¥y ratings (Index 4)
        ratings = stats_values_locator.nth(4).inner_text()
        
        # Láº¥y pages/words (Index 5 - GiÃ¡ trá»‹ cuá»‘i cÃ¹ng)
        pages = stats_values_locator.nth(5).inner_text()

        # Táº¡o cáº¥u trÃºc dá»¯ liá»‡u tá»•ng quan sau khi Ä‘Ã£ láº¥y háº¿t cÃ¡c biáº¿n
        # Theo scheme: fiction id, fiction name, fiction url, cover image, author, category, status, tags, description
        fiction_data = {
            "id": fiction_id,
            "name": title,  # Scheme: fiction name
            "url": fiction_url,  # Scheme: fiction url
            "cover_image": local_img_path,  # Scheme: cover image
            "author": author,
            "category": category,
            "status": status,
            "tags": tags,
            "description": description,
            "stats": {
                "score": {
                    "overall_score": overall_score,
                    "style_score": style_score,
                    "story_score": story_score,
                    "grammar_score": grammar_score,
                    "character_score": character_score,
                },
                "views": {
                    "total_views": total_views,
                    "average_views": average_views,
                    "followers": followers,
                    "favorites": favorites,
                    "ratings": ratings,
                    "page_views": pages,
                }
            },
            "reviews": [],  # Sáº½ Ä‘Æ°á»£c Ä‘iá»n sau
            "chapters": []     # Chuáº©n bá»‹ cÃ¡i máº£ng rá»—ng Ä‘á»ƒ chá»©a cÃ¡c chÆ°Æ¡ng
        }

        # 3. Láº¥y danh sÃ¡ch link chÆ°Æ¡ng tá»« Táº¤T Cáº¢ cÃ¡c trang phÃ¢n trang
        safe_print("... Äang láº¥y danh sÃ¡ch chÆ°Æ¡ng tá»« táº¥t cáº£ cÃ¡c trang")
        chapter_urls = self._get_all_chapters_from_pagination(fiction_url)
        
        safe_print(f"--> Tá»•ng cá»™ng tÃ¬m tháº¥y {len(chapter_urls)} chÆ°Æ¡ng tá»« táº¥t cáº£ cÃ¡c trang.")

        # 3.5. Láº¥y reviews cho toÃ n bá»™ truyá»‡n
        safe_print("... Äang láº¥y reviews cho toÃ n bá»™ truyá»‡n")
        reviews = self._scrape_reviews(fiction_url)
        fiction_data["reviews"] = reviews
        safe_print(f"âœ… ÄÃ£ láº¥y Ä‘Æ°á»£c {len(reviews)} reviews")

        # 4. CÃ o cÃ¡c chÆ°Æ¡ng song song vá»›i ThreadPoolExecutor (GIá»® ÄÃšNG THá»¨ Tá»°)
        safe_print(f"ğŸš€ Báº¯t Ä‘áº§u cÃ o {len(chapter_urls)} chÆ°Æ¡ng vá»›i {self.max_workers} thread...")
        
        # Táº¡o list káº¿t quáº£ cá»‘ Ä‘á»‹nh theo index - má»—i index = 1 chÆ°Æ¡ng
        chapter_results = [None] * len(chapter_urls)
        
        # Dictionary Ä‘á»ƒ map future -> index Ä‘á»ƒ biáº¿t chÆ°Æ¡ng nÃ o
        future_to_index = {}
        
        # Sá»­ dá»¥ng ThreadPoolExecutor - NÃ“ Tá»° Äá»˜NG PHÃ‚N PHá»I cÃ´ng viá»‡c!
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            # Submit Táº¤T Cáº¢ chapters vÃ o pool - má»—i chÆ°Æ¡ng chá»‰ submit 1 Láº¦N
            for index, chap_url in enumerate(chapter_urls):
                future = executor.submit(self._scrape_single_chapter_worker, chap_url, index)
                future_to_index[future] = index
            
            # Thu tháº­p káº¿t quáº£ - cÃ¡c thread cÃ³ thá»ƒ hoÃ n thÃ nh báº¥t ká»³ lÃºc nÃ o
            completed = 0
            for future in as_completed(future_to_index):
                index = future_to_index[future]  # Láº¥y index cá»§a chÆ°Æ¡ng nÃ y
                try:
                    chapter_data = future.result()
                    # LÆ¯U VÃ€O ÄÃšNG Vá»Š TRÃ INDEX - khÃ´ng pháº£i append!
                    chapter_results[index] = chapter_data
                    completed += 1
                    status = "âœ…" if chapter_data else "âš ï¸"
                    safe_print(f"    {status} HoÃ n thÃ nh chÆ°Æ¡ng {index + 1}/{len(chapter_urls)} (Ä‘Ã£ xong {completed}/{len(chapter_urls)})")
                except Exception as e:
                    safe_print(f"    âŒ Lá»—i khi cÃ o chÆ°Æ¡ng {index + 1}: {e}")
                    chapter_results[index] = None

        # SAU KHI Táº¤T Cáº¢ XONG: ThÃªm vÃ o fiction_data THEO ÄÃšNG THá»¨ Tá»°
        safe_print(f"ğŸ“ Sáº¯p xáº¿p káº¿t quáº£ theo Ä‘Ãºng thá»© tá»±...")
        for index in range(len(chapter_results)):
            chapter_data = chapter_results[index]
            if chapter_data:
                fiction_data["chapters"].append(chapter_data)
            else:
                safe_print(f"    âš ï¸ Bá» qua chÆ°Æ¡ng {index + 1} (lá»—i hoáº·c khÃ´ng cÃ³ dá»¯ liá»‡u)")

        safe_print(f"âœ… ÄÃ£ hoÃ n thÃ nh {len(fiction_data['chapters'])}/{len(chapter_urls)} chÆ°Æ¡ng (theo Ä‘Ãºng thá»© tá»±)")

        # 5. LÆ°u káº¿t quáº£ ra JSON
        self._save_to_json(fiction_data)

    def _get_all_chapters_from_pagination(self, fiction_url):
        """
        Láº¥y táº¥t cáº£ chapters tá»« táº¥t cáº£ cÃ¡c trang phÃ¢n trang
        Pagination sá»­ dá»¥ng JavaScript (AJAX), khÃ´ng Ä‘á»•i URL
        Tráº£ vá» danh sÃ¡ch URL cá»§a táº¥t cáº£ chapters
        """
        all_chapter_urls = []
        
        try:
            # Trang Ä‘áº§u tiÃªn: Láº¥y tá»« trang fiction chÃ­nh
            safe_print(f"    ğŸ“„ Äang láº¥y chapters tá»« trang 1 (trang fiction chÃ­nh)...")
            self.page.goto(fiction_url, timeout=config.TIMEOUT)
            time.sleep(2)
            
            # Láº¥y chapters tá»« trang fiction chÃ­nh
            page_chapters = self._get_chapters_from_current_page()
            all_chapter_urls.extend(page_chapters)
            safe_print(f"    âœ… Trang 1: Láº¥y Ä‘Æ°á»£c {len(page_chapters)} chapters")
            
            # TÃ¬m sá»‘ trang tá»‘i Ä‘a cho chapters tá»« pagination trÃªn trang fiction chÃ­nh
            max_page = self._get_max_chapter_page()
            
            # Náº¿u chá»‰ cÃ³ 1 trang, return luÃ´n
            if max_page <= 1:
                safe_print(f"    ğŸ“š Chá»‰ cÃ³ 1 trang chapters")
                return all_chapter_urls
            
            safe_print(f"    ğŸ“š TÃ¬m tháº¥y {max_page} trang chapters (trang 1 Ä‘Ã£ láº¥y, cÃ²n {max_page - 1} trang ná»¯a)")
            
            # Loop qua tá»«ng trang cÃ²n láº¡i (tá»« trang 2 trá»Ÿ Ä‘i)
            # Sá»­ dá»¥ng click vÃ o pagination Ä‘á»ƒ load thÃªm chapters (AJAX, khÃ´ng Ä‘á»•i URL)
            for page_num in range(2, max_page + 1):
                safe_print(f"    ğŸ“„ Äang láº¥y chapters tá»« trang {page_num}/{max_page}...")
                
                # Click vÃ o nÃºt pagination Ä‘á»ƒ chuyá»ƒn trang (AJAX load, khÃ´ng Ä‘á»•i URL)
                if not self._go_to_chapter_page(page_num):
                    safe_print(f"    âš ï¸ KhÃ´ng thá»ƒ chuyá»ƒn Ä‘áº¿n trang {page_num}, dá»«ng láº¡i")
                    break
                
                # Äá»£i AJAX load xong
                time.sleep(2)
                
                # Láº¥y chapters tá»« trang hiá»‡n táº¡i
                page_chapters = self._get_chapters_from_current_page()
                all_chapter_urls.extend(page_chapters)
                
                safe_print(f"    âœ… Trang {page_num}: Láº¥y Ä‘Æ°á»£c {len(page_chapters)} chapters")
                
                # Delay giá»¯a cÃ¡c trang
                if page_num < max_page:
                    time.sleep(1)
            
            return all_chapter_urls
            
        except Exception as e:
            safe_print(f"    âš ï¸ Lá»—i khi láº¥y chapters tá»« pagination: {e}")
            # Fallback: Láº¥y tá»« trang Ä‘áº§u tiÃªn (trang fiction chÃ­nh)
            try:
                self.page.goto(fiction_url, timeout=config.TIMEOUT)
                time.sleep(2)
                return self._get_chapters_from_current_page()
            except:
                return []

    def _get_max_chapter_page(self):
        """Láº¥y sá»‘ trang chapters tá»‘i Ä‘a tá»« pagination"""
        try:
            # Scroll xuá»‘ng Ä‘á»ƒ load pagination
            self.page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
            time.sleep(2)
            
            max_page = 1  # Máº·c Ä‘á»‹nh lÃ  1 trang
            
            # TÃ¬m pagination element - cÃ³ thá»ƒ lÃ  pagination-small hoáº·c pagination
            pagination_selectors = [
                "ul.pagination-small",
                "ul.pagination",
                ".pagination-small",
                ".pagination"
            ]
            
            pagination = None
            for selector in pagination_selectors:
                try:
                    pagination = self.page.locator(selector).first
                    if pagination.count() > 0:
                        break
                except:
                    continue
            
            if pagination and pagination.count() > 0:
                # Láº¥y táº¥t cáº£ cÃ¡c link cÃ³ data-page attribute
                page_links = pagination.locator("a[data-page]").all()
                
                page_numbers = []
                for link in page_links:
                    try:
                        page_num_str = link.get_attribute("data-page")
                        if page_num_str:
                            page_num = int(page_num_str)
                            page_numbers.append(page_num)
                    except:
                        continue
                
                # Náº¿u khÃ´ng cÃ³ data-page, thá»­ láº¥y tá»« text content
                if not page_numbers:
                    try:
                        all_links = pagination.locator("a").all()
                        for link in all_links:
                            try:
                                link_text = link.inner_text().strip()
                                # Bá» qua cÃ¡c nÃºt navigation (Next, Previous) vÃ  icon
                                if link_text.isdigit():
                                    page_num = int(link_text)
                                    page_numbers.append(page_num)
                            except:
                                continue
                    except:
                        pass
                
                if page_numbers:
                    max_page = max(page_numbers)
                    safe_print(f"        ğŸ“„ TÃ¬m tháº¥y {max_page} trang chapters")
                else:
                    # Náº¿u khÃ´ng tÃ¬m tháº¥y sá»‘ trang, cÃ³ thá»ƒ chá»‰ cÃ³ 1 trang
                    safe_print(f"        ğŸ“„ KhÃ´ng tÃ¬m tháº¥y pagination, giáº£ sá»­ cÃ³ 1 trang")
            
            return max_page
        except Exception as e:
            safe_print(f"        âš ï¸ Lá»—i khi láº¥y sá»‘ trang chapters: {e}")
            return 1

    def _get_chapter_page_urls(self, base_url, max_page):
        """Láº¥y táº¥t cáº£ URL cá»§a cÃ¡c trang chapters tá»« pagination"""
        page_urls = [base_url]  # Trang 1 lÃ  base_url
        
        try:
            # TÃ¬m pagination
            pagination_selectors = [
                "ul.pagination-small",
                "ul.pagination",
                ".pagination-small",
                ".pagination"
            ]
            
            pagination = None
            for selector in pagination_selectors:
                try:
                    pagination = self.page.locator(selector).first
                    if pagination.count() > 0:
                        break
                except:
                    continue
            
            if pagination and pagination.count() > 0:
                # Láº¥y táº¥t cáº£ cÃ¡c link cÃ³ data-page attribute
                page_links = pagination.locator("a[data-page]").all()
                
                url_map = {}  # {page_num: url}
                for link in page_links:
                    try:
                        page_num_str = link.get_attribute("data-page")
                        if page_num_str:
                            page_num = int(page_num_str)
                            href = link.get_attribute("href")
                            if href:
                                # Táº¡o full URL
                                if href.startswith("/"):
                                    full_url = config.BASE_URL + href
                                elif href.startswith("http"):
                                    full_url = href
                                else:
                                    full_url = config.BASE_URL + "/" + href
                                url_map[page_num] = full_url
                    except:
                        continue
                
                # Sáº¯p xáº¿p vÃ  thÃªm vÃ o list
                for page_num in sorted(url_map.keys()):
                    if page_num <= max_page:
                        page_urls.append(url_map[page_num])
            
        except Exception as e:
            safe_print(f"        âš ï¸ Lá»—i khi láº¥y URLs tá»« pagination: {e}")
        
        return page_urls

    def _go_to_chapter_page(self, page_num):
        """
        Chuyá»ƒn Ä‘áº¿n trang chapters cá»¥ thá»ƒ báº±ng cÃ¡ch click vÃ o link hoáº·c nÃºt Next
        Tráº£ vá» True náº¿u thÃ nh cÃ´ng, False náº¿u tháº¥t báº¡i
        """
        try:
            # TÃ¬m pagination
            pagination_selectors = [
                "ul.pagination-small",
                "ul.pagination",
                ".pagination-small",
                ".pagination"
            ]
            
            pagination = None
            for selector in pagination_selectors:
                try:
                    pagination = self.page.locator(selector).first
                    if pagination.count() > 0:
                        break
                except:
                    continue
            
            if not pagination or pagination.count() == 0:
                return False
            
            # CÃ¡ch 1: Thá»­ tÃ¬m link cÃ³ data-page = page_num
            try:
                page_link = pagination.locator(f'a[data-page="{page_num}"]').first
                if page_link.count() > 0:
                    page_link.click()
                    time.sleep(2)
                    return True
            except:
                pass
            
            # CÃ¡ch 2: Náº¿u khÃ´ng cÃ³ data-page, thá»­ tÃ¬m link cÃ³ text = page_num
            # Láº¥y táº¥t cáº£ cÃ¡c link trong pagination vÃ  tÃ¬m link cÃ³ text = page_num
            try:
                all_links = pagination.locator("a").all()
                for link in all_links:
                    try:
                        link_text = link.inner_text().strip()
                        # Kiá»ƒm tra xem text cÃ³ pháº£i lÃ  sá»‘ vÃ  báº±ng page_num khÃ´ng
                        if link_text.isdigit() and int(link_text) == page_num:
                            # Kiá»ƒm tra xem khÃ´ng pháº£i lÃ  nÃºt navigation (khÃ´ng cÃ³ class nav-arrow)
                            parent_class = link.evaluate("el => el.closest('li')?.className || ''")
                            if "nav-arrow" not in parent_class:
                                link.click()
                                time.sleep(2)
                                return True
                    except:
                        continue
            except:
                pass
            
            # CÃ¡ch 3: Click nÃºt "Next" nhiá»u láº§n (chá»‰ dÃ¹ng náº¿u page_num nhá»)
            # TÃ¬m nÃºt Next (cÃ³ class nav-arrow hoáº·c chá»©a icon chevron-right)
            if page_num <= 10:  # Giá»›i háº¡n Ä‘á»ƒ trÃ¡nh click quÃ¡ nhiá»u
                # TÃ¬m trang hiá»‡n táº¡i
                current_page = 1
                try:
                    active_page = pagination.locator("li.page-active a").first
                    if active_page.count() > 0:
                        active_text = active_page.inner_text().strip()
                        if active_text.isdigit():
                            current_page = int(active_text)
                except:
                    pass
                
                # Click Next cho Ä‘áº¿n khi Ä‘áº¿n trang cáº§n
                while current_page < page_num:
                    # TÃ¬m nÃºt Next (cÃ³ thá»ƒ lÃ  .nav-arrow vá»›i icon chevron-right)
                    next_selectors = [
                        'a.pagination-button:has(i.fa-chevron-right)',
                        '.nav-arrow a:has(i.fa-chevron-right)',
                        'a:has(i.fa-chevron-right)',
                        '.nav-arrow a',
                        'a.pagination-button'
                    ]
                    
                    next_button = None
                    for selector in next_selectors:
                        try:
                            next_button = pagination.locator(selector).last  # Láº¥y nÃºt cuá»‘i (Next)
                            if next_button.count() > 0:
                                # Kiá»ƒm tra xem cÃ³ pháº£i nÃºt Next khÃ´ng (khÃ´ng pháº£i Previous)
                                href = next_button.get_attribute("href") or ""
                                if "page" in href.lower() or "next" in href.lower() or not href:
                                    break
                        except:
                            continue
                    
                    if next_button and next_button.count() > 0:
                        try:
                            next_button.click()
                            time.sleep(2)
                            current_page += 1
                        except:
                            return False
                    else:
                        return False
                
                return True
            
            return False
            
        except Exception as e:
            safe_print(f"        âš ï¸ Lá»—i khi chuyá»ƒn Ä‘áº¿n trang {page_num}: {e}")
            return False

    def _get_chapters_from_current_page(self):
        """Láº¥y danh sÃ¡ch chapters tá»« trang hiá»‡n táº¡i"""
        chapter_urls = []
        
        try:
            # Láº¥y táº¥t cáº£ cÃ¡c rows trong table chapters
            chapter_rows = self.page.locator("table#chapters tbody tr").all()
            
            for row in chapter_rows:
                try:
                    link_el = row.locator("td").first.locator("a")
                    if link_el.count() > 0:
                        url = link_el.get_attribute("href")
                        if url:
                            # Táº¡o full URL
                            if url.startswith("/"):
                                full_url = config.BASE_URL + url
                            elif url.startswith("http"):
                                full_url = url
                            else:
                                full_url = config.BASE_URL + "/" + url
                            
                            # TrÃ¡nh duplicate
                            if full_url not in chapter_urls:
                                chapter_urls.append(full_url)
                except:
                    continue
            
            return chapter_urls
            
        except Exception as e:
            safe_print(f"        âš ï¸ Lá»—i khi láº¥y chapters tá»« trang hiá»‡n táº¡i: {e}")
            return []

    def _convert_html_to_formatted_text(self, html_content):
        """
        Chuyá»ƒn Ä‘á»•i HTML sang text vá»›i Ä‘á»‹nh dáº¡ng Ä‘Ãºng (giá»¯ nguyÃªn xuá»‘ng dÃ²ng nhÆ° trong UI)
        - Má»—i tháº» <p> = má»™t Ä‘oáº¡n vÄƒn, cÃ¡c Ä‘oáº¡n cÃ¡ch nhau báº±ng má»™t dÃ²ng trá»‘ng
        - Tháº» <br> = xuá»‘ng dÃ²ng
        - Giá»¯ nguyÃªn cáº¥u trÃºc nhÆ° trong UI
        """
        if not html_content:
            return ""
        
        import html as html_module
        
        # Decode HTML entities trÆ°á»›c
        html_content = html_module.unescape(html_content)
        
        # Xá»­ lÃ½ theo thá»© tá»± Ä‘á»ƒ Ä‘áº£m báº£o Ä‘á»‹nh dáº¡ng Ä‘Ãºng
        text = html_content
        
        # 1. Xá»­ lÃ½ <br> vÃ  <br/> trÆ°á»›c - xuá»‘ng dÃ²ng ngay láº­p tá»©c
        text = re.sub(r'<br\s*/?>', '\n', text, flags=re.IGNORECASE)
        
        # 2. Xá»­ lÃ½ cÃ¡c tháº» block: <p> - má»—i Ä‘oáº¡n vÄƒn cÃ¡ch nhau 1 dÃ²ng trá»‘ng
        # Thay tháº¿ </p> thÃ nh dáº¥u phÃ¢n cÃ¡ch Ä‘oáº¡n (2 dÃ²ng xuá»‘ng)
        text = re.sub(r'</p>', '\n\n', text, flags=re.IGNORECASE)
        # XÃ³a tháº» má»Ÿ <p>
        text = re.sub(r'<p[^>]*>', '', text, flags=re.IGNORECASE)
        
        # 3. Xá»­ lÃ½ cÃ¡c tháº» block khÃ¡c: <div> - xuá»‘ng dÃ²ng
        text = re.sub(r'</div>', '\n', text, flags=re.IGNORECASE)
        text = re.sub(r'<div[^>]*>', '', text, flags=re.IGNORECASE)
        
        # 4. Xá»­ lÃ½ cÃ¡c tháº» heading (h1, h2, h3, ...) - xuá»‘ng dÃ²ng trÆ°á»›c vÃ  sau
        text = re.sub(r'</h[1-6]>', '\n\n', text, flags=re.IGNORECASE)
        text = re.sub(r'<h[1-6][^>]*>', '\n', text, flags=re.IGNORECASE)
        
        # 5. XÃ³a táº¥t cáº£ cÃ¡c tháº» HTML cÃ²n láº¡i (giá»¯ láº¡i text)
        text = re.sub(r'<[^>]+>', '', text)
        
        # 6. LÃ m sáº¡ch: xá»­ lÃ½ cÃ¡c dÃ²ng trá»‘ng vÃ  khoáº£ng tráº¯ng thá»«a
        lines = text.split('\n')
        cleaned_lines = []
        
        prev_empty = False
        for line in lines:
            # Strip cáº£ 2 bÃªn Ä‘á»ƒ loáº¡i bá» khoáº£ng tráº¯ng thá»«a (tá»« HTML indentation)
            stripped_line = line.strip()
            
            # Xá»­ lÃ½ dÃ²ng trá»‘ng
            if not stripped_line:
                # Chá»‰ thÃªm 1 dÃ²ng trá»‘ng giá»¯a cÃ¡c Ä‘oáº¡n (khÃ´ng thÃªm nhiá»u dÃ²ng trá»‘ng liÃªn tiáº¿p)
                if not prev_empty:
                    cleaned_lines.append('')
                prev_empty = True
            else:
                # Giá»¯ nguyÃªn dÃ²ng cÃ³ ná»™i dung (Ä‘Ã£ strip khoáº£ng tráº¯ng thá»«a)
                cleaned_lines.append(stripped_line)
                prev_empty = False
        
        # Loáº¡i bá» dÃ²ng trá»‘ng á»Ÿ Ä‘áº§u vÃ  cuá»‘i (nhÆ°ng giá»¯ dÃ²ng trá»‘ng giá»¯a cÃ¡c Ä‘oáº¡n)
        while cleaned_lines and not cleaned_lines[0].strip():
            cleaned_lines.pop(0)
        while cleaned_lines and not cleaned_lines[-1].strip():
            cleaned_lines.pop()
        
        result = '\n'.join(cleaned_lines)
        
        # Loáº¡i bá» khoáº£ng tráº¯ng thá»«a á»Ÿ Ä‘áº§u vÃ  cuá»‘i toÃ n bá»™ text
        # NhÆ°ng váº«n giá»¯ nguyÃªn cáº¥u trÃºc bÃªn trong (cÃ¡c dÃ²ng trá»‘ng giá»¯a Ä‘oáº¡n)
        result = result.strip()
        
        # Äáº£m báº£o khÃ´ng cÃ³ khoáº£ng tráº¯ng thá»«a á»Ÿ Ä‘áº§u má»—i dÃ²ng (tá»« HTML indentation)
        # Normalize láº¡i Ä‘á»ƒ cháº¯c cháº¯n
        if result:
            lines = result.split('\n')
            final_lines = []
            for line in lines:
                # Strip tá»«ng dÃ²ng Ä‘á»ƒ loáº¡i bá» khoáº£ng tráº¯ng thá»«a
                clean_line = line.strip()
                # Giá»¯ dÃ²ng trá»‘ng náº¿u lÃ  dÃ²ng trá»‘ng tháº­t
                if not clean_line:
                    final_lines.append('')
                else:
                    final_lines.append(clean_line)
            result = '\n'.join(final_lines).strip()
        
        return result

    def _scrape_single_chapter(self, url):
        """HÃ m con: Chá»‰ chá»‹u trÃ¡ch nhiá»‡m vÃ o 1 link chÆ°Æ¡ng vÃ  tráº£ vá» cá»¥c data cá»§a chÆ°Æ¡ng Ä‘Ã³"""
        try:
            self.page.goto(url, timeout=config.TIMEOUT)
            self.page.wait_for_selector(".chapter-inner", timeout=10000)

            title = self.page.locator("h1").first.inner_text()
            
            # Láº¥y content vá»›i Ä‘á»‹nh dáº¡ng Ä‘Ãºng (giá»¯ nguyÃªn xuá»‘ng dÃ²ng nhÆ° trong UI)
            content = ""
            try:
                content_container = self.page.locator(".chapter-inner").first
                if content_container.count() > 0:
                    # Láº¥y HTML Ä‘á»ƒ giá»¯ Ä‘á»‹nh dáº¡ng
                    html_content = content_container.inner_html()
                    # Chuyá»ƒn HTML sang text vá»›i Ä‘á»‹nh dáº¡ng Ä‘Ãºng
                    content = self._convert_html_to_formatted_text(html_content)
                else:
                    # Fallback: dÃ¹ng inner_text náº¿u khÃ´ng tÃ¬m tháº¥y
                    content = self.page.locator(".chapter-inner").inner_text()
            except Exception as e:
                safe_print(f"      âš ï¸ Lá»—i khi láº¥y content: {e}")
                content = self.page.locator(".chapter-inner").inner_text()

            # Láº¥y comments cho chapter nÃ y
            safe_print(f"      ... Äang láº¥y comments cho chÆ°Æ¡ng")
            chapter_comments = self._scrape_comments(url, "chapter")
            
            # Láº¥y chapter_id tá»« URL (vÃ­ dá»¥: /chapter/123456/ -> 123456)
            chapter_id = ""
            try:
                url_parts = url.split("/chapter/")
                if len(url_parts) > 1:
                    chapter_id = url_parts[1].split("/")[0]
            except:
                chapter_id = ""

            # Transform chapter comments theo schema má»›i
            transformed_comments = []
            for comment in chapter_comments:
                transformed_comment = {
                    "comment_id": comment.get("comment_id", ""),
                    "content_id": chapter_id,  # Chapter ID cho chapter comments
                    "comment_text": comment.get("content", ""),  # Äá»•i content â†’ comment_text
                    "time": comment.get("time", ""),
                    "user_id": comment.get("user_id", ""),
                    "parent_id": comment.get("parent_id", ""),
                    "is_root": not comment.get("parent_id"),  # Root náº¿u khÃ´ng cÃ³ parent
                    "react": 0,  # TODO: scrape
                    "replies": comment.get("replies", [])
                }
                transformed_comments.append(transformed_comment)

            return {
                "chapter_id": chapter_id,
                "story_id": "",  # TODO: pass fiction_id
                "order": 0,  # TODO: pass index
                "chapter_name": title,
                "chapter_url": url,
                "content": content,
                "published_time": "",  # TODO: scrape
                "last_updated": "",  # TODO: scrape
                "voted": 0,  # TODO: scrape
                "views": "",  # TODO: scrape
                "comments": transformed_comments
            }
        except Exception as e:
            safe_print(f"âš ï¸ Lá»—i cÃ o chÆ°Æ¡ng {url}: {e}")
            return None

    def _scrape_single_chapter_worker(self, url, index):
        """
        Worker function Ä‘á»ƒ cÃ o Má»˜T chÆ°Æ¡ng - má»—i worker cÃ³ browser instance riÃªng
        Thread-safe: Má»—i worker cÃ³ browser instance riÃªng
        
        Args:
            url: URL cá»§a chÆ°Æ¡ng cáº§n cÃ o (DUY NHáº¤T - khÃ´ng trÃ¹ng láº·p)
            index: Thá»© tá»± chÆ°Æ¡ng trong list (DUY NHáº¤T - khÃ´ng trÃ¹ng láº·p)
        """
        worker_playwright = None
        worker_browser = None
        
        try:
            # Delay Ä‘á»ƒ stagger cÃ¡c thread - trÃ¡nh táº¥t cáº£ thread báº¯t Ä‘áº§u cÃ¹ng lÃºc
            time.sleep(index * config.DELAY_THREAD_START)
            
            # Táº¡o browser instance riÃªng cho worker nÃ y
            worker_playwright = sync_playwright().start()
            worker_browser = worker_playwright.chromium.launch(headless=config.HEADLESS)
            worker_context = worker_browser.new_context()
            worker_page = worker_context.new_page()
            
            safe_print(f"    ğŸ”„ Thread-{index}: Äang cÃ o chÆ°Æ¡ng {index + 1}")
            
            # Delay trÆ°á»›c khi request Ä‘á»ƒ trÃ¡nh ban IP
            time.sleep(config.DELAY_BETWEEN_REQUESTS)
            
            # CÃ o chÆ°Æ¡ng
            worker_page.goto(url, timeout=config.TIMEOUT)
            worker_page.wait_for_selector(".chapter-inner", timeout=10000)
            
            # Delay sau khi load page
            time.sleep(config.DELAY_BETWEEN_REQUESTS)

            title = worker_page.locator("h1").first.inner_text()
            
            # Láº¥y content vá»›i Ä‘á»‹nh dáº¡ng Ä‘Ãºng
            content = ""
            try:
                content_container = worker_page.locator(".chapter-inner").first
                if content_container.count() > 0:
                    html_content = content_container.inner_html()
                    content = self._convert_html_to_formatted_text(html_content)
                else:
                    content = worker_page.locator(".chapter-inner").inner_text()
            except Exception as e:
                safe_print(f"      âš ï¸ Thread-{index}: Lá»—i khi láº¥y content: {e}")
                content = worker_page.locator(".chapter-inner").inner_text()

            # Delay trÆ°á»›c khi láº¥y comments
            time.sleep(config.DELAY_BETWEEN_REQUESTS)
            
            # Láº¥y comments cho chapter nÃ y
            safe_print(f"      ğŸ’¬ Thread-{index}: Äang láº¥y comments cho chÆ°Æ¡ng")
            # Kiá»ƒm tra xem cÃ³ pháº£i Webnovel khÃ´ng
            if "webnovel.com" in url:
                chapter_comments = self._scrape_webnovel_chapter_comments(worker_page, url)
            else:
                chapter_comments = self._scrape_comments_worker(worker_page, url, "chapter")

            # Delay sau khi hoÃ n thÃ nh chÆ°Æ¡ng
            time.sleep(config.DELAY_BETWEEN_CHAPTERS)
            
            # Láº¥y chapter_id tá»« URL (vÃ­ dá»¥: /chapter/123456/ -> 123456)
            chapter_id = ""
            try:
                url_parts = url.split("/chapter/")
                if len(url_parts) > 1:
                    chapter_id = url_parts[1].split("/")[0]
            except:
                chapter_id = ""

            # Transform chapter comments theo schema má»›i (worker)
            transformed_comments = []
            for comment in chapter_comments:
                transformed_comment = {
                    "comment_id": comment.get("comment_id", ""),
                    "content_id": chapter_id,  # Chapter ID
                    "comment_text": comment.get("content", ""),
                    "time": comment.get("time", ""),
                    "user_id": comment.get("user_id", ""),
                    "parent_id": comment.get("parent_id", ""),
                    "is_root": not comment.get("parent_id"),
                    "react": 0,  # TODO: scrape
                    "replies": comment.get("replies", [])
                }
                transformed_comments.append(transformed_comment)

            return {
                "chapter_id": chapter_id,
                "story_id": "",  # TODO: pass fiction_id
                "order": index + 1,  # Thá»© tá»± chapter
                "chapter_name": title,
                "chapter_url": url,
                "content": content,
                "published_time": "",  # TODO: scrape
                "last_updated": "",  # TODO: scrape
                "voted": 0,  # TODO: scrape
                "views": "",  # TODO: scrape
                "comments": transformed_comments
            }
            
        except Exception as e:
            safe_print(f"âš ï¸ Thread-{index}: Lá»—i cÃ o chÆ°Æ¡ng {index + 1}: {e}")
            return None
        finally:
            # ÄÃ³ng browser cá»§a worker
            if worker_browser:
                worker_browser.close()
            if worker_playwright:
                worker_playwright.stop()

    def _get_max_comment_page(self, url):
        """Láº¥y sá»‘ trang comments tá»‘i Ä‘a tá»« pagination"""
        try:
            # Äáº£m báº£o Ä‘ang á»Ÿ Ä‘Ãºng trang (trang 1 - khÃ´ng cÃ³ query comments)
            base_url = url.split('?')[0]
            current_url = self.page.url.split('?')[0]
            
            if base_url not in current_url:
                self.page.goto(base_url, timeout=config.TIMEOUT)
                time.sleep(2)
            
            # Scroll xuá»‘ng Ä‘á»ƒ load pagination
            self.page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
            time.sleep(2)
            
            max_page = 1  # Máº·c Ä‘á»‹nh lÃ  1 trang
            
            # TÃ¬m pagination element - cÃ³ thá»ƒ trong .chapter-nav hoáº·c trá»±c tiáº¿p
            pagination_selectors = [
                "ul.pagination",
                ".chapter-nav ul.pagination",
                ".pagination"
            ]
            
            pagination = None
            for selector in pagination_selectors:
                try:
                    pagination = self.page.locator(selector).first
                    if pagination.count() > 0:
                        break
                except:
                    continue
            
            if pagination and pagination.count() > 0:
                # Láº¥y táº¥t cáº£ cÃ¡c link cÃ³ data-page attribute
                page_links = pagination.locator("a[data-page]").all()
                
                page_numbers = []
                for link in page_links:
                    try:
                        page_num_str = link.get_attribute("data-page")
                        if page_num_str:
                            page_num = int(page_num_str)
                            page_numbers.append(page_num)
                    except:
                        continue
                
                # CÅ©ng thá»­ láº¥y tá»« text content (náº¿u khÃ´ng cÃ³ data-page)
                if not page_numbers:
                    try:
                        all_links = pagination.locator("a").all()
                        for link in all_links:
                            try:
                                link_text = link.inner_text().strip()
                                # Thá»­ parse sá»‘ tá»« text (vÃ­ dá»¥: "31", "Next >" sáº½ bá»‹ skip)
                                if link_text.isdigit():
                                    page_num = int(link_text)
                                    page_numbers.append(page_num)
                            except:
                                continue
                    except:
                        pass
                
                if page_numbers:
                    max_page = max(page_numbers)
                    safe_print(f"        ğŸ“„ TÃ¬m tháº¥y {max_page} trang comments")
                else:
                    # Náº¿u khÃ´ng tÃ¬m tháº¥y sá»‘ trang, cÃ³ thá»ƒ chá»‰ cÃ³ 1 trang hoáº·c chÆ°a load
                    safe_print(f"        ğŸ“„ KhÃ´ng tÃ¬m tháº¥y pagination, giáº£ sá»­ cÃ³ 1 trang")
            
            return max_page
        except Exception as e:
            safe_print(f"        âš ï¸ Lá»—i khi láº¥y sá»‘ trang: {e}")
            return 1  # Náº¿u lá»—i, máº·c Ä‘á»‹nh chá»‰ cÃ³ 1 trang

    def _scrape_comments_from_page(self, page_url):
        """Láº¥y comments tá»« má»™t trang cá»¥ thá»ƒ"""
        comments = []
        
        try:
            self.page.goto(page_url, timeout=config.TIMEOUT)
            time.sleep(2)  # Chá» page load
            
            # Scroll xuá»‘ng Ä‘á»ƒ load comments (lazy load)
            self.page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
            time.sleep(2)
            
            # Láº¥y táº¥t cáº£ div.comment vÃ  filter nhá»¯ng cÃ¡i khÃ´ng náº±m trong ul.subcomments
            all_comments = self.page.locator("div.comment").all()
            
            for comment_elem in all_comments:
                try:
                    # Kiá»ƒm tra xem comment nÃ y cÃ³ náº±m trong ul.subcomments khÃ´ng
                    is_in_subcomments = comment_elem.evaluate("""
                        el => {
                            let parent = el.parentElement;
                            while (parent) {
                                if (parent.tagName === 'UL' && parent.classList.contains('subcomments')) {
                                    return true;
                                }
                                parent = parent.parentElement;
                            }
                            return false;
                        }
                    """)
                    
                    # Náº¿u náº±m trong subcomments thÃ¬ skip (Ä‘Ã¢y lÃ  reply, sáº½ Ä‘Æ°á»£c láº¥y Ä‘á»‡ quy)
                    if is_in_subcomments:
                        continue
                    
                    # ÄÃ¢y lÃ  comment gá»‘c, láº¥y nÃ³ vÃ  táº¥t cáº£ replies
                    comment_data = self._scrape_single_comment_recursive(comment_elem)
                    if comment_data:
                        comments.append(comment_data)
                except Exception as e:
                    continue
            
            return comments
            
        except Exception as e:
            safe_print(f"        âš ï¸ Lá»—i khi láº¥y comments tá»« trang: {e}")
            return []

    def _scrape_comments(self, url, comment_type="chapter"):
        """
        Láº¥y táº¥t cáº£ comments tá»« Táº¤T Cáº¢ cÃ¡c trang phÃ¢n trang
        Tráº£ vá» danh sÃ¡ch comments vá»›i threading (comment gá»‘c + replies)
        """
        try:
            # Äáº£m báº£o Ä‘ang á»Ÿ Ä‘Ãºng trang Ä‘á»ƒ kiá»ƒm tra pagination
            current_url = self.page.url
            if url not in current_url:
                self.page.goto(url, timeout=config.TIMEOUT)
                time.sleep(2)
            
            safe_print(f"      ğŸ’¬ Äang láº¥y comments ({comment_type}-level)...")
            
            # BÆ°á»›c 1: TÃ¬m sá»‘ trang tá»‘i Ä‘a
            max_page = self._get_max_comment_page(url)
            
            all_comments = []
            
            # BÆ°á»›c 2: Láº¥y comments tá»« táº¥t cáº£ cÃ¡c trang
            for page_num in range(1, max_page + 1):
                safe_print(f"        ğŸ“„ Äang láº¥y trang {page_num}/{max_page}...")
                
                # Táº¡o URL cho trang nÃ y
                if page_num == 1:
                    # Trang 1: Loáº¡i bá» query parameter comments náº¿u cÃ³
                    base_url = url.split('?')[0]  # Láº¥y URL gá»‘c khÃ´ng cÃ³ query
                    page_url = base_url
                else:
                    # Trang khÃ¡c: ThÃªm query parameter comments=N
                    base_url = url.split('?')[0]  # Láº¥y URL gá»‘c
                    # TÃ¬m cÃ¡c query parameter hiá»‡n cÃ³ (trá»« comments)
                    if '?' in url:
                        existing_params = url.split('?', 1)[1]
                        # Loáº¡i bá» comments parameter náº¿u cÃ³
                        params_list = []
                        for param in existing_params.split('&'):
                            if not param.startswith('comments='):
                                params_list.append(param)
                        if params_list:
                            other_params = '&'.join(params_list)
                            page_url = f"{base_url}?{other_params}&comments={page_num}"
                        else:
                            page_url = f"{base_url}?comments={page_num}"
                    else:
                        page_url = f"{base_url}?comments={page_num}"
                
                # Láº¥y comments tá»« trang nÃ y
                page_comments = self._scrape_comments_from_page(page_url)
                all_comments.extend(page_comments)
                
                safe_print(f"        âœ… Trang {page_num}: Láº¥y Ä‘Æ°á»£c {len(page_comments)} comments")
                
                # Delay giá»¯a cÃ¡c trang Ä‘á»ƒ trÃ¡nh bá»‹ ban
                if page_num < max_page:
                    time.sleep(1)
            
            safe_print(f"      âœ… Tá»•ng cá»™ng láº¥y Ä‘Æ°á»£c {len(all_comments)} comments tá»« {max_page} trang ({comment_type}-level)")
            return all_comments
            
        except Exception as e:
            safe_print(f"      âš ï¸ Lá»—i khi láº¥y comments: {e}")
            return []

    def _scrape_comments_worker(self, page, url, comment_type="chapter"):
        """
        Worker function Ä‘á»ƒ láº¥y comments - dÃ¹ng page tá»« worker thay vÃ¬ self.page
        """
        try:
            current_url = page.url
            if url not in current_url:
                # Delay trÆ°á»›c khi request comments
                time.sleep(config.DELAY_BETWEEN_REQUESTS)
                page.goto(url, timeout=config.TIMEOUT)
                time.sleep(2)
            
            safe_print(f"      ğŸ’¬ Äang láº¥y comments ({comment_type}-level)...")
            
            # Delay trÆ°á»›c khi láº¥y sá»‘ trang
            time.sleep(config.DELAY_BETWEEN_REQUESTS)
            
            # TÃ¬m sá»‘ trang tá»‘i Ä‘a
            max_page = self._get_max_comment_page_worker(page, url)
            
            all_comments = []
            
            # Láº¥y comments tá»« táº¥t cáº£ cÃ¡c trang
            for page_num in range(1, max_page + 1):
                safe_print(f"        ğŸ“„ Äang láº¥y trang {page_num}/{max_page}...")
                
                # Táº¡o URL cho trang nÃ y
                if page_num == 1:
                    base_url = url.split('?')[0]
                    page_url = base_url
                else:
                    base_url = url.split('?')[0]
                    if '?' in url:
                        existing_params = url.split('?', 1)[1]
                        params_list = []
                        for param in existing_params.split('&'):
                            if not param.startswith('comments='):
                                params_list.append(param)
                        if params_list:
                            other_params = '&'.join(params_list)
                            page_url = f"{base_url}?{other_params}&comments={page_num}"
                        else:
                            page_url = f"{base_url}?comments={page_num}"
                    else:
                        page_url = f"{base_url}?comments={page_num}"
                
                # Delay trÆ°á»›c khi request trang comments
                if page_num > 1:
                    time.sleep(config.DELAY_BETWEEN_REQUESTS)
                
                # Láº¥y comments tá»« trang nÃ y
                page_comments = self._scrape_comments_from_page_worker(page, page_url)
                all_comments.extend(page_comments)
                
                safe_print(f"        âœ… Trang {page_num}: Láº¥y Ä‘Æ°á»£c {len(page_comments)} comments")
                
                # Delay giá»¯a cÃ¡c trang comments
                if page_num < max_page:
                    time.sleep(config.DELAY_BETWEEN_REQUESTS)
            
            safe_print(f"      âœ… Tá»•ng cá»™ng láº¥y Ä‘Æ°á»£c {len(all_comments)} comments tá»« {max_page} trang ({comment_type}-level)")
            return all_comments
            
        except Exception as e:
            safe_print(f"      âš ï¸ Lá»—i khi láº¥y comments: {e}")
            return []

    def _get_max_comment_page_worker(self, page, url):
        """Láº¥y sá»‘ trang comments tá»‘i Ä‘a tá»« pagination - dÃ¹ng page tá»« worker"""
        try:
            base_url = url.split('?')[0]
            current_url = page.url.split('?')[0]
            
            if base_url not in current_url:
                page.goto(base_url, timeout=config.TIMEOUT)
                time.sleep(2)
            
            page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
            time.sleep(2)
            
            max_page = 1
            
            pagination_selectors = [
                "ul.pagination",
                ".chapter-nav ul.pagination",
                ".pagination"
            ]
            
            pagination = None
            for selector in pagination_selectors:
                try:
                    pagination = page.locator(selector).first
                    if pagination.count() > 0:
                        break
                except:
                    continue
            
            if pagination and pagination.count() > 0:
                page_links = pagination.locator("a[data-page]").all()
                
                page_numbers = []
                for link in page_links:
                    try:
                        page_num_str = link.get_attribute("data-page")
                        if page_num_str:
                            page_num = int(page_num_str)
                            page_numbers.append(page_num)
                    except:
                        continue
                
                if not page_numbers:
                    try:
                        all_links = pagination.locator("a").all()
                        for link in all_links:
                            try:
                                link_text = link.inner_text().strip()
                                if link_text.isdigit():
                                    page_num = int(link_text)
                                    page_numbers.append(page_num)
                            except:
                                continue
                    except:
                        pass
                
                if page_numbers:
                    max_page = max(page_numbers)
            
            return max_page
        except Exception as e:
            safe_print(f"        âš ï¸ Lá»—i khi láº¥y sá»‘ trang: {e}")
            return 1

    def _scrape_comments_from_page_worker(self, page, page_url):
        """Láº¥y comments tá»« má»™t trang cá»¥ thá»ƒ - dÃ¹ng page tá»« worker"""
        comments = []
        
        try:
            # Delay trÆ°á»›c khi request
            time.sleep(config.DELAY_BETWEEN_REQUESTS)
            page.goto(page_url, timeout=config.TIMEOUT)
            time.sleep(2)
            
            page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
            time.sleep(2)
            
            all_comments = page.locator("div.comment").all()
            
            for comment_elem in all_comments:
                try:
                    is_in_subcomments = comment_elem.evaluate("""
                        el => {
                            let parent = el.parentElement;
                            while (parent) {
                                if (parent.tagName === 'UL' && parent.classList.contains('subcomments')) {
                                    return true;
                                }
                                parent = parent.parentElement;
                            }
                            return false;
                        }
                    """)
                    
                    if is_in_subcomments:
                        continue
                    
                    comment_data = self._scrape_single_comment_recursive(comment_elem)
                    if comment_data:
                        comments.append(comment_data)
                except Exception as e:
                    continue
            
            return comments
            
        except Exception as e:
            safe_print(f"        âš ï¸ Lá»—i khi láº¥y comments tá»« trang: {e}")
            return []

    def _scrape_webnovel_chapter_comments(self, page, chapter_url):
        """
        Scrape Táº¤T Cáº¢ comments cá»§a má»™t chapter trÃªn Webnovel
        Webnovel dÃ¹ng infinite scroll vÃ  cÃ³ nÃºt comment Ä‘á»ƒ má»Ÿ comment section
        """
        comments = []
        try:
            safe_print(f"        ğŸ’¬ Äang láº¥y Webnovel chapter comments...")
            
            # BÆ°á»›c 1: TÃ¬m vÃ  click nÃºt comment
            try:
                comment_button_selectors = [
                    "button:has-text('Comment')",
                    "button:has-text('comment')",
                    "a:has-text('Comment')",
                    ".comment-btn",
                    "button[class*='comment']",
                    "div[class*='comment-button']"
                ]
                
                comment_button = None
                for selector in comment_button_selectors:
                    try:
                        btn = page.locator(selector).first
                        if btn.count() > 0:
                            comment_button = btn
                            safe_print(f"        ğŸ”˜ TÃ¬m tháº¥y comment button: {selector}")
                            break
                    except:
                        continue
                
                if comment_button:
                    # Scroll Ä‘áº¿n button vÃ  click
                    comment_button.scroll_into_view_if_needed()
                    time.sleep(1)
                    comment_button.click()
                    safe_print(f"        âœ… ÄÃ£ click comment button")
                    time.sleep(3)  # Äá»£i comment section load
                else:
                    safe_print(f"        âš ï¸ KhÃ´ng tÃ¬m tháº¥y comment button, thá»­ scroll xuá»‘ng")
                    page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
                    time.sleep(2)
            except Exception as e:
                safe_print(f"        âš ï¸ Lá»—i khi click comment button: {e}")
            
            # BÆ°á»›c 2: Scroll infinite Ä‘á»ƒ load Táº¤T Cáº¢ comments
            safe_print(f"        ğŸ“œ Äang scroll Ä‘á»ƒ load Táº¤T Cáº¢ chapter comments...")
            previous_height = 0
            no_change_count = 0
            max_scrolls = 30  # Giá»›i háº¡n cho chapter comments (Ã­t hÆ¡n story comments)
            
            for scroll_attempt in range(max_scrolls):
                # Scroll xuá»‘ng
                page.evaluate("window.scrollBy(0, 500)")
                time.sleep(1)
                
                # Kiá»ƒm tra xem page cÃ³ tÄƒng chiá»u cao khÃ´ng
                current_height = page.evaluate("document.body.scrollHeight")
                if current_height == previous_height:
                    no_change_count += 1
                    if no_change_count >= 3:  # 3 láº§n liÃªn tiáº¿p khÃ´ng thay Ä‘á»•i -> Ä‘Ã£ háº¿t
                        safe_print(f"        âœ… ÄÃ£ scroll háº¿t chapter comments (sau {scroll_attempt + 1} láº§n)")
                        break
                else:
                    no_change_count = 0
                    previous_height = current_height
            
            # BÆ°á»›c 3: Parse comments giá»‘ng nhÆ° story comments
            # TÃ¬m cÃ¡c comment items
            page_text = page.locator("body").inner_text()
            safe_print(f"        ğŸ” Äang tÃ¬m chapter comments...")
            
            # Thá»­ tÃ¬m comment containers vá»›i nhiá»u selectors
            comment_containers = []
            comment_selectors = [
                "div[class*='comment']",
                "li[class*='comment']",
                "div[class*='review']",
                ".j_comment_list li",
                "div.comment-item"
            ]
            
            for selector in comment_selectors:
                try:
                    items = page.locator(selector).all()
                    if len(items) > 0:
                        safe_print(f"        âœ… TÃ¬m tháº¥y {len(items)} items vá»›i selector: {selector}")
                        comment_containers = items
                        break
                except:
                    continue
            
            if not comment_containers:
                safe_print(f"        âš ï¸ KhÃ´ng tÃ¬m tháº¥y comment containers")
                return []
            
            # Parse tá»«ng comment
            import hashlib
            for idx, container in enumerate(comment_containers):
                try:
                    # Láº¥y toÃ n bá»™ text
                    full_text = container.inner_text().strip()
                    if not full_text or len(full_text) < 5:
                        continue
                    
                    # TÃ¬m username (thÆ°á»ng cÃ³ link profile)
                    username = ""
                    try:
                        username_links = container.locator("a[href*='/profile'], a[href*='/user']").all()
                        if username_links:
                            username = username_links[0].inner_text().strip()
                    except:
                        pass
                    
                    if not username:
                        username = f"User_{idx}"
                    
                    # Láº¥y comment content
                    content_text = full_text
                    
                    # Filter out metadata (LV, time, etc.)
                    lines = content_text.split('\n')
                    filtered_lines = []
                    for line in lines:
                        line = line.strip()
                        if not line:
                            continue
                        # Skip metadata lines
                        if re.match(r'^LV\s+\d+', line):
                            continue
                        if line == username:
                            continue
                        if re.match(r'^\d+(s|m|h|d|w|mth|yr)$', line):
                            continue
                        filtered_lines.append(line)
                    
                    content_text = '\n'.join(filtered_lines)
                    
                    if not content_text:
                        continue
                    
                    # TÃ¬m GIF náº¿u cÃ³
                    try:
                        gif_imgs = container.locator("img[src*='.gif']").all()
                        for gif_img in gif_imgs:
                            gif_url = gif_img.get_attribute("src")
                            if gif_url and not gif_url.startswith("http"):
                                gif_url = "https:" + gif_url
                            content_text += f"\n[GIF: {gif_url}]"
                    except:
                        pass
                    
                    # Láº¥y time
                    time_text = ""
                    try:
                        time_patterns = [r'\d+s', r'\d+m', r'\d+h', r'\d+d', r'\d+w', r'\d+mth', r'\d+yr']
                        for pattern in time_patterns:
                            matches = re.findall(pattern, full_text)
                            if matches:
                                time_text = matches[0]
                                break
                    except:
                        pass
                    
                    # Generate comment_id
                    comment_id = (
                        container.get_attribute("id") or 
                        container.get_attribute("data-id") or 
                        container.get_attribute("data-comment-id") or
                        ""
                    )
                    if not comment_id:
                        comment_id = hashlib.md5(f"{username}_{time_text}_{content_text[:20]}".encode()).hexdigest()[:12]
                    
                    # Táº¡o comment object
                    comment_data = {
                        "comment_id": comment_id,
                        "content": content_text,
                        "time": time_text,
                        "user_id": username,
                        "parent_id": "",
                        "replies": []
                    }
                    
                    comments.append(comment_data)
                    
                except Exception as e:
                    safe_print(f"        âš ï¸ Lá»—i parse comment {idx}: {e}")
                    continue
            
            safe_print(f"        âœ… ÄÃ£ láº¥y {len(comments)} chapter comments")
            return comments
            
        except Exception as e:
            safe_print(f"        âš ï¸ Lá»—i khi láº¥y chapter comments: {e}")
            return []

    def _scrape_single_comment_recursive(self, comment_elem):
        """
        HÃ m Ä‘á»‡ quy Ä‘á»ƒ láº¥y má»™t comment vÃ  táº¥t cáº£ replies cá»§a nÃ³
        Cáº¥u trÃºc HTML:
        - div.comment
          - div.media.media-v2 (ná»™i dung comment chÃ­nh)
          - ul.subcomments (chá»©a cÃ¡c replies)
            - div.comment (reply, cÃ³ thá»ƒ cÃ³ ul.subcomments riÃªng)
        """
        try:
            # Láº¥y comment container (div.media.media-v2)
            media_elem = comment_elem.locator("div.media.media-v2").first
            if media_elem.count() == 0:
                return None
            
            # Láº¥y comment ID tá»« id attribute
            comment_id = media_elem.get_attribute("id") or ""
            if comment_id.startswith("comment-container-"):
                comment_id = comment_id.replace("comment-container-", "")
            
            # Láº¥y username - theo cáº¥u trÃºc HTML: h4.media-heading > span.name > strong > a
            username = ""
            try:
                # Cáº¥u trÃºc: h4.media-heading > span.name > a[href*='/profile/']
                username_selectors = [
                    "h4.media-heading span.name a",
                    "h4.media-heading .name a",
                    ".media-heading span.name a",
                    ".media-heading .name a[href*='/profile/']",
                    "h4.media-heading a[href*='/profile/']",
                    ".media-heading a[href*='/profile/']"
                ]
                
                for selector in username_selectors:
                    try:
                        username_elem = media_elem.locator(selector).first
                        if username_elem.count() > 0:
                            username = username_elem.inner_text().strip()
                            if username:
                                break
                    except:
                        continue
                
                # Náº¿u váº«n khÃ´ng tÃ¬m tháº¥y, thá»­ láº¥y tá»« báº¥t ká»³ link profile nÃ o trong media-heading
                if not username:
                    try:
                        username_elem = media_elem.locator(".media-heading a[href*='/profile/']").first
                        if username_elem.count() > 0:
                            username = username_elem.inner_text().strip()
                    except:
                        pass
                        
                if not username:
                    username = "[Unknown]"
            except:
                username = "[Unknown]"
            
            # Láº¥y comment text/content - láº¥y táº¥t cáº£ cÃ¡c Ä‘oáº¡n vÄƒn Ä‘á»ƒ giá»¯ format
            comment_text = ""
            try:
                media_body = media_elem.locator(".media-body").first
                if media_body.count() > 0:
                    # Láº¥y táº¥t cáº£ cÃ¡c Ä‘oáº¡n vÄƒn trong comment
                    paragraphs = media_body.locator("p").all()
                    
                    if paragraphs:
                        # Náº¿u cÃ³ nhiá»u Ä‘oáº¡n vÄƒn, ná»‘i láº¡i vá»›i xuá»‘ng dÃ²ng
                        text_parts = []
                        for para in paragraphs:
                            try:
                                para_text = para.inner_text().strip()
                                if para_text:
                                    text_parts.append(para_text)
                            except:
                                continue
                        comment_text = "\n\n".join(text_parts)
                    else:
                        # Náº¿u khÃ´ng cÃ³ tháº» p, láº¥y toÃ n bá»™ text tá»« media-body
                        full_text = media_body.inner_text().strip()
                        
                        # Loáº¡i bá» username náº¿u cÃ³ á»Ÿ Ä‘áº§u
                        if username and full_text.startswith(username):
                            comment_text = full_text[len(username):].strip()
                        else:
                            comment_text = full_text
                        
                        # Loáº¡i bá» cÃ¡c pháº§n khÃ´ng pháº£i ná»™i dung (nhÆ° timestamp, rep count)
                        # CÃ¡c pháº§n nÃ y thÆ°á»ng á»Ÿ cuá»‘i, cÃ³ thá»ƒ cÃ³ format nhÆ° "7 years ago" hoáº·c "Rep (63)"
                        lines = comment_text.split('\n')
                        cleaned_lines = []
                        for line in lines:
                            line = line.strip()
                            if not line:
                                continue
                            # Bá» qua dÃ²ng chá»©a "years ago", "Rep (", "Reply", "Report"
                            if any(x in line.lower() for x in ['years ago', 'months ago', 'days ago', 'hours ago', 
                                                                'rep (', 'reply', 'report']):
                                continue
                            cleaned_lines.append(line)
                        comment_text = '\n'.join(cleaned_lines).strip()
            except Exception as e:
                comment_text = ""
            
            # Láº¥y timestamp
            timestamp = ""
            try:
                time_elem = media_elem.locator("time, .timestamp, [class*='time'], [class*='date']").first
                if time_elem.count() > 0:
                    timestamp = time_elem.get_attribute("datetime") or time_elem.inner_text().strip()
            except:
                pass
            
            # Táº¡o cáº¥u trÃºc comment theo schema má»›i
            comment_data = {
                "comment_id": comment_id,
                "time": timestamp,
                "content": comment_text,
                "user_id": "",  # TODO: extract if available
                "story_id": "",  # Will be filled by parent
                "chapter_id": "",  # Will be filled if chapter comment
                "parent_id": "",  # Will be filled if reply
                "replies": []
            }
            
            # Láº¥y replies (subcomments) - Äá»† QUY
            try:
                subcomments_list = comment_elem.locator("ul.subcomments").first
                if subcomments_list.count() > 0:
                    # Láº¥y táº¥t cáº£ cÃ¡c comment con trong ul.subcomments
                    reply_comments = subcomments_list.locator("div.comment").all()
                    
                    for reply_elem in reply_comments:
                        reply_data = self._scrape_single_comment_recursive(reply_elem)
                        if reply_data:
                            comment_data["replies"].append(reply_data)
            except Exception as e:
                # KhÃ´ng cÃ³ replies hoáº·c lá»—i khi láº¥y
                pass
            
            return comment_data
            
        except Exception as e:
            safe_print(f"        âš ï¸ Lá»—i khi parse comment: {e}")
            return None

    def _scrape_reviews(self, fiction_url):
        """
        Láº¥y táº¥t cáº£ reviews tá»« trang fiction
        Theo scheme: review id, title, username, at chapter, time, content, score (overall, style, story, grammar, character)
        """
        reviews = []
        try:
            safe_print("      ğŸ“ Äang láº¥y reviews tá»« trang fiction...")
            
            # Äáº£m báº£o Ä‘ang á»Ÿ trang fiction
            self.page.goto(fiction_url, timeout=config.TIMEOUT)
            time.sleep(2)
            
            # Scroll xuá»‘ng Ä‘á»ƒ load reviews section
            self.page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
            time.sleep(2)
            
            # TÃ¬m reviews section - cÃ³ thá»ƒ lÃ  tab "Reviews" hoáº·c section riÃªng
            # Thá»­ tÃ¬m cÃ¡c selector phá»• biáº¿n cho reviews
            review_selectors = [
                ".review",
                ".review-item",
                ".review-container",
                "[class*='review']",
                ".rating-review"
            ]
            
            review_elements = []
            for selector in review_selectors:
                try:
                    elements = self.page.locator(selector).all()
                    if elements:
                        review_elements = elements
                        safe_print(f"      âœ… TÃ¬m tháº¥y {len(elements)} reviews vá»›i selector: {selector}")
                        break
                except:
                    continue
            
            # Náº¿u khÃ´ng tÃ¬m tháº¥y vá»›i selector thÃ´ng thÆ°á»ng, thá»­ tÃ¬m trong tabs
            if not review_elements:
                try:
                    # Thá»­ click vÃ o tab "Reviews" náº¿u cÃ³
                    reviews_tab = self.page.locator("a[href*='reviews'], button:has-text('Reviews'), .nav-tabs a:has-text('Reviews')").first
                    if reviews_tab.count() > 0:
                        reviews_tab.click()
                        time.sleep(3)
                        # Thá»­ láº¡i vá»›i cÃ¡c selector
                        for selector in review_selectors:
                            try:
                                elements = self.page.locator(selector).all()
                                if elements:
                                    review_elements = elements
                                    break
                            except:
                                continue
                except:
                    pass
            
            # Parse tá»«ng review
            for review_elem in review_elements:
                try:
                    review_data = self._parse_single_review(review_elem)
                    if review_data:
                        reviews.append(review_data)
                except Exception as e:
                    safe_print(f"        âš ï¸ Lá»—i khi parse review: {e}")
                    continue
            
            safe_print(f"      âœ… ÄÃ£ láº¥y Ä‘Æ°á»£c {len(reviews)} reviews")
            return reviews
            
        except Exception as e:
            safe_print(f"      âš ï¸ Lá»—i khi láº¥y reviews: {e}")
            return []

    def _parse_single_review(self, review_elem):
        """
        Parse má»™t review element thÃ nh dictionary theo scheme
        """
        try:
            # Láº¥y review ID
            review_id = ""
            try:
                review_id = review_elem.get_attribute("id") or review_elem.get_attribute("data-id") or ""
                if review_id.startswith("review-"):
                    review_id = review_id.replace("review-", "")
            except:
                pass
            
            # Láº¥y title
            title = ""
            try:
                title_elem = review_elem.locator("h3, h4, .review-title, [class*='title']").first
                if title_elem.count() > 0:
                    title = title_elem.inner_text().strip()
            except:
                pass
            
            # Láº¥y username
            username = ""
            try:
                username_elem = review_elem.locator("a[href*='/profile/'], .username, .reviewer-name, [class*='username']").first
                if username_elem.count() > 0:
                    username = username_elem.inner_text().strip()
            except:
                username = "[Unknown]"
            
            # Láº¥y "at chapter" - chapter mÃ  review Ä‘Æ°á»£c viáº¿t
            at_chapter = ""
            try:
                chapter_elem = review_elem.locator("a[href*='/chapter/'], .chapter-link, [class*='chapter']").first
                if chapter_elem.count() > 0:
                    at_chapter = chapter_elem.inner_text().strip()
                    # Hoáº·c láº¥y tá»« href
                    if not at_chapter:
                        href = chapter_elem.get_attribute("href") or ""
                        if "/chapter/" in href:
                            at_chapter = href.split("/chapter/")[1].split("/")[0]
            except:
                pass
            
            # Láº¥y time
            time_str = ""
            try:
                time_elem = review_elem.locator("time, .timestamp, [class*='time'], [class*='date']").first
                if time_elem.count() > 0:
                    time_str = time_elem.get_attribute("datetime") or time_elem.inner_text().strip()
            except:
                pass
            
            # Láº¥y content
            content = ""
            try:
                content_elem = review_elem.locator(".review-content, .review-text, [class*='content'], [class*='text']").first
                if content_elem.count() > 0:
                    content = content_elem.inner_text().strip()
            except:
                pass
            
            # Láº¥y scores (overall, style, story, grammar, character)
            scores = {
                "overall": "",
                "style": "",
                "story": "",
                "grammar": "",
                "character": ""
            }
            
            try:
                # TÃ¬m cÃ¡c score elements
                score_elements = review_elem.locator(".score, .rating, [class*='score'], [class*='rating']").all()
                for score_elem in score_elements:
                    try:
                        score_text = score_elem.inner_text().strip()
                        score_label = score_elem.get_attribute("data-label") or ""
                        # CÃ³ thá»ƒ parse tá»« text hoáº·c tá»« data attributes
                        if "overall" in score_label.lower() or "overall" in score_text.lower():
                            scores["overall"] = score_text
                        elif "style" in score_label.lower() or "style" in score_text.lower():
                            scores["style"] = score_text
                        elif "story" in score_label.lower() or "story" in score_text.lower():
                            scores["story"] = score_text
                        elif "grammar" in score_label.lower() or "grammar" in score_text.lower():
                            scores["grammar"] = score_text
                        elif "character" in score_label.lower() or "character" in score_text.lower():
                            scores["character"] = score_text
                    except:
                        continue
            except:
                pass
            
            # Táº¡o review data theo scheme
            review_data = {
                "review_id": review_id,
                "title": title,
                "username": username,
                "at_chapter": at_chapter,
                "time": time_str,
                "content": content,
                "score": scores
            }
            
            return review_data
            
        except Exception as e:
            safe_print(f"        âš ï¸ Lá»—i khi parse review: {e}")
            return None

    def _save_to_json(self, data):
        """
        LÆ°u dá»¯ liá»‡u vÃ o cáº£ file JSON vÃ  MongoDB (náº¿u Ä‘Æ°á»£c báº­t)
        TÃ¡ch dá»¯ liá»‡u thÃ nh nhiá»u collections: stories, chapters, comments, reviews, scores, users
        """
        # 1. LÆ°u vÃ o file JSON (luÃ´n luÃ´n)
        # Sanitize filename for Windows (remove colons, replace spaces with underscores)
        title = utils.clean_text(data.get('name', data.get('title', 'unknown')))
        title = re.sub(r'[<>:"/\\|?*]', '', title)  # Remove Windows-illegal chars
        title = re.sub(r'\s+', '_', title)  # Replace spaces with underscores
        filename = f"{data['id']}_{title}.json"
        save_path = os.path.join(config.JSON_DIR, filename)
        
        with open(save_path, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=4)
        safe_print(f"ğŸ’¾ ÄÃ£ lÆ°u dá»¯ liá»‡u vÃ o file: {save_path}")
        
        # 2. LÆ°u vÃ o MongoDB - tÃ¡ch thÃ nh nhiá»u collections
        if self.mongo_collections:
            try:
                story_id = data['id']
                
                # 2.1. LÆ°u STORY vÃ o collection "stories"
                story_data = {
                    "id": story_id,
                    "name": data.get("name", ""),
                    "url": data.get("url", ""),
                    "cover_image": data.get("cover_image", ""),
                    "author": data.get("author", ""),
                    "category": data.get("category", ""),
                    "status": data.get("status", ""),
                    "tags": data.get("tags", []),
                    "description": data.get("description", ""),
                    "stats": {
                        "views": data.get("stats", {}).get("views", {})
                    }
                }
                
                stories_col = self.mongo_collections["stories"]
                existing_story = stories_col.find_one({"id": story_id})
                if existing_story:
                    stories_col.update_one({"id": story_id}, {"$set": story_data})
                    safe_print(f"ğŸ”„ ÄÃ£ cáº­p nháº­t story trong MongoDB (ID: {story_id})")
                else:
                    stories_col.insert_one(story_data)
                    safe_print(f"âœ… ÄÃ£ lÆ°u story vÃ o MongoDB (ID: {story_id})")
                
                # 2.2. LÆ°u SCORES vÃ o collection "scores"
                if "stats" in data and "score" in data["stats"]:
                    score_data = {
                        "story_id": story_id,
                        "overall_score": data["stats"]["score"].get("overall_score", ""),
                        "style_score": data["stats"]["score"].get("style_score", ""),
                        "story_score": data["stats"]["score"].get("story_score", ""),
                        "grammar_score": data["stats"]["score"].get("grammar_score", ""),
                        "character_score": data["stats"]["score"].get("character_score", "")
                    }
                    
                    scores_col = self.mongo_collections["scores"]
                    existing_score = scores_col.find_one({"story_id": story_id})
                    if existing_score:
                        scores_col.update_one({"story_id": story_id}, {"$set": score_data})
                    else:
                        scores_col.insert_one(score_data)
                    safe_print(f"âœ… ÄÃ£ lÆ°u scores vÃ o MongoDB (story_id: {story_id})")
                
                # 2.3. LÆ°u CHAPTERS vÃ o collection "chapters"
                chapters_col = self.mongo_collections["chapters"]
                chapters = data.get("chapters", [])
                chapters_saved = 0
                for chapter in chapters:
                    chapter_data = {
                        "id": chapter.get("id", ""),
                        "story_id": story_id,
                        "name": chapter.get("name", ""),
                        "url": chapter.get("url", ""),
                        "content": chapter.get("content", "")
                    }
                    
                    chapter_id = chapter_data["id"]
                    if chapter_id:
                        existing_chapter = chapters_col.find_one({"id": chapter_id, "story_id": story_id})
                        if existing_chapter:
                            chapters_col.update_one(
                                {"id": chapter_id, "story_id": story_id},
                                {"$set": chapter_data}
                            )
                        else:
                            chapters_col.insert_one(chapter_data)
                        chapters_saved += 1
                        
                        # 2.4. LÆ°u COMMENTS cá»§a chapter vÃ o collection "comments"
                        chapter_comments = chapter.get("comments", [])
                        if chapter_comments:
                            self._save_comments_to_mongo(chapter_comments, story_id, chapter_id, "chapter")
                
                safe_print(f"âœ… ÄÃ£ lÆ°u {chapters_saved} chapters vÃ o MongoDB (story_id: {story_id})")
                
                # 2.5. LÆ°u REVIEWS vÃ o collection "reviews"
                reviews_col = self.mongo_collections["reviews"]
                reviews = data.get("reviews", [])
                reviews_saved = 0
                for review in reviews:
                    review_data = {
                        "review_id": review.get("review_id", ""),
                        "story_id": story_id,
                        "title": review.get("title", ""),
                        "username": review.get("username", ""),
                        "at_chapter": review.get("at_chapter", ""),
                        "time": review.get("time", ""),
                        "content": review.get("content", ""),
                        "score": review.get("score", {})
                    }
                    
                    review_id = review_data["review_id"]
                    if review_id:
                        existing_review = reviews_col.find_one({"review_id": review_id, "story_id": story_id})
                        if existing_review:
                            reviews_col.update_one(
                                {"review_id": review_id, "story_id": story_id},
                                {"$set": review_data}
                            )
                        else:
                            reviews_col.insert_one(review_data)
                        reviews_saved += 1
                        
                        # LÆ°u user tá»« review
                        username = review_data.get("username", "")
                        if username:
                            self._save_user_to_mongo(username)
                
                safe_print(f"âœ… ÄÃ£ lÆ°u {reviews_saved} reviews vÃ o MongoDB (story_id: {story_id})")
                
                # 2.6. LÆ°u vÃ o collection cÅ© Ä‘á»ƒ tÆ°Æ¡ng thÃ­ch (náº¿u cáº§n)
                if self.mongo_collection:
                    existing = self.mongo_collection.find_one({"id": story_id})
                    if existing:
                        self.mongo_collection.update_one({"id": story_id}, {"$set": data})
                    else:
                        self.mongo_collection.insert_one(data)
                
                safe_print(f"ğŸ‰ ÄÃ£ hoÃ n thÃ nh lÆ°u táº¥t cáº£ dá»¯ liá»‡u vÃ o MongoDB!")
                
            except Exception as e:
                safe_print(f"âš ï¸ Lá»—i khi lÆ°u vÃ o MongoDB: {e}")
                safe_print("   Dá»¯ liá»‡u váº«n Ä‘Æ°á»£c lÆ°u vÃ o file JSON")
                import traceback
                safe_print(traceback.format_exc())
    
    def _save_comments_to_mongo(self, comments, story_id, parent_id, parent_type="chapter"):
        """
        LÆ°u comments vÃ o MongoDB (Ä‘á»‡ quy Ä‘á»ƒ lÆ°u cáº£ replies)
        parent_type: "chapter" hoáº·c "story"
        """
        if not self.mongo_collections:
            return
        
        comments_col = self.mongo_collections["comments"]
        
        for comment in comments:
            comment_data = {
                "comment_id": comment.get("comment_id", ""),
                "story_id": story_id,
                "parent_id": parent_id,
                "parent_type": parent_type,
                "username": comment.get("username", ""),
                "comment_text": comment.get("comment_text", ""),
                "time": comment.get("time", "")
            }
            
            comment_id = comment_data["comment_id"]
            if comment_id:
                # Kiá»ƒm tra xem Ä‘Ã£ cÃ³ comment nÃ y chÆ°a (thÃªm parent_type Ä‘á»ƒ cháº¯c cháº¯n)
                existing = comments_col.find_one({
                    "comment_id": comment_id,
                    "story_id": story_id,
                    "parent_id": parent_id,
                    "parent_type": parent_type
                })
                
                if existing:
                    comments_col.update_one(
                        {"comment_id": comment_id, "story_id": story_id, "parent_id": parent_id, "parent_type": parent_type},
                        {"$set": comment_data}
                    )
                else:
                    comments_col.insert_one(comment_data)
                
                # LÆ°u user tá»« comment
                username = comment_data.get("username", "")
                if username:
                    self._save_user_to_mongo(username)
                
                # LÆ°u replies (Ä‘á»‡ quy)
                replies = comment.get("replies", [])
                if replies:
                    self._save_comments_to_mongo(replies, story_id, comment_id, "comment")
    
    def _save_user_to_mongo(self, username):
        """
        LÆ°u user vÃ o collection "users" (chá»‰ lÆ°u username, cÃ³ thá»ƒ má»Ÿ rá»™ng sau)
        """
        if not self.mongo_collections or not username or username == "[Unknown]":
            return
        
        users_col = self.mongo_collections["users"]
        
        # Kiá»ƒm tra xem user Ä‘Ã£ tá»“n táº¡i chÆ°a
        existing_user = users_col.find_one({"username": username})
        if not existing_user:
            user_data = {
                "username": username,
                "created_at": time.time()  # Timestamp khi táº¡o
            }
            users_col.insert_one(user_data)