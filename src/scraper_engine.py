import time
import json
import os
import re
import sys
from concurrent.futures import ThreadPoolExecutor, as_completed
from urllib.parse import urljoin
from playwright.sync_api import sync_playwright
from src import config, utils

# Import MongoDB
try:
    from pymongo import MongoClient
    MONGODB_AVAILABLE = True
except ImportError:
    MONGODB_AVAILABLE = False

# Helper function ƒë·ªÉ print an to√†n v·ªõi encoding UTF-8
def safe_print(*args, **kwargs):
    """Print function an to√†n v·ªõi encoding UTF-8 tr√™n Windows"""
    try:
        # Th·ª≠ print b√¨nh th∆∞·ªùng
        print(*args, **kwargs)
    except UnicodeEncodeError:
        # N·∫øu l·ªói encoding, encode l·∫°i th√†nh ASCII-safe
        message = ' '.join(str(arg) for arg in args)
        # Thay th·∫ø emoji v√† k√Ω t·ª± ƒë·∫∑c bi·ªát
        message = message.encode('ascii', 'replace').decode('ascii')
        print(message, **kwargs)

class ScribbleHubScraper:
    def __init__(self, max_workers=None):
        self.browser = None
        self.context = None
        self.page = None
        self.playwright = None
        self.max_workers = max_workers or config.MAX_WORKERS
        
        # Kh·ªüi t·∫°o MongoDB client n·∫øu ƒë∆∞·ª£c b·∫≠t
        self.mongo_client = None
        self.mongo_db = None
        # Kh·ªüi t·∫°o c√°c collections ri√™ng bi·ªát
        self.mongo_collections = {}
        if config.MONGODB_ENABLED and MONGODB_AVAILABLE:
            try:
                self.mongo_client = MongoClient(config.MONGODB_URI)
                self.mongo_db = self.mongo_client[config.MONGODB_DB_NAME]
                # Kh·ªüi t·∫°o t·∫•t c·∫£ c√°c collections
                self.mongo_collections = {
                    "stories": self.mongo_db[config.MONGODB_COLLECTION_STORIES],
                    "chapters": self.mongo_db[config.MONGODB_COLLECTION_CHAPTERS],
                    "comments": self.mongo_db[config.MONGODB_COLLECTION_COMMENTS],
                    "reviews": self.mongo_db[config.MONGODB_COLLECTION_REVIEWS],
                    "scores": self.mongo_db[config.MONGODB_COLLECTION_SCORES],
                    "users": self.mongo_db[config.MONGODB_COLLECTION_USERS],
                }
                # Gi·ªØ l·∫°i collection c≈© ƒë·ªÉ t∆∞∆°ng th√≠ch
                self.mongo_collection = self.mongo_db[config.MONGODB_COLLECTION_FICTIONS]
                safe_print("‚úÖ ƒê√£ k·∫øt n·ªëi MongoDB v·ªõi c√°c collections: stories, chapters, comments, reviews, scores, users")
            except Exception as e:
                safe_print(f"‚ö†Ô∏è Kh√¥ng th·ªÉ k·∫øt n·ªëi MongoDB: {e}")
                safe_print("   Ti·∫øp t·ª•c l∆∞u v√†o file JSON...")
                self.mongo_client = None

    def start(self):
        """Kh·ªüi ƒë·ªông tr√¨nh duy·ªát"""
        self.playwright = sync_playwright().start()
        self.browser = self.playwright.chromium.launch(
            headless=config.HEADLESS,
            args=['--disable-blink-features=AutomationControlled']  # ·∫®n automation flags
        )
        
        # T·∫°o context v·ªõi user agent v√† headers th·∫≠t ƒë·ªÉ tr√°nh bot detection
        self.context = self.browser.new_context(
            user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            viewport={'width': 1920, 'height': 1080},
            locale='en-US',
            extra_http_headers={
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
                'Accept-Language': 'en-US,en;q=0.9',
                'Accept-Encoding': 'gzip, deflate, br',
                'Connection': 'keep-alive',
                'Upgrade-Insecure-Requests': '1',
                'Sec-Fetch-Dest': 'document',
                'Sec-Fetch-Mode': 'navigate',
                'Sec-Fetch-Site': 'none',
                'Cache-Control': 'max-age=0',
            }
        )
        self.page = self.context.new_page()
        
        # ·∫®n webdriver property ƒë·ªÉ tr√°nh bot detection
        self.page.add_init_script("""
            Object.defineProperty(navigator, 'webdriver', {
                get: () => undefined
            });
        """)
        
        safe_print("‚úÖ Bot ƒë√£ kh·ªüi ƒë·ªông!")

    def stop(self):
        """ƒê√≥ng tr√¨nh duy·ªát v√† MongoDB connection"""
        if self.browser:
            self.browser.close()
        if self.playwright:
            self.playwright.stop()
        if self.mongo_client:
            self.mongo_client.close()
            safe_print("‚úÖ ƒê√£ ƒë√≥ng k·∫øt n·ªëi MongoDB")
        safe_print("zzz Bot ƒë√£ t·∫Øt.")

    def scrape_best_rated_fictions(self, best_rated_url, num_fictions=10, start_from=0):
        """
        C√†o nhi·ªÅu b·ªô truy·ªán t·ª´ trang best-rated
        Args:
            best_rated_url: URL trang best-rated
            num_fictions: S·ªë l∆∞·ª£ng b·ªô truy·ªán mu·ªën c√†o (m·∫∑c ƒë·ªãnh 10)
            start_from: B·∫Øt ƒë·∫ßu t·ª´ v·ªã tr√≠ th·ª© m·∫•y (0 = b·ªô ƒë·∫ßu ti√™n, 5 = b·ªè qua 5 b·ªô ƒë·∫ßu)
        """
        safe_print(f"üìö ƒêang truy c·∫≠p trang best-rated: {best_rated_url}")
        self.page.goto(best_rated_url, timeout=config.TIMEOUT)
        
        # ƒê·ª£i page load ho√†n to√†n (quan tr·ªçng cho ScribbleHub)
        try:
            self.page.wait_for_load_state("networkidle", timeout=30000)
            safe_print("   ‚úÖ Page ƒë√£ load xong (networkidle)")
        except:
            # N·∫øu networkidle timeout, ƒë·ª£i domcontentloaded
            try:
                self.page.wait_for_load_state("domcontentloaded", timeout=10000)
                safe_print("   ‚úÖ Page ƒë√£ load xong (domcontentloaded)")
            except:
                safe_print("   ‚ö†Ô∏è Page load timeout, ti·∫øp t·ª•c...")
        
        time.sleep(3)  # ƒê·ª£i th√™m ƒë·ªÉ JavaScript render xong
        
        # L·∫•y danh s√°ch c√°c b·ªô truy·ªán t·ª´ trang best-rated
        if start_from > 0:
            safe_print(f"üîç ƒêang l·∫•y danh s√°ch {num_fictions} b·ªô truy·ªán (b·∫Øt ƒë·∫ßu t·ª´ v·ªã tr√≠ {start_from + 1})...")
        else:
            safe_print(f"üîç ƒêang l·∫•y danh s√°ch {num_fictions} b·ªô truy·ªán ƒë·∫ßu ti√™n...")
        fiction_urls = self._get_fiction_urls_from_best_rated(num_fictions, start_from)
        
        if not fiction_urls:
            safe_print("‚ùå Kh√¥ng t√¨m th·∫•y b·ªô truy·ªán n√†o!")
            return
        
        safe_print(f"‚úÖ ƒê√£ t√¨m th·∫•y {len(fiction_urls)} b·ªô truy·ªán:")
        for i, url in enumerate(fiction_urls, 1):
            safe_print(f"   {i}. {url}")
        
        # C√†o t·ª´ng b·ªô truy·ªán tu·∫ßn t·ª±
        for index, fiction_url in enumerate(fiction_urls, 1):
            safe_print(f"\n{'='*60}")
            safe_print(f"üìñ B·∫Øt ƒë·∫ßu c√†o b·ªô truy·ªán {index}/{len(fiction_urls)}")
            safe_print(f"{'='*60}")
            try:
                self.scrape_fiction(fiction_url)
                safe_print(f"‚úÖ Ho√†n th√†nh b·ªô truy·ªán {index}/{len(fiction_urls)}")
            except Exception as e:
                safe_print(f"‚ùå L·ªói khi c√†o b·ªô truy·ªán {index}: {e}")
                continue
            
            # Delay gi·ªØa c√°c b·ªô truy·ªán
            if index < len(fiction_urls):
                safe_print(f"‚è≥ Ngh·ªâ {config.DELAY_BETWEEN_CHAPTERS * 2} gi√¢y tr∆∞·ªõc khi c√†o b·ªô ti·∫øp theo...")
                time.sleep(config.DELAY_BETWEEN_CHAPTERS * 2)
        
        safe_print(f"\n{'='*60}")
        safe_print(f"üéâ ƒê√£ ho√†n th√†nh c√†o {len(fiction_urls)} b·ªô truy·ªán!")
        safe_print(f"{'='*60}")

    def _get_fiction_urls_from_best_rated(self, num_fictions=10, start_from=0):
        """
        L·∫•y danh s√°ch URL c·ªßa c√°c b·ªô truy·ªán t·ª´ trang best-rated (ScribbleHub)
        Selector: div.search_main_box .search_title a
        Args:
            num_fictions: S·ªë l∆∞·ª£ng b·ªô truy·ªán mu·ªën l·∫•y
            start_from: B·∫Øt ƒë·∫ßu t·ª´ v·ªã tr√≠ th·ª© m·∫•y (0 = b·ªô ƒë·∫ßu ti√™n)
        """
        fiction_urls = []
        
        try:
            self.page.wait_for_load_state("networkidle", timeout=20000)
            
            # L·∫•y t·∫•t c·∫£ c√°c link truy·ªán t·ª´ ScribbleHub ranking page
            cards = self.page.locator("div.search_main_box .search_title a").all()
            
            if not cards:
                safe_print("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y link truy·ªán n√†o v·ªõi selector div.search_main_box .search_title a")
                return []
            
            safe_print(f"‚úÖ T√¨m th·∫•y {len(cards)} links truy·ªán")
            
            # L·∫•y URLs v√† c·∫Øt theo start_from, num_fictions
            for a in cards:
                href = a.get_attribute("href")
                if href and href not in fiction_urls:
                    # Chu·∫©n h√≥a URL
                    if href.startswith("/"):
                        full_url = config.BASE_URL + href
                    elif href.startswith("http"):
                        full_url = href
                    else:
                        full_url = config.BASE_URL + "/" + href
                    fiction_urls.append(full_url)
            
            # C·∫Øt theo start_from v√† num_fictions
            start_index = start_from
            end_index = start_from + num_fictions
            selected_urls = fiction_urls[start_index:end_index]
            
            return selected_urls
            
        except Exception as e:
            safe_print(f"‚ö†Ô∏è L·ªói khi l·∫•y danh s√°ch truy·ªán t·ª´ best-rated: {e}")
            import traceback
            safe_print(traceback.format_exc())
            return []

    def scrape_fiction(self, fiction_url):
        """
        H√†m ch√≠nh ƒë·ªÉ c√†o to√†n b·ªô 1 b·ªô truy·ªán.
        Lu·ªìng ƒëi: V√†o trang truy·ªán -> L·∫•y Info -> L·∫•y List Chapter -> V√†o t·ª´ng Chapter -> L·∫•y Content.
        """
        safe_print(f"üåç ƒêang truy c·∫≠p truy·ªán: {fiction_url}")
        self.page.goto(fiction_url, timeout=config.TIMEOUT)
        
        # ƒê·ª£i page load ho√†n to√†n
        try:
            self.page.wait_for_load_state("networkidle", timeout=30000)
        except:
            try:
                self.page.wait_for_load_state("domcontentloaded", timeout=10000)
            except:
                pass
        
        time.sleep(3)  # ƒê·ª£i th√™m ƒë·ªÉ JavaScript render xong

        # 1. L·∫•y ID truy·ªán t·ª´ URL (ScribbleHub format: /series/ID/title/)
        # V√≠ d·ª•: https://www.scribblehub.com/series/664073/rebirth-of-the-nephilim/
        try:
            url_parts = fiction_url.rstrip('/').split('/')
            # T√¨m ph·∫ßn s·ªë ID (th∆∞·ªùng l√† ph·∫ßn th·ª© 4 sau /series/)
            fiction_id = ""
            for i, part in enumerate(url_parts):
                if part == "series" and i + 1 < len(url_parts):
                    fiction_id = url_parts[i + 1]
                    break
            if not fiction_id:
                # Fallback: l·∫•y t·ª´ cu·ªëi URL
                fiction_id = url_parts[-1] if url_parts else ""
        except:
            fiction_id = fiction_url.split("/")[-2] if "/" in fiction_url else ""

        # 2. L·∫•y th√¥ng tin t·ªïng quan (Metadata) - ScribbleHub
        safe_print("... ƒêang l·∫•y th√¥ng tin chung")
        
        # L·∫•y title - ScribbleHub th∆∞·ªùng d√πng h1.fic_title ho·∫∑c .fic_title
        title = ""
        try:
            title_selectors = ["h1.fic_title", ".fic_title", "h1", ".wi_fic_title"]
            for selector in title_selectors:
                try:
                    title_elem = self.page.locator(selector).first
                    if title_elem.count() > 0:
                        title = title_elem.inner_text().strip()
                        break
                except:
                    continue
        except Exception as e:
            safe_print(f"‚ö†Ô∏è L·ªói khi l·∫•y title: {e}")
        
        # L·∫•y URL ·∫£nh b√¨a r·ªìi t·∫£i v·ªÅ lu√¥n - ScribbleHub
        img_url_raw = ""
        try:
            cover_selectors = [".fic_image img", ".cover img", ".nov_cover img", "img[src*='cover']"]
            for selector in cover_selectors:
                try:
                    img_elem = self.page.locator(selector).first
                    if img_elem.count() > 0:
                        img_url_raw = img_elem.get_attribute("src")
                        if img_url_raw:
                            break
                except:
                    continue
        except Exception as e:
            safe_print(f"‚ö†Ô∏è L·ªói khi l·∫•y cover image: {e}")
        
        local_img_path = utils.download_image(img_url_raw, fiction_id) if img_url_raw else None

        # L·∫•y author - ScribbleHub
        author = ""
        try:
            author_selectors = [".auth_name_fic a", ".fic_author a", ".auth_name a", "a[href*='/profile/']"]
            for selector in author_selectors:
                try:
                    author_elem = self.page.locator(selector).first
                    if author_elem.count() > 0:
                        author = author_elem.inner_text().strip()
                        if author:
                            break
                except:
                    continue
        except Exception as e:
            safe_print(f"‚ö†Ô∏è L·ªói khi l·∫•y author: {e}")

        # L·∫•y category/genre - ScribbleHub
        category = ""
        try:
            category_selectors = [".fic_genre", ".genre", ".search_genre a"]
            for selector in category_selectors:
                try:
                    category_elems = self.page.locator(selector).all()
                    if category_elems:
                        categories = [elem.inner_text().strip() for elem in category_elems[:3]]  # L·∫•y 3 ƒë·∫ßu ti√™n
                        category = ", ".join(categories) if categories else ""
                        if category:
                            break
                except:
                    continue
        except Exception as e:
            safe_print(f"‚ö†Ô∏è L·ªói khi l·∫•y category: {e}")

        # L·∫•y status - ScribbleHub
        status = ""
        try:
            status_selectors = [".fic_status", ".status", "[class*='status']"]
            for selector in status_selectors:
                try:
                    status_elem = self.page.locator(selector).first
                    if status_elem.count() > 0:
                        status = status_elem.inner_text().strip()
                        if status:
                            break
                except:
                    continue
        except Exception as e:
            safe_print(f"‚ö†Ô∏è L·ªói khi l·∫•y status: {e}")

        # L·∫•y tags/genres - ScribbleHub
        tags = []
        try:
            tag_selectors = [".fic_genre", ".genre", ".search_genre a", ".tags a"]
            for selector in tag_selectors:
                try:
                    tag_elems = self.page.locator(selector).all()
                    if tag_elems:
                        tags = [elem.inner_text().strip() for elem in tag_elems]
                        if tags:
                            break
                except:
                    continue
        except Exception as e:
            safe_print(f"‚ö†Ô∏è L·ªói khi l·∫•y tags: {e}")

        # L·∫•y description - ScribbleHub (trang chi ti·∫øt)
        description = ""
        try:
            # Scroll ƒë·ªÉ ƒë·∫£m b·∫£o description ƒë∆∞·ª£c load (c√≥ th·ªÉ c√≥ "more>>" c·∫ßn expand)
            self.page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
            time.sleep(1)
            
            desc_selectors = [
                ".wi_fic_description",  # With wrapper description
                ".fic_description",  # Fiction description
                ".description",  # Generic description
                ".novel_description",  # Novel description
                "[class*='description']",  # B·∫•t k·ª≥ class n√†o c√≥ 'description'
            ]
            
            for selector in desc_selectors:
                try:
                    desc_container = self.page.locator(selector).first
                    if desc_container.count() > 0:
                        # Th·ª≠ click "more>>" n·∫øu c√≥ ƒë·ªÉ expand description
                        try:
                            more_link = desc_container.locator(".morelink, [onclick*='showtext']").first
                            if more_link.count() > 0:
                                more_link.click()
                                time.sleep(1)
                        except:
                            pass
                        
                        # L·∫•y HTML ƒë·ªÉ gi·ªØ ƒë·ªãnh d·∫°ng
                        html_content = desc_container.inner_html()
                        # Chuy·ªÉn HTML sang text v·ªõi ƒë·ªãnh d·∫°ng ƒë√∫ng
                        description = self._convert_html_to_formatted_text(html_content)
                        if description:
                            safe_print(f"   ‚úÖ T√¨m th·∫•y description v·ªõi selector: {selector}")
                            break
                except:
                    continue
        except Exception as e:
            safe_print(f"‚ö†Ô∏è L·ªói khi l·∫•y description: {e}")
            description = ""

        # L·∫•y stats - ScribbleHub (trang chi ti·∫øt)
        # C·∫•u tr√∫c: .fic_stats > .st_item
        overall_score = ""
        style_score = ""
        story_score = ""
        grammar_score = ""
        character_score = ""
        
        total_views = ""
        average_views = ""
        followers = ""
        favorites = ""
        ratings = ""
        pages = ""
        
        try:
            # Scroll ƒë·ªÉ ƒë·∫£m b·∫£o stats ƒë∆∞·ª£c load
            self.page.evaluate("window.scrollTo(0, 0)")
            time.sleep(1)
            
            # T√¨m stats container
            stats_container = self.page.locator(".fic_stats").first
            if stats_container.count() > 0:
                # L·∫•y t·∫•t c·∫£ c√°c stat items
                stat_items = stats_container.locator(".st_item").all()
                
                for stat_item in stat_items:
                    try:
                        stat_text = stat_item.inner_text().strip()
                        stat_lower = stat_text.lower()
                        
                        # Parse c√°c stats
                        if "view" in stat_lower and not total_views:
                            match = re.search(r'([\d.]+[KMkm]?)\s*views?', stat_lower)
                            if match:
                                total_views = match.group(1)
                        
                        if "favorite" in stat_lower and not favorites:
                            match = re.search(r'([\d,]+)\s*favorites?', stat_lower)
                            if match:
                                favorites = match.group(1)
                        
                        if "chapter" in stat_lower and "week" not in stat_lower and not pages:
                            match = re.search(r'([\d,]+)\s*chapters?', stat_lower)
                            if match:
                                pages = match.group(1) + " Chapters"
                        
                        if "reader" in stat_lower and not followers:
                            match = re.search(r'([\d,]+)\s*readers?', stat_lower)
                            if match:
                                followers = match.group(1)
                    except:
                        continue
        except Exception as e:
            safe_print(f"‚ö†Ô∏è L·ªói khi l·∫•y stats: {e}")

        # T·∫°o c·∫•u tr√∫c d·ªØ li·ªáu t·ªïng quan sau khi ƒë√£ l·∫•y h·∫øt c√°c bi·∫øn
        # Theo scheme: fiction id, fiction name, fiction url, cover image, author, category, status, tags, description
        fiction_data = {
            "id": fiction_id,
            "name": title,  # Scheme: fiction name
            "url": fiction_url,  # Scheme: fiction url
            "cover_image": local_img_path,  # Scheme: cover image
            "author": author,
            "category": category,
            "status": status,
            "tags": tags,
            "description": description,
            "stats": {
                "score": {
                    "overall_score": overall_score,
                    "style_score": style_score,
                    "story_score": story_score,
                    "grammar_score": grammar_score,
                    "character_score": character_score,
                },
                "views": {
                    "total_views": total_views,
                    "average_views": average_views,
                    "followers": followers,
                    "favorites": favorites,
                    "ratings": ratings,
                    "page_views": pages,
                }
            },
            "reviews": [],  # S·∫Ω ƒë∆∞·ª£c ƒëi·ªÅn sau
            "chapters": []     # Chu·∫©n b·ªã c√°i m·∫£ng r·ªóng ƒë·ªÉ ch·ª©a c√°c ch∆∞∆°ng
        }

        # 3. L·∫•y danh s√°ch link ch∆∞∆°ng t·ª´ T·∫§T C·∫¢ c√°c trang TOC
        safe_print("... ƒêang l·∫•y danh s√°ch ch∆∞∆°ng t·ª´ t·∫•t c·∫£ c√°c trang TOC")
        chapter_urls = self._get_all_chapters_for_story(fiction_url)
        
        safe_print(f"--> T·ªïng c·ªông t√¨m th·∫•y {len(chapter_urls)} ch∆∞∆°ng t·ª´ t·∫•t c·∫£ c√°c trang.")

        # 3.5. L·∫•y reviews cho to√†n b·ªô truy·ªán
        safe_print("... ƒêang l·∫•y reviews cho to√†n b·ªô truy·ªán")
        reviews = self._scrape_reviews(fiction_url)
        fiction_data["reviews"] = reviews
        safe_print(f"‚úÖ ƒê√£ l·∫•y ƒë∆∞·ª£c {len(reviews)} reviews")

        # 4. C√†o c√°c ch∆∞∆°ng song song v·ªõi ThreadPoolExecutor (GI·ªÆ ƒê√öNG TH·ª® T·ª∞)
        safe_print(f"üöÄ B·∫Øt ƒë·∫ßu c√†o {len(chapter_urls)} ch∆∞∆°ng v·ªõi {self.max_workers} thread...")
        
        # T·∫°o list k·∫øt qu·∫£ c·ªë ƒë·ªãnh theo index - m·ªói index = 1 ch∆∞∆°ng
        chapter_results = [None] * len(chapter_urls)
        
        # Dictionary ƒë·ªÉ map future -> index ƒë·ªÉ bi·∫øt ch∆∞∆°ng n√†o
        future_to_index = {}
        
        # S·ª≠ d·ª•ng ThreadPoolExecutor - N√ì T·ª∞ ƒê·ªòNG PH√ÇN PH·ªêI c√¥ng vi·ªác!
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            # Submit T·∫§T C·∫¢ chapters v√†o pool - m·ªói ch∆∞∆°ng ch·ªâ submit 1 L·∫¶N
            for index, chap_url in enumerate(chapter_urls):
                future = executor.submit(self._scrape_single_chapter_worker, chap_url, index)
                future_to_index[future] = index
            
            # Thu th·∫≠p k·∫øt qu·∫£ - c√°c thread c√≥ th·ªÉ ho√†n th√†nh b·∫•t k·ª≥ l√∫c n√†o
            completed = 0
            for future in as_completed(future_to_index):
                index = future_to_index[future]  # L·∫•y index c·ªßa ch∆∞∆°ng n√†y
                try:
                    chapter_data = future.result()
                    # L∆ØU V√ÄO ƒê√öNG V·ªä TR√ç INDEX - kh√¥ng ph·∫£i append!
                    chapter_results[index] = chapter_data
                    completed += 1
                    status = "‚úÖ" if chapter_data else "‚ö†Ô∏è"
                    safe_print(f"    {status} Ho√†n th√†nh ch∆∞∆°ng {index + 1}/{len(chapter_urls)} (ƒë√£ xong {completed}/{len(chapter_urls)})")
                except Exception as e:
                    safe_print(f"    ‚ùå L·ªói khi c√†o ch∆∞∆°ng {index + 1}: {e}")
                    chapter_results[index] = None

        # SAU KHI T·∫§T C·∫¢ XONG: Th√™m v√†o fiction_data THEO ƒê√öNG TH·ª® T·ª∞
        safe_print(f"üìù S·∫Øp x·∫øp k·∫øt qu·∫£ theo ƒë√∫ng th·ª© t·ª±...")
        for index in range(len(chapter_results)):
            chapter_data = chapter_results[index]
            if chapter_data:
                fiction_data["chapters"].append(chapter_data)
            else:
                safe_print(f"    ‚ö†Ô∏è B·ªè qua ch∆∞∆°ng {index + 1} (l·ªói ho·∫∑c kh√¥ng c√≥ d·ªØ li·ªáu)")

        safe_print(f"‚úÖ ƒê√£ ho√†n th√†nh {len(fiction_data['chapters'])}/{len(chapter_urls)} ch∆∞∆°ng (theo ƒë√∫ng th·ª© t·ª±)")

        # 5. L∆∞u k·∫øt qu·∫£ ra JSON
        self._save_to_json(fiction_data)

    def _get_all_chapters_for_story(self, story_url):
        """
        V√†o truy·ªán, duy·ªát to√†n b·ªô TOC pages, tr·∫£ list chapter URLs.
        Ch·ªâ c√†o khi th·∫≠t s·ª± c√≥ ol.toc_ol trong HTML.
        """
        all_chapters = []
        
        try:
            # TOC page 1
            toc_url = story_url.rstrip("/") + "/?toc=1#content1"
            safe_print(f"    üîó V√†o TOC: {toc_url}")
            self.page.goto(toc_url, timeout=config.TIMEOUT)
            
            # ƒê·ª£i page load
            try:
                self.page.wait_for_load_state("networkidle", timeout=20000)
            except:
                self.page.wait_for_load_state("domcontentloaded", timeout=10000)
            
            # ƒê·ª£i th√™m ƒë·ªÉ JavaScript render TOC
            time.sleep(3)
            
            # Scroll xu·ªëng ƒë·ªÉ trigger lazy load v√† ƒë·∫£m b·∫£o TOC ƒë∆∞·ª£c render
            self.page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
            time.sleep(2)
            self.page.evaluate("window.scrollTo(0, 0)")
            time.sleep(1)
            
            # Debug: Ki·ªÉm tra URL v√† HTML
            safe_print(f"    Debug: ƒêang ·ªü URL: {self.page.url}")
            page_content = self.page.content()
            has_toc_ol = "toc_ol" in page_content
            safe_print(f"    Debug: C√≥ 'toc_ol' trong HTML: {has_toc_ol}")
            
            page_chapters = self._get_chapters_from_current_page()
            all_chapters.extend(page_chapters)
            safe_print(f"    ‚úÖ Trang 1: L·∫•y ƒë∆∞·ª£c {len(page_chapters)} chapters")
            
            # N·∫øu kh√¥ng t√¨m th·∫•y chapters ·ªü URL ?toc=1, th·ª≠ v√†o trang ch√≠nh
            if len(page_chapters) == 0:
                safe_print("    ‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y chapters ·ªü URL ?toc=1, th·ª≠ v√†o trang ch√≠nh...")
                self.page.goto(story_url, timeout=config.TIMEOUT)
                time.sleep(3)
                self.page.wait_for_load_state("networkidle", timeout=20000)
                # Scroll xu·ªëng ƒë·∫øn ph·∫ßn TOC
                self.page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
                time.sleep(2)
                page_chapters = self._get_chapters_from_current_page()
                if page_chapters:
                    safe_print(f"    ‚úÖ T√¨m th·∫•y {len(page_chapters)} chapters t·ª´ trang ch√≠nh")
                    all_chapters.extend(page_chapters)
            
            # C√°c trang TOC ti·∫øp theo (2,3,4...)
            pag_links = self.page.locator("#pagination-mesh-toc a.page-link").all()
            seen = set()
            
            for a in pag_links:
                href = a.get_attribute("href")
                if not href:
                    continue
                
                full = urljoin(toc_url, href)
                if full in seen:
                    continue
                seen.add(full)
                
                safe_print(f"    üîó V√†o TOC page: {full}")
                self.page.goto(full, timeout=config.TIMEOUT)
                
                # ƒê·ª£i page load
                try:
                    self.page.wait_for_load_state("networkidle", timeout=20000)
                except:
                    self.page.wait_for_load_state("domcontentloaded", timeout=10000)
                
                # ƒê·ª£i th√™m ƒë·ªÉ JavaScript render
                time.sleep(3)
                
                # Scroll ƒë·ªÉ trigger render
                self.page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
                time.sleep(2)
                
                all_chapters.extend(self._get_chapters_from_current_page())
            
            safe_print(f"    ‚úÖ T·ªïng c·ªông {len(all_chapters)} chapter URLs")
            return all_chapters
            
        except Exception as e:
            safe_print(f"    ‚ö†Ô∏è L·ªói khi l·∫•y chapters t·ª´ TOC: {e}")
            import traceback
            safe_print(traceback.format_exc())
            return []

    def _get_max_chapter_page(self):
        """L·∫•y s·ªë trang chapters t·ªëi ƒëa t·ª´ pagination (ScribbleHub: ul#pagination-mesh-toc)"""
        try:
            # Scroll xu·ªëng ƒë·ªÉ load pagination
            self.page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
            time.sleep(2)
            
            max_page = 1  # M·∫∑c ƒë·ªãnh l√† 1 trang
            
            # ScribbleHub d√πng: ul#pagination-mesh-toc v·ªõi c√°c link a.page-link
            pagination_selectors = [
                "ul#pagination-mesh-toc",  # ScribbleHub TOC pagination
                "#pagination-mesh-toc",  # Alternative
                "ul.pagination-small",
                "ul.pagination",
                ".pagination-small",
                ".pagination"
            ]
            
            pagination = None
            for selector in pagination_selectors:
                try:
                    pagination = self.page.locator(selector).first
                    if pagination.count() > 0:
                        break
                except:
                    continue
            
            if pagination and pagination.count() > 0:
                # L·∫•y t·∫•t c·∫£ c√°c link page (a.page-link cho ScribbleHub)
                page_links = pagination.locator("a.page-link, a[href*='?toc=']").all()
                
                page_numbers = []
                for link in page_links:
                    try:
                        # ScribbleHub: href="?toc=2#content1" -> extract s·ªë 2
                        href = link.get_attribute("href") or ""
                        if "?toc=" in href:
                            # Extract s·ªë t·ª´ ?toc=N
                            import re
                            match = re.search(r'\?toc=(\d+)', href)
                            if match:
                                page_num = int(match.group(1))
                                page_numbers.append(page_num)
                        
                        # Fallback: l·∫•y t·ª´ text n·∫øu l√† s·ªë
                        link_text = link.inner_text().strip()
                        if link_text.isdigit():
                            page_num = int(link_text)
                            page_numbers.append(page_num)
                    except:
                        continue
                
                # N·∫øu kh√¥ng c√≥, th·ª≠ l·∫•y t·ª´ data-page attribute
                if not page_numbers:
                    try:
                        page_links = pagination.locator("a[data-page]").all()
                        for link in page_links:
                            try:
                                page_num_str = link.get_attribute("data-page")
                                if page_num_str:
                                    page_num = int(page_num_str)
                                    page_numbers.append(page_num)
                            except:
                                continue
                    except:
                        pass
                
                if page_numbers:
                    max_page = max(page_numbers)
                    safe_print(f"        üìÑ T√¨m th·∫•y {max_page} trang chapters")
                else:
                    # N·∫øu kh√¥ng t√¨m th·∫•y s·ªë trang, c√≥ th·ªÉ ch·ªâ c√≥ 1 trang
                    safe_print(f"        üìÑ Kh√¥ng t√¨m th·∫•y pagination, gi·∫£ s·ª≠ c√≥ 1 trang")
            
            return max_page
        except Exception as e:
            safe_print(f"        ‚ö†Ô∏è L·ªói khi l·∫•y s·ªë trang chapters: {e}")
            return 1

    def _get_chapter_page_urls(self, base_url, max_page):
        """L·∫•y t·∫•t c·∫£ URL c·ªßa c√°c trang chapters t·ª´ pagination"""
        page_urls = [base_url]  # Trang 1 l√† base_url
        
        try:
            # T√¨m pagination
            pagination_selectors = [
                "ul.pagination-small",
                "ul.pagination",
                ".pagination-small",
                ".pagination"
            ]
            
            pagination = None
            for selector in pagination_selectors:
                try:
                    pagination = self.page.locator(selector).first
                    if pagination.count() > 0:
                        break
                except:
                    continue
            
            if pagination and pagination.count() > 0:
                # L·∫•y t·∫•t c·∫£ c√°c link c√≥ data-page attribute
                page_links = pagination.locator("a[data-page]").all()
                
                url_map = {}  # {page_num: url}
                for link in page_links:
                    try:
                        page_num_str = link.get_attribute("data-page")
                        if page_num_str:
                            page_num = int(page_num_str)
                            href = link.get_attribute("href")
                            if href:
                                # T·∫°o full URL
                                if href.startswith("/"):
                                    full_url = config.BASE_URL + href
                                elif href.startswith("http"):
                                    full_url = href
                                else:
                                    full_url = config.BASE_URL + "/" + href
                                url_map[page_num] = full_url
                    except:
                        continue
                
                # S·∫Øp x·∫øp v√† th√™m v√†o list
                for page_num in sorted(url_map.keys()):
                    if page_num <= max_page:
                        page_urls.append(url_map[page_num])
            
        except Exception as e:
            safe_print(f"        ‚ö†Ô∏è L·ªói khi l·∫•y URLs t·ª´ pagination: {e}")
        
        return page_urls

    def _go_to_chapter_page(self, page_num):
        """
        Chuy·ªÉn ƒë·∫øn trang chapters c·ª• th·ªÉ (ScribbleHub: d√πng URL ?toc=N#content1)
        Tr·∫£ v·ªÅ True n·∫øu th√†nh c√¥ng, False n·∫øu th·∫•t b·∫°i
        """
        try:
            # ScribbleHub d√πng URL pattern: ?toc=N#content1
            # L·∫•y base URL (b·ªè query params hi·ªán t·∫°i)
            base_url = self.page.url.split('?')[0].split('#')[0]
            toc_url = f"{base_url}?toc={page_num}#content1"
            
            # Goto URL m·ªõi (ScribbleHub s·∫Ω load AJAX)
            self.page.goto(toc_url, timeout=config.TIMEOUT)
            time.sleep(3)
            
            # ƒê·ª£i page load
            try:
                self.page.wait_for_load_state("networkidle", timeout=15000)
            except:
                pass
            
            # ƒê·ª£i TOC container xu·∫•t hi·ªán - d√πng selector c·ª• th·ªÉ cho ScribbleHub
            try:
                self.page.wait_for_selector("ol.toc_ol li.toc_w", timeout=10000)
                return True
            except:
                # Fallback: th·ª≠ click pagination link
                try:
                    pagination = self.page.locator("ul#pagination-mesh-toc").first
                    if pagination.count() > 0:
                        # T√¨m link c√≥ href ch·ª©a ?toc=page_num
                        page_link = pagination.locator(f'a[href*="?toc={page_num}"]').first
                        if page_link.count() > 0:
                            page_link.click()
                            time.sleep(3)
                            return True
                except:
                    pass
                return False
            
        except Exception as e:
            safe_print(f"        ‚ö†Ô∏è L·ªói khi chuy·ªÉn ƒë·∫øn trang {page_num}: {e}")
            return False

    def _get_chapters_from_current_page(self):
        """
        L·∫•y danh s√°ch chapter URLs t·ª´ trang TOC hi·ªán t·∫°i (layout c√≥ ol.toc_ol).
        Ch·ªâ c√†o khi th·∫≠t s·ª± c√≥ ol.toc_ol trong HTML.
        """
        chapter_urls = []
        
        try:
            # ƒê·∫£m b·∫£o page ƒë√£ load
            try:
                self.page.wait_for_load_state("networkidle", timeout=15000)
            except:
                self.page.wait_for_load_state("domcontentloaded", timeout=10000)
            
            # Scroll ƒë·ªÉ ƒë·∫£m b·∫£o TOC ƒë∆∞·ª£c render
            self.page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
            time.sleep(1)
            
            html = self.page.content()
            
            # N·∫øu kh√¥ng c√≥ toc_ol th√¨ coi nh∆∞ layout kh√°c -> b·ªè qua
            if "toc_ol" not in html:
                safe_print("        ‚ö†Ô∏è Trang n√†y kh√¥ng c√≥ TOC ki·ªÉu ol.toc_ol -> b·ªè qua")
                safe_print(f"        Debug URL: {self.page.url}")
                # Debug: Ki·ªÉm tra c√°c element li√™n quan
                try:
                    toc_table = self.page.locator("div.wi_fic_table.toc").count()
                    safe_print(f"        Debug: T√¨m th·∫•y {toc_table} div.wi_fic_table.toc")
                    toc_ol = self.page.locator("ol.toc_ol").count()
                    safe_print(f"        Debug: T√¨m th·∫•y {toc_ol} ol.toc_ol")
                except:
                    pass
                return []
            
            # L·∫•y t·∫•t c·∫£ link ch∆∞∆°ng trong TOC
            self.page.wait_for_selector("ol.toc_ol li.toc_w a.toc_a", timeout=15000)
            links = self.page.locator("ol.toc_ol li.toc_w a.toc_a").all()
            safe_print(f"        ‚úÖ T√¨m th·∫•y {len(links)} chapters tr√™n trang TOC hi·ªán t·∫°i")
            
            for el in links:
                href = el.get_attribute("href")
                if not href:
                    continue
                
                if href.startswith("http"):
                    full = href
                else:
                    full = urljoin(config.BASE_URL + "/", href.lstrip("/"))
                
                if full not in chapter_urls:
                    chapter_urls.append(full)
            
            return chapter_urls
            
        except Exception as e:
            safe_print(f"        ‚ö†Ô∏è L·ªói khi l·∫•y chapters t·ª´ trang hi·ªán t·∫°i: {e}")
            return []

    def _convert_html_to_formatted_text(self, html_content):
        """
        Chuy·ªÉn ƒë·ªïi HTML sang text v·ªõi ƒë·ªãnh d·∫°ng ƒë√∫ng (gi·ªØ nguy√™n xu·ªëng d√≤ng nh∆∞ trong UI)
        - M·ªói th·∫ª <p> = m·ªôt ƒëo·∫°n vƒÉn, c√°c ƒëo·∫°n c√°ch nhau b·∫±ng m·ªôt d√≤ng tr·ªëng
        - Th·∫ª <br> = xu·ªëng d√≤ng
        - Gi·ªØ nguy√™n c·∫•u tr√∫c nh∆∞ trong UI
        """
        if not html_content:
            return ""
        
        import html as html_module
        
        # Decode HTML entities tr∆∞·ªõc
        html_content = html_module.unescape(html_content)
        
        # X·ª≠ l√Ω theo th·ª© t·ª± ƒë·ªÉ ƒë·∫£m b·∫£o ƒë·ªãnh d·∫°ng ƒë√∫ng
        text = html_content
        
        # 1. X·ª≠ l√Ω <br> v√† <br/> tr∆∞·ªõc - xu·ªëng d√≤ng ngay l·∫≠p t·ª©c
        text = re.sub(r'<br\s*/?>', '\n', text, flags=re.IGNORECASE)
        
        # 2. X·ª≠ l√Ω c√°c th·∫ª block: <p> - m·ªói ƒëo·∫°n vƒÉn c√°ch nhau 1 d√≤ng tr·ªëng
        # Thay th·∫ø </p> th√†nh d·∫•u ph√¢n c√°ch ƒëo·∫°n (2 d√≤ng xu·ªëng)
        text = re.sub(r'</p>', '\n\n', text, flags=re.IGNORECASE)
        # X√≥a th·∫ª m·ªü <p>
        text = re.sub(r'<p[^>]*>', '', text, flags=re.IGNORECASE)
        
        # 3. X·ª≠ l√Ω c√°c th·∫ª block kh√°c: <div> - xu·ªëng d√≤ng
        text = re.sub(r'</div>', '\n', text, flags=re.IGNORECASE)
        text = re.sub(r'<div[^>]*>', '', text, flags=re.IGNORECASE)
        
        # 4. X·ª≠ l√Ω c√°c th·∫ª heading (h1, h2, h3, ...) - xu·ªëng d√≤ng tr∆∞·ªõc v√† sau
        text = re.sub(r'</h[1-6]>', '\n\n', text, flags=re.IGNORECASE)
        text = re.sub(r'<h[1-6][^>]*>', '\n', text, flags=re.IGNORECASE)
        
        # 5. X√≥a t·∫•t c·∫£ c√°c th·∫ª HTML c√≤n l·∫°i (gi·ªØ l·∫°i text)
        text = re.sub(r'<[^>]+>', '', text)
        
        # 6. L√†m s·∫°ch: x·ª≠ l√Ω c√°c d√≤ng tr·ªëng v√† kho·∫£ng tr·∫Øng th·ª´a
        lines = text.split('\n')
        cleaned_lines = []
        
        prev_empty = False
        for line in lines:
            # Strip c·∫£ 2 b√™n ƒë·ªÉ lo·∫°i b·ªè kho·∫£ng tr·∫Øng th·ª´a (t·ª´ HTML indentation)
            stripped_line = line.strip()
            
            # X·ª≠ l√Ω d√≤ng tr·ªëng
            if not stripped_line:
                # Ch·ªâ th√™m 1 d√≤ng tr·ªëng gi·ªØa c√°c ƒëo·∫°n (kh√¥ng th√™m nhi·ªÅu d√≤ng tr·ªëng li√™n ti·∫øp)
                if not prev_empty:
                    cleaned_lines.append('')
                prev_empty = True
            else:
                # Gi·ªØ nguy√™n d√≤ng c√≥ n·ªôi dung (ƒë√£ strip kho·∫£ng tr·∫Øng th·ª´a)
                cleaned_lines.append(stripped_line)
                prev_empty = False
        
        # Lo·∫°i b·ªè d√≤ng tr·ªëng ·ªü ƒë·∫ßu v√† cu·ªëi (nh∆∞ng gi·ªØ d√≤ng tr·ªëng gi·ªØa c√°c ƒëo·∫°n)
        while cleaned_lines and not cleaned_lines[0].strip():
            cleaned_lines.pop(0)
        while cleaned_lines and not cleaned_lines[-1].strip():
            cleaned_lines.pop()
        
        result = '\n'.join(cleaned_lines)
        
        # Lo·∫°i b·ªè kho·∫£ng tr·∫Øng th·ª´a ·ªü ƒë·∫ßu v√† cu·ªëi to√†n b·ªô text
        # Nh∆∞ng v·∫´n gi·ªØ nguy√™n c·∫•u tr√∫c b√™n trong (c√°c d√≤ng tr·ªëng gi·ªØa ƒëo·∫°n)
        result = result.strip()
        
        # ƒê·∫£m b·∫£o kh√¥ng c√≥ kho·∫£ng tr·∫Øng th·ª´a ·ªü ƒë·∫ßu m·ªói d√≤ng (t·ª´ HTML indentation)
        # Normalize l·∫°i ƒë·ªÉ ch·∫Øc ch·∫Øn
        if result:
            lines = result.split('\n')
            final_lines = []
            for line in lines:
                # Strip t·ª´ng d√≤ng ƒë·ªÉ lo·∫°i b·ªè kho·∫£ng tr·∫Øng th·ª´a
                clean_line = line.strip()
                # Gi·ªØ d√≤ng tr·ªëng n·∫øu l√† d√≤ng tr·ªëng th·∫≠t
                if not clean_line:
                    final_lines.append('')
                else:
                    final_lines.append(clean_line)
            result = '\n'.join(final_lines).strip()
        
        return result

    def _scrape_single_chapter(self, url):
        """H√†m con: Ch·ªâ ch·ªãu tr√°ch nhi·ªám v√†o 1 link ch∆∞∆°ng v√† tr·∫£ v·ªÅ c·ª•c data c·ªßa ch∆∞∆°ng ƒë√≥ (ScribbleHub)"""
        try:
            self.page.goto(url, timeout=config.TIMEOUT)
            time.sleep(2)
            
            # ƒê·ª£i page load - th·ª≠ nhi·ªÅu selector
            content_selectors = [".chapter-inner", ".chp_raw", ".wi_chapter_content", ".chapter_content"]
            content_loaded = False
            for selector in content_selectors:
                try:
                    self.page.wait_for_selector(selector, timeout=5000)
                    content_loaded = True
                    break
                except:
                    continue

            # L·∫•y title - ScribbleHub
            title = ""
            try:
                title_selectors = ["h1", ".chapter-title", ".chp_title", "h2.chapter-title"]
                for selector in title_selectors:
                    try:
                        title_elem = self.page.locator(selector).first
                        if title_elem.count() > 0:
                            title = title_elem.inner_text().strip()
                            break
                    except:
                        continue
            except:
                pass
            
            # L·∫•y content v·ªõi ƒë·ªãnh d·∫°ng ƒë√∫ng (ScribbleHub)
            content = ""
            try:
                content_selectors = [".chp_raw", ".wi_chapter_content", ".chapter-inner", ".chapter_content"]
                for selector in content_selectors:
                    try:
                        content_container = self.page.locator(selector).first
                        if content_container.count() > 0:
                            # L·∫•y HTML ƒë·ªÉ gi·ªØ ƒë·ªãnh d·∫°ng
                            html_content = content_container.inner_html()
                            # Chuy·ªÉn HTML sang text v·ªõi ƒë·ªãnh d·∫°ng ƒë√∫ng
                            content = self._convert_html_to_formatted_text(html_content)
                            if content:
                                break
                    except:
                        continue
                
                # Fallback: d√πng inner_text n·∫øu kh√¥ng t√¨m th·∫•y
                if not content:
                    for selector in content_selectors:
                        try:
                            content = self.page.locator(selector).first.inner_text()
                            if content:
                                break
                        except:
                            continue
            except Exception as e:
                safe_print(f"      ‚ö†Ô∏è L·ªói khi l·∫•y content: {e}")

            # L·∫•y comments cho chapter n√†y
            safe_print(f"      ... ƒêang l·∫•y comments cho ch∆∞∆°ng")
            chapter_comments = self._scrape_comments(url, "chapter")
            
            # L·∫•y chapter_id t·ª´ URL (ScribbleHub format: /read/ID/title/chapter/CHAPTER_ID/)
            chapter_id = ""
            try:
                # ScribbleHub: /read/1672529-title/chapter/2013841/
                # Chapter ID l√† s·ªë sau /chapter/
                match = re.search(r'/chapter/(\d+)/', url)
                if match:
                    chapter_id = match.group(1)
            except:
                chapter_id = ""

            return {
                "id": chapter_id,  # Scheme: chapter id
                "name": title,  # Scheme: chapter name
                "url": url,  # Scheme: chapter url
                "content": content,  # Scheme: content
                "comments": chapter_comments
            }
        except Exception as e:
            safe_print(f"‚ö†Ô∏è L·ªói c√†o ch∆∞∆°ng {url}: {e}")
            return None

    def _scrape_single_chapter_worker(self, url, index):
        """
        Worker function ƒë·ªÉ c√†o M·ªòT ch∆∞∆°ng - m·ªói worker c√≥ browser instance ri√™ng
        Thread-safe: M·ªói worker c√≥ browser instance ri√™ng
        
        Args:
            url: URL c·ªßa ch∆∞∆°ng c·∫ßn c√†o (DUY NH·∫§T - kh√¥ng tr√πng l·∫∑p)
            index: Th·ª© t·ª± ch∆∞∆°ng trong list (DUY NH·∫§T - kh√¥ng tr√πng l·∫∑p)
        """
        worker_playwright = None
        worker_browser = None
        
        try:
            # Delay ƒë·ªÉ stagger c√°c thread - tr√°nh t·∫•t c·∫£ thread b·∫Øt ƒë·∫ßu c√πng l√∫c
            time.sleep(index * config.DELAY_THREAD_START)
            
            # T·∫°o browser instance ri√™ng cho worker n√†y
            worker_playwright = sync_playwright().start()
            worker_browser = worker_playwright.chromium.launch(headless=config.HEADLESS)
            worker_context = worker_browser.new_context()
            worker_page = worker_context.new_page()
            
            safe_print(f"    üîÑ Thread-{index}: ƒêang c√†o ch∆∞∆°ng {index + 1}")
            
            # Delay tr∆∞·ªõc khi request ƒë·ªÉ tr√°nh ban IP
            time.sleep(config.DELAY_BETWEEN_REQUESTS)
            
            # C√†o ch∆∞∆°ng
            worker_page.goto(url, timeout=config.TIMEOUT)
            time.sleep(2)
            
            # ƒê·ª£i page load - th·ª≠ nhi·ªÅu selector (ScribbleHub)
            content_selectors = [".chapter-inner", ".chp_raw", ".wi_chapter_content", ".chapter_content"]
            for selector in content_selectors:
                try:
                    worker_page.wait_for_selector(selector, timeout=5000)
                    break
                except:
                    continue
            
            # Delay sau khi load page
            time.sleep(config.DELAY_BETWEEN_REQUESTS)

            # L·∫•y title - ScribbleHub
            title = ""
            try:
                title_selectors = ["h1", ".chapter-title", ".chp_title", "h2.chapter-title"]
                for selector in title_selectors:
                    try:
                        title_elem = worker_page.locator(selector).first
                        if title_elem.count() > 0:
                            title = title_elem.inner_text().strip()
                            break
                    except:
                        continue
            except:
                pass
            
            # L·∫•y content v·ªõi ƒë·ªãnh d·∫°ng ƒë√∫ng (ScribbleHub)
            content = ""
            try:
                content_selectors = [".chp_raw", ".wi_chapter_content", ".chapter-inner", ".chapter_content"]
                for selector in content_selectors:
                    try:
                        content_container = worker_page.locator(selector).first
                        if content_container.count() > 0:
                            html_content = content_container.inner_html()
                            content = self._convert_html_to_formatted_text(html_content)
                            if content:
                                break
                    except:
                        continue
                
                # Fallback
                if not content:
                    for selector in content_selectors:
                        try:
                            content = worker_page.locator(selector).first.inner_text()
                            if content:
                                break
                        except:
                            continue
            except Exception as e:
                safe_print(f"      ‚ö†Ô∏è Thread-{index}: L·ªói khi l·∫•y content: {e}")

            # Delay tr∆∞·ªõc khi l·∫•y comments
            time.sleep(config.DELAY_BETWEEN_REQUESTS)
            
            # L·∫•y comments cho chapter n√†y
            safe_print(f"      üí¨ Thread-{index}: ƒêang l·∫•y comments cho ch∆∞∆°ng")
            chapter_comments = self._scrape_comments_worker(worker_page, url, "chapter")

            # Delay sau khi ho√†n th√†nh ch∆∞∆°ng
            time.sleep(config.DELAY_BETWEEN_CHAPTERS)
            
            # L·∫•y chapter_id t·ª´ URL (ScribbleHub format: /read/ID/title/chapter/CHAPTER_ID/)
            chapter_id = ""
            try:
                # ScribbleHub: /read/1672529-title/chapter/2013841/
                # Chapter ID l√† s·ªë sau /chapter/
                match = re.search(r'/chapter/(\d+)/', url)
                if match:
                    chapter_id = match.group(1)
            except:
                chapter_id = ""

            return {
                "id": chapter_id,  # Scheme: chapter id
                "name": title,  # Scheme: chapter name
                "url": url,  # Scheme: chapter url
                "content": content,  # Scheme: content
                "comments": chapter_comments
            }
            
        except Exception as e:
            safe_print(f"‚ö†Ô∏è Thread-{index}: L·ªói c√†o ch∆∞∆°ng {index + 1}: {e}")
            return None
        finally:
            # ƒê√≥ng browser c·ªßa worker
            if worker_browser:
                worker_browser.close()
            if worker_playwright:
                worker_playwright.stop()

    def _get_max_comment_page(self, url):
        """L·∫•y s·ªë trang comments t·ªëi ƒëa t·ª´ pagination"""
        try:
            # ƒê·∫£m b·∫£o ƒëang ·ªü ƒë√∫ng trang (trang 1 - kh√¥ng c√≥ query comments)
            base_url = url.split('?')[0]
            current_url = self.page.url.split('?')[0]
            
            if base_url not in current_url:
                self.page.goto(base_url, timeout=config.TIMEOUT)
                time.sleep(2)
            
            # Scroll xu·ªëng ƒë·ªÉ load pagination
            self.page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
            time.sleep(2)
            
            max_page = 1  # M·∫∑c ƒë·ªãnh l√† 1 trang
            
            # T√¨m pagination element - c√≥ th·ªÉ trong .chapter-nav ho·∫∑c tr·ª±c ti·∫øp
            pagination_selectors = [
                "ul.pagination",
                ".chapter-nav ul.pagination",
                ".pagination"
            ]
            
            pagination = None
            for selector in pagination_selectors:
                try:
                    pagination = self.page.locator(selector).first
                    if pagination.count() > 0:
                        break
                except:
                    continue
            
            if pagination and pagination.count() > 0:
                # L·∫•y t·∫•t c·∫£ c√°c link c√≥ data-page attribute
                page_links = pagination.locator("a[data-page]").all()
                
                page_numbers = []
                for link in page_links:
                    try:
                        page_num_str = link.get_attribute("data-page")
                        if page_num_str:
                            page_num = int(page_num_str)
                            page_numbers.append(page_num)
                    except:
                        continue
                
                # C≈©ng th·ª≠ l·∫•y t·ª´ text content (n·∫øu kh√¥ng c√≥ data-page)
                if not page_numbers:
                    try:
                        all_links = pagination.locator("a").all()
                        for link in all_links:
                            try:
                                link_text = link.inner_text().strip()
                                # Th·ª≠ parse s·ªë t·ª´ text (v√≠ d·ª•: "31", "Next >" s·∫Ω b·ªã skip)
                                if link_text.isdigit():
                                    page_num = int(link_text)
                                    page_numbers.append(page_num)
                            except:
                                continue
                    except:
                        pass
                
                if page_numbers:
                    max_page = max(page_numbers)
                    safe_print(f"        üìÑ T√¨m th·∫•y {max_page} trang comments")
                else:
                    # N·∫øu kh√¥ng t√¨m th·∫•y s·ªë trang, c√≥ th·ªÉ ch·ªâ c√≥ 1 trang ho·∫∑c ch∆∞a load
                    safe_print(f"        üìÑ Kh√¥ng t√¨m th·∫•y pagination, gi·∫£ s·ª≠ c√≥ 1 trang")
            
            return max_page
        except Exception as e:
            safe_print(f"        ‚ö†Ô∏è L·ªói khi l·∫•y s·ªë trang: {e}")
            return 1  # N·∫øu l·ªói, m·∫∑c ƒë·ªãnh ch·ªâ c√≥ 1 trang

    def _scrape_comments_from_page(self, page_url):
        """L·∫•y comments t·ª´ m·ªôt trang c·ª• th·ªÉ (ScribbleHub chapter page)"""
        comments = []
        
        try:
            self.page.goto(page_url, timeout=config.TIMEOUT)
            time.sleep(2)  # Ch·ªù page load
            
            # Scroll xu·ªëng ƒë·ªÉ load comments (lazy load)
            self.page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
            time.sleep(2)
            
            # ScribbleHub chapter comments: div#comments.comments-area.chp > ol.comment-list.chapters > li
            # L·∫•y t·∫•t c·∫£ c√°c li trong ol.comment-list.chapters (level 1 comments)
            all_comment_lis = self.page.locator("div#comments.comments-area.chp ol.comment-list.chapters > li").all()
            
            # N·∫øu kh√¥ng t√¨m th·∫•y v·ªõi selector m·ªõi, th·ª≠ selector c≈© (RoyalRoad)
            if not all_comment_lis:
                all_comment_lis = self.page.locator("div.comment").all()
            
            for comment_li in all_comment_lis:
                try:
                    # Parse comment v√† replies ƒë·ªá quy
                    comment_data = self._scrape_single_comment_recursive(comment_li)
                    if comment_data:
                        comments.append(comment_data)
                except Exception as e:
                    continue
            
            return comments
            
        except Exception as e:
            safe_print(f"        ‚ö†Ô∏è L·ªói khi l·∫•y comments t·ª´ trang: {e}")
            return []

    def _scrape_comments(self, url, comment_type="chapter"):
        """
        L·∫•y t·∫•t c·∫£ comments t·ª´ T·∫§T C·∫¢ c√°c trang ph√¢n trang
        Tr·∫£ v·ªÅ danh s√°ch comments v·ªõi threading (comment g·ªëc + replies)
        """
        try:
            # ƒê·∫£m b·∫£o ƒëang ·ªü ƒë√∫ng trang ƒë·ªÉ ki·ªÉm tra pagination
            current_url = self.page.url
            if url not in current_url:
                self.page.goto(url, timeout=config.TIMEOUT)
                time.sleep(2)
            
            safe_print(f"      üí¨ ƒêang l·∫•y comments ({comment_type}-level)...")
            
            # B∆∞·ªõc 1: T√¨m s·ªë trang t·ªëi ƒëa
            max_page = self._get_max_comment_page(url)
            
            all_comments = []
            
            # B∆∞·ªõc 2: L·∫•y comments t·ª´ t·∫•t c·∫£ c√°c trang
            for page_num in range(1, max_page + 1):
                safe_print(f"        üìÑ ƒêang l·∫•y trang {page_num}/{max_page}...")
                
                # T·∫°o URL cho trang n√†y
                if page_num == 1:
                    # Trang 1: Lo·∫°i b·ªè query parameter comments n·∫øu c√≥
                    base_url = url.split('?')[0]  # L·∫•y URL g·ªëc kh√¥ng c√≥ query
                    page_url = base_url
                else:
                    # Trang kh√°c: Th√™m query parameter comments=N
                    base_url = url.split('?')[0]  # L·∫•y URL g·ªëc
                    # T√¨m c√°c query parameter hi·ªán c√≥ (tr·ª´ comments)
                    if '?' in url:
                        existing_params = url.split('?', 1)[1]
                        # Lo·∫°i b·ªè comments parameter n·∫øu c√≥
                        params_list = []
                        for param in existing_params.split('&'):
                            if not param.startswith('comments='):
                                params_list.append(param)
                        if params_list:
                            other_params = '&'.join(params_list)
                            page_url = f"{base_url}?{other_params}&comments={page_num}"
                        else:
                            page_url = f"{base_url}?comments={page_num}"
                    else:
                        page_url = f"{base_url}?comments={page_num}"
                
                # L·∫•y comments t·ª´ trang n√†y
                page_comments = self._scrape_comments_from_page(page_url)
                all_comments.extend(page_comments)
                
                safe_print(f"        ‚úÖ Trang {page_num}: L·∫•y ƒë∆∞·ª£c {len(page_comments)} comments")
                
                # Delay gi·ªØa c√°c trang ƒë·ªÉ tr√°nh b·ªã ban
                if page_num < max_page:
                    time.sleep(1)
            
            safe_print(f"      ‚úÖ T·ªïng c·ªông l·∫•y ƒë∆∞·ª£c {len(all_comments)} comments t·ª´ {max_page} trang ({comment_type}-level)")
            return all_comments
            
        except Exception as e:
            safe_print(f"      ‚ö†Ô∏è L·ªói khi l·∫•y comments: {e}")
            return []

    def _scrape_comments_worker(self, page, url, comment_type="chapter"):
        """
        Worker function ƒë·ªÉ l·∫•y comments - d√πng page t·ª´ worker thay v√¨ self.page
        """
        try:
            current_url = page.url
            if url not in current_url:
                # Delay tr∆∞·ªõc khi request comments
                time.sleep(config.DELAY_BETWEEN_REQUESTS)
                page.goto(url, timeout=config.TIMEOUT)
                time.sleep(2)
            
            safe_print(f"      üí¨ ƒêang l·∫•y comments ({comment_type}-level)...")
            
            # Delay tr∆∞·ªõc khi l·∫•y s·ªë trang
            time.sleep(config.DELAY_BETWEEN_REQUESTS)
            
            # T√¨m s·ªë trang t·ªëi ƒëa
            max_page = self._get_max_comment_page_worker(page, url)
            
            all_comments = []
            
            # L·∫•y comments t·ª´ t·∫•t c·∫£ c√°c trang
            for page_num in range(1, max_page + 1):
                safe_print(f"        üìÑ ƒêang l·∫•y trang {page_num}/{max_page}...")
                
                # T·∫°o URL cho trang n√†y
                if page_num == 1:
                    base_url = url.split('?')[0]
                    page_url = base_url
                else:
                    base_url = url.split('?')[0]
                    if '?' in url:
                        existing_params = url.split('?', 1)[1]
                        params_list = []
                        for param in existing_params.split('&'):
                            if not param.startswith('comments='):
                                params_list.append(param)
                        if params_list:
                            other_params = '&'.join(params_list)
                            page_url = f"{base_url}?{other_params}&comments={page_num}"
                        else:
                            page_url = f"{base_url}?comments={page_num}"
                    else:
                        page_url = f"{base_url}?comments={page_num}"
                
                # Delay tr∆∞·ªõc khi request trang comments
                if page_num > 1:
                    time.sleep(config.DELAY_BETWEEN_REQUESTS)
                
                # L·∫•y comments t·ª´ trang n√†y
                page_comments = self._scrape_comments_from_page_worker(page, page_url)
                all_comments.extend(page_comments)
                
                safe_print(f"        ‚úÖ Trang {page_num}: L·∫•y ƒë∆∞·ª£c {len(page_comments)} comments")
                
                # Delay gi·ªØa c√°c trang comments
                if page_num < max_page:
                    time.sleep(config.DELAY_BETWEEN_REQUESTS)
            
            safe_print(f"      ‚úÖ T·ªïng c·ªông l·∫•y ƒë∆∞·ª£c {len(all_comments)} comments t·ª´ {max_page} trang ({comment_type}-level)")
            return all_comments
            
        except Exception as e:
            safe_print(f"      ‚ö†Ô∏è L·ªói khi l·∫•y comments: {e}")
            return []

    def _get_max_comment_page_worker(self, page, url):
        """L·∫•y s·ªë trang comments t·ªëi ƒëa t·ª´ pagination - d√πng page t·ª´ worker"""
        try:
            base_url = url.split('?')[0]
            current_url = page.url.split('?')[0]
            
            if base_url not in current_url:
                page.goto(base_url, timeout=config.TIMEOUT)
                time.sleep(2)
            
            page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
            time.sleep(2)
            
            max_page = 1
            
            pagination_selectors = [
                "ul.pagination",
                ".chapter-nav ul.pagination",
                ".pagination"
            ]
            
            pagination = None
            for selector in pagination_selectors:
                try:
                    pagination = page.locator(selector).first
                    if pagination.count() > 0:
                        break
                except:
                    continue
            
            if pagination and pagination.count() > 0:
                page_links = pagination.locator("a[data-page]").all()
                
                page_numbers = []
                for link in page_links:
                    try:
                        page_num_str = link.get_attribute("data-page")
                        if page_num_str:
                            page_num = int(page_num_str)
                            page_numbers.append(page_num)
                    except:
                        continue
                
                if not page_numbers:
                    try:
                        all_links = pagination.locator("a").all()
                        for link in all_links:
                            try:
                                link_text = link.inner_text().strip()
                                if link_text.isdigit():
                                    page_num = int(link_text)
                                    page_numbers.append(page_num)
                            except:
                                continue
                    except:
                        pass
                
                if page_numbers:
                    max_page = max(page_numbers)
            
            return max_page
        except Exception as e:
            safe_print(f"        ‚ö†Ô∏è L·ªói khi l·∫•y s·ªë trang: {e}")
            return 1

    def _scrape_comments_from_page_worker(self, page, page_url):
        """L·∫•y comments t·ª´ m·ªôt trang c·ª• th·ªÉ - d√πng page t·ª´ worker (ScribbleHub chapter)"""
        comments = []
        
        try:
            # Delay tr∆∞·ªõc khi request
            time.sleep(config.DELAY_BETWEEN_REQUESTS)
            page.goto(page_url, timeout=config.TIMEOUT)
            time.sleep(2)
            
            page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
            time.sleep(2)
            
            # ScribbleHub chapter comments: div#comments.comments-area.chp > ol.comment-list.chapters > li
            all_comment_lis = page.locator("div#comments.comments-area.chp ol.comment-list.chapters > li").all()
            
            # N·∫øu kh√¥ng t√¨m th·∫•y v·ªõi selector m·ªõi, th·ª≠ selector c≈© (RoyalRoad)
            if not all_comment_lis:
                all_comment_lis = page.locator("div.comment").all()
            
            for comment_li in all_comment_lis:
                try:
                    comment_data = self._scrape_single_comment_recursive(comment_li)
                    if comment_data:
                        comments.append(comment_data)
                except Exception as e:
                    continue
            
            return comments
            
        except Exception as e:
            safe_print(f"        ‚ö†Ô∏è L·ªói khi l·∫•y comments t·ª´ trang: {e}")
            return []

    def _scrape_single_comment_recursive(self, comment_elem):
        """
        H√†m ƒë·ªá quy ƒë·ªÉ l·∫•y m·ªôt comment v√† t·∫•t c·∫£ replies c·ªßa n√≥
        H·ªó tr·ª£ c·∫£ ScribbleHub (li#comment-XXX) v√† RoyalRoad (div.comment)
        """
        try:
            import re
            
            # Ki·ªÉm tra xem l√† ScribbleHub format (li#comment-XXX) hay RoyalRoad format (div.comment)
            li_id = comment_elem.get_attribute("id") or ""
            if li_id.startswith("comment-"):
                # ƒê√¢y l√† ScribbleHub format
                return self._parse_scribblehub_comment(comment_elem)
            
            # Th·ª≠ RoyalRoad format
            media_elem = comment_elem.locator("div.media.media-v2").first
            if media_elem.count() == 0:
                return None
            
            # L·∫•y comment ID t·ª´ id attribute
            comment_id = media_elem.get_attribute("id") or ""
            if comment_id.startswith("comment-container-"):
                comment_id = comment_id.replace("comment-container-", "")
            
            # L·∫•y username - theo c·∫•u tr√∫c HTML: h4.media-heading > span.name > strong > a
            username = ""
            try:
                # C·∫•u tr√∫c: h4.media-heading > span.name > a[href*='/profile/']
                username_selectors = [
                    "h4.media-heading span.name a",
                    "h4.media-heading .name a",
                    ".media-heading span.name a",
                    ".media-heading .name a[href*='/profile/']",
                    "h4.media-heading a[href*='/profile/']",
                    ".media-heading a[href*='/profile/']"
                ]
                
                for selector in username_selectors:
                    try:
                        username_elem = media_elem.locator(selector).first
                        if username_elem.count() > 0:
                            username = username_elem.inner_text().strip()
                            if username:
                                break
                    except:
                        continue
                
                # N·∫øu v·∫´n kh√¥ng t√¨m th·∫•y, th·ª≠ l·∫•y t·ª´ b·∫•t k·ª≥ link profile n√†o trong media-heading
                if not username:
                    try:
                        username_elem = media_elem.locator(".media-heading a[href*='/profile/']").first
                        if username_elem.count() > 0:
                            username = username_elem.inner_text().strip()
                    except:
                        pass
                        
                if not username:
                    username = "[Unknown]"
            except:
                username = "[Unknown]"
            
            # L·∫•y comment text/content - l·∫•y t·∫•t c·∫£ c√°c ƒëo·∫°n vƒÉn ƒë·ªÉ gi·ªØ format
            comment_text = ""
            try:
                media_body = media_elem.locator(".media-body").first
                if media_body.count() > 0:
                    # L·∫•y t·∫•t c·∫£ c√°c ƒëo·∫°n vƒÉn trong comment
                    paragraphs = media_body.locator("p").all()
                    
                    if paragraphs:
                        # N·∫øu c√≥ nhi·ªÅu ƒëo·∫°n vƒÉn, n·ªëi l·∫°i v·ªõi xu·ªëng d√≤ng
                        text_parts = []
                        for para in paragraphs:
                            try:
                                para_text = para.inner_text().strip()
                                if para_text:
                                    text_parts.append(para_text)
                            except:
                                continue
                        comment_text = "\n\n".join(text_parts)
                    else:
                        # N·∫øu kh√¥ng c√≥ th·∫ª p, l·∫•y to√†n b·ªô text t·ª´ media-body
                        full_text = media_body.inner_text().strip()
                        
                        # Lo·∫°i b·ªè username n·∫øu c√≥ ·ªü ƒë·∫ßu
                        if username and full_text.startswith(username):
                            comment_text = full_text[len(username):].strip()
                        else:
                            comment_text = full_text
                        
                        # Lo·∫°i b·ªè c√°c ph·∫ßn kh√¥ng ph·∫£i n·ªôi dung (nh∆∞ timestamp, rep count)
                        # C√°c ph·∫ßn n√†y th∆∞·ªùng ·ªü cu·ªëi, c√≥ th·ªÉ c√≥ format nh∆∞ "7 years ago" ho·∫∑c "Rep (63)"
                        lines = comment_text.split('\n')
                        cleaned_lines = []
                        for line in lines:
                            line = line.strip()
                            if not line:
                                continue
                            # B·ªè qua d√≤ng ch·ª©a "years ago", "Rep (", "Reply", "Report"
                            if any(x in line.lower() for x in ['years ago', 'months ago', 'days ago', 'hours ago', 
                                                                'rep (', 'reply', 'report']):
                                continue
                            cleaned_lines.append(line)
                        comment_text = '\n'.join(cleaned_lines).strip()
            except Exception as e:
                comment_text = ""
            
            # L·∫•y timestamp
            timestamp = ""
            try:
                time_elem = media_elem.locator("time, .timestamp, [class*='time'], [class*='date']").first
                if time_elem.count() > 0:
                    timestamp = time_elem.get_attribute("datetime") or time_elem.inner_text().strip()
            except:
                pass
            
            # T·∫°o c·∫•u tr√∫c comment theo scheme
            comment_data = {
                "comment_id": comment_id,
                "username": username,
                "comment_text": comment_text,
                "time": timestamp,  # Scheme: time (ƒë·ªïi t·ª´ timestamp)
                "replies": []  # S·∫Ω ƒë∆∞·ª£c ƒëi·ªÅn ƒë·ªá quy
            }
            
            # L·∫•y replies (subcomments) - ƒê·ªÜ QUY
            try:
                subcomments_list = comment_elem.locator("ul.subcomments").first
                if subcomments_list.count() > 0:
                    # L·∫•y t·∫•t c·∫£ c√°c comment con trong ul.subcomments
                    reply_comments = subcomments_list.locator("div.comment").all()
                    
                    for reply_elem in reply_comments:
                        reply_data = self._scrape_single_comment_recursive(reply_elem)
                        if reply_data:
                            comment_data["replies"].append(reply_data)
            except Exception as e:
                # Kh√¥ng c√≥ replies ho·∫∑c l·ªói khi l·∫•y
                pass
            
            return comment_data
            
        except Exception as e:
            safe_print(f"        ‚ö†Ô∏è L·ªói khi parse comment: {e}")
            return None
    
    def _parse_scribblehub_comment(self, comment_li):
        """Parse comment theo c·∫•u tr√∫c ScribbleHub chapter comments"""
        try:
            import re
            
            # Comment ID: id="comment-3636791" -> 3636791
            li_id = comment_li.get_attribute("id") or ""
            match = re.search(r'comment-(\d+)', li_id)
            comment_id = match.group(1) if match else ""
            
            # Username: span.fn a
            username = ""
            user_id = ""
            try:
                username_elem = comment_li.locator("span.fn a").first
                if username_elem.count() > 0:
                    username = username_elem.inner_text().strip()
                    # User ID t·ª´ href: /profile/65092/username/ -> 65092
                    user_url = username_elem.get_attribute("href") or ""
                    user_id_match = re.search(r'/profile/(\d+)/', user_url)
                    user_id = user_id_match.group(1) if user_id_match else ""
            except:
                username = "[Unknown]"
                user_id = ""
            
            # Time: span.com_date a v·ªõi attribute title
            timestamp = ""
            comment_url = ""
            try:
                date_elem = comment_li.locator("span.com_date a").first
                if date_elem.count() > 0:
                    timestamp = date_elem.get_attribute("title") or date_elem.inner_text().strip()
                    comment_url = date_elem.get_attribute("href") or ""
            except:
                pass
            
            # Chapter ID t·ª´ comment_url: ?cid=3636791&chapter=1709464 -> 1709464
            chapter_id = ""
            try:
                if "chapter=" in comment_url:
                    match = re.search(r'chapter=(\d+)', comment_url)
                    if match:
                        chapter_id = match.group(1)
            except:
                pass
            
            # Comment text: div.user-comment.comment
            comment_text = ""
            try:
                comment_body = comment_li.locator("div.user-comment.comment").first
                if comment_body.count() > 0:
                    comment_text = comment_body.inner_text().strip()
            except:
                pass
            
            comment_data = {
                "comment_id": comment_id,
                "username": username,
                "user_id": user_id,
                "chapter_id": chapter_id,
                "comment_text": comment_text,
                "time": timestamp,
                "comment_url": comment_url,
                "replies": []
            }
            
            # L·∫•y replies: ol.children > li
            try:
                children_ol = comment_li.locator("ol.children").first
                if children_ol.count() > 0:
                    reply_lis = children_ol.locator("> li").all()
                    for reply_li in reply_lis:
                        reply_data = self._parse_scribblehub_comment(reply_li)
                        if reply_data:
                            comment_data["replies"].append(reply_data)
            except:
                pass
            
            return comment_data
        except Exception as e:
            safe_print(f"        ‚ö†Ô∏è L·ªói khi parse ScribbleHub comment: {e}")
            return None

    def _scrape_reviews(self, fiction_url):
        """
        L·∫•y t·∫•t c·∫£ reviews t·ª´ trang fiction (ScribbleHub)
        """
        reviews = []
        try:
            safe_print("      üìù ƒêang l·∫•y reviews t·ª´ trang fiction...")
            
            # ƒê·∫£m b·∫£o ƒëang ·ªü trang fiction
            self.page.goto(fiction_url, timeout=config.TIMEOUT)
            time.sleep(2)
            
            # Scroll xu·ªëng ƒë·ªÉ load reviews section
            self.page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
            time.sleep(2)
            
            # ScribbleHub d√πng: .w-comments-item cho reviews
            review_elements = self.page.locator(".w-comments-item").all()
            
            if not review_elements:
                safe_print("      ‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y reviews!")
                return []
            
            safe_print(f"      ‚úÖ T√¨m th·∫•y {len(review_elements)} reviews")
            
            # Parse t·ª´ng review
            for review_elem in review_elements:
                try:
                    review_data = self._parse_single_review(review_elem)
                    if review_data:
                        reviews.append(review_data)
                except Exception as e:
                    safe_print(f"        ‚ö†Ô∏è L·ªói khi parse review: {e}")
                    continue
            
            safe_print(f"      ‚úÖ ƒê√£ l·∫•y ƒë∆∞·ª£c {len(reviews)} reviews")
            return reviews
            
        except Exception as e:
            safe_print(f"      ‚ö†Ô∏è L·ªói khi l·∫•y reviews: {e}")
            return []

    def _parse_single_review(self, review_elem):
        """
        Parse m·ªôt review element th√†nh dictionary theo scheme (ScribbleHub)
        """
        try:
            # L·∫•y review ID t·ª´ id attribute
            review_id = ""
            try:
                review_id = review_elem.get_attribute("id") or ""
                if review_id.startswith("comment-"):
                    review_id = review_id.replace("comment-", "")
            except:
                pass
            
            # L·∫•y title - ScribbleHub kh√¥ng c√≥ title ri√™ng, l·∫•y t·ª´ status
            title = ""
            try:
                status_elem = review_elem.locator(".status_cmt .fic_r_stats").first
                if status_elem.count() > 0:
                    title = status_elem.inner_text().strip()
            except:
                pass
            
            # L·∫•y username
            username = ""
            try:
                username_elem = review_elem.locator(".revname, a[id^='revname']").first
                if username_elem.count() > 0:
                    username = username_elem.inner_text().strip()
            except:
                username = "[Unknown]"
            
            # L·∫•y "at chapter"
            at_chapter = ""
            try:
                status_elem = review_elem.locator(".status_cmt .fic_r_stats").first
                if status_elem.count() > 0:
                    at_chapter = status_elem.inner_text().strip()
            except:
                pass
            
            # L·∫•y time
            time_str = ""
            try:
                time_elem = review_elem.locator(".pro_item_al a").first
                if time_elem.count() > 0:
                    time_str = time_elem.inner_text().strip()
            except:
                pass
            
            # L·∫•y content
            content = ""
            try:
                content_elem = review_elem.locator(".w-comments-item-text").first
                if content_elem.count() > 0:
                    # L·∫•y HTML ƒë·ªÉ gi·ªØ ƒë·ªãnh d·∫°ng
                    html_content = content_elem.inner_html()
                    content = self._convert_html_to_formatted_text(html_content)
            except:
                pass
            
            # L·∫•y scores t·ª´ stars
            scores = {
                "overall": "",
                "style": "",
                "story": "",
                "grammar": "",
                "character": ""
            }
            
            try:
                # ƒê·∫øm s·ªë sao ƒë∆∞·ª£c ch·ªçn (filled stars)
                filled_stars = review_elem.locator(".userreview.fa-star").count()
                empty_stars = review_elem.locator(".userreview.fa-star-o").count()
                half_stars = review_elem.locator(".userreview.fa-star-half-o").count()
                
                # T√≠nh overall score
                if filled_stars > 0:
                    overall = filled_stars + (half_stars * 0.5)
                    scores["overall"] = str(overall)
            except:
                pass
            
            # T·∫°o review data theo scheme
            review_data = {
                "review_id": review_id,
                "title": title,
                "username": username,
                "at_chapter": at_chapter,
                "time": time_str,
                "content": content,
                "score": scores
            }
            
            return review_data
            
        except Exception as e:
            safe_print(f"        ‚ö†Ô∏è L·ªói khi parse review: {e}")
            return None

    def _save_to_json(self, data):
        """
        L∆∞u d·ªØ li·ªáu v√†o c·∫£ file JSON v√† MongoDB (n·∫øu ƒë∆∞·ª£c b·∫≠t)
        T√°ch d·ªØ li·ªáu th√†nh nhi·ªÅu collections: stories, chapters, comments, reviews, scores, users
        """
        # 1. L∆∞u v√†o file JSON (lu√¥n lu√¥n)
        filename = f"{data['id']}_{utils.clean_text(data.get('name', data.get('title', 'unknown')))}.json"
        save_path = os.path.join(config.JSON_DIR, filename)
        
        with open(save_path, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=4)
        safe_print(f"üíæ ƒê√£ l∆∞u d·ªØ li·ªáu v√†o file: {save_path}")
        
        # 2. L∆∞u v√†o MongoDB - t√°ch th√†nh nhi·ªÅu collections
        if self.mongo_collections:
            try:
                story_id = data['id']
                
                # 2.1. L∆∞u STORY v√†o collection "stories"
                story_data = {
                    "id": story_id,
                    "name": data.get("name", ""),
                    "url": data.get("url", ""),
                    "cover_image": data.get("cover_image", ""),
                    "author": data.get("author", ""),
                    "category": data.get("category", ""),
                    "status": data.get("status", ""),
                    "tags": data.get("tags", []),
                    "description": data.get("description", ""),
                    "stats": {
                        "views": data.get("stats", {}).get("views", {})
                    }
                }
                
                stories_col = self.mongo_collections["stories"]
                existing_story = stories_col.find_one({"id": story_id})
                if existing_story:
                    stories_col.update_one({"id": story_id}, {"$set": story_data})
                    safe_print(f"üîÑ ƒê√£ c·∫≠p nh·∫≠t story trong MongoDB (ID: {story_id})")
                else:
                    stories_col.insert_one(story_data)
                    safe_print(f"‚úÖ ƒê√£ l∆∞u story v√†o MongoDB (ID: {story_id})")
                
                # 2.2. L∆∞u SCORES v√†o collection "scores"
                if "stats" in data and "score" in data["stats"]:
                    score_data = {
                        "story_id": story_id,
                        "overall_score": data["stats"]["score"].get("overall_score", ""),
                        "style_score": data["stats"]["score"].get("style_score", ""),
                        "story_score": data["stats"]["score"].get("story_score", ""),
                        "grammar_score": data["stats"]["score"].get("grammar_score", ""),
                        "character_score": data["stats"]["score"].get("character_score", "")
                    }
                    
                    scores_col = self.mongo_collections["scores"]
                    existing_score = scores_col.find_one({"story_id": story_id})
                    if existing_score:
                        scores_col.update_one({"story_id": story_id}, {"$set": score_data})
                    else:
                        scores_col.insert_one(score_data)
                    safe_print(f"‚úÖ ƒê√£ l∆∞u scores v√†o MongoDB (story_id: {story_id})")
                
                # 2.3. L∆∞u CHAPTERS v√†o collection "chapters"
                chapters_col = self.mongo_collections["chapters"]
                chapters = data.get("chapters", [])
                chapters_saved = 0
                for chapter in chapters:
                    chapter_data = {
                        "id": chapter.get("id", ""),
                        "story_id": story_id,
                        "name": chapter.get("name", ""),
                        "url": chapter.get("url", ""),
                        "content": chapter.get("content", "")
                    }
                    
                    chapter_id = chapter_data["id"]
                    if chapter_id:
                        existing_chapter = chapters_col.find_one({"id": chapter_id, "story_id": story_id})
                        if existing_chapter:
                            chapters_col.update_one(
                                {"id": chapter_id, "story_id": story_id},
                                {"$set": chapter_data}
                            )
                        else:
                            chapters_col.insert_one(chapter_data)
                        chapters_saved += 1
                        
                        # 2.4. L∆∞u COMMENTS c·ªßa chapter v√†o collection "comments"
                        chapter_comments = chapter.get("comments", [])
                        if chapter_comments:
                            self._save_comments_to_mongo(chapter_comments, story_id, chapter_id, "chapter")
                
                safe_print(f"‚úÖ ƒê√£ l∆∞u {chapters_saved} chapters v√†o MongoDB (story_id: {story_id})")
                
                # 2.5. L∆∞u REVIEWS v√†o collection "reviews"
                reviews_col = self.mongo_collections["reviews"]
                reviews = data.get("reviews", [])
                reviews_saved = 0
                for review in reviews:
                    review_data = {
                        "review_id": review.get("review_id", ""),
                        "story_id": story_id,
                        "title": review.get("title", ""),
                        "username": review.get("username", ""),
                        "at_chapter": review.get("at_chapter", ""),
                        "time": review.get("time", ""),
                        "content": review.get("content", ""),
                        "score": review.get("score", {})
                    }
                    
                    review_id = review_data["review_id"]
                    if review_id:
                        existing_review = reviews_col.find_one({"review_id": review_id, "story_id": story_id})
                        if existing_review:
                            reviews_col.update_one(
                                {"review_id": review_id, "story_id": story_id},
                                {"$set": review_data}
                            )
                        else:
                            reviews_col.insert_one(review_data)
                        reviews_saved += 1
                        
                        # L∆∞u user t·ª´ review
                        username = review_data.get("username", "")
                        if username:
                            self._save_user_to_mongo(username)
                
                safe_print(f"‚úÖ ƒê√£ l∆∞u {reviews_saved} reviews v√†o MongoDB (story_id: {story_id})")
                
                # 2.6. L∆∞u v√†o collection c≈© ƒë·ªÉ t∆∞∆°ng th√≠ch (n·∫øu c·∫ßn)
                if self.mongo_collection:
                    existing = self.mongo_collection.find_one({"id": story_id})
                    if existing:
                        self.mongo_collection.update_one({"id": story_id}, {"$set": data})
                    else:
                        self.mongo_collection.insert_one(data)
                
                safe_print(f"üéâ ƒê√£ ho√†n th√†nh l∆∞u t·∫•t c·∫£ d·ªØ li·ªáu v√†o MongoDB!")
                
            except Exception as e:
                safe_print(f"‚ö†Ô∏è L·ªói khi l∆∞u v√†o MongoDB: {e}")
                safe_print("   D·ªØ li·ªáu v·∫´n ƒë∆∞·ª£c l∆∞u v√†o file JSON")
                import traceback
                safe_print(traceback.format_exc())
    
    def _save_comments_to_mongo(self, comments, story_id, parent_id, parent_type="chapter"):
        """
        L∆∞u comments v√†o MongoDB (ƒë·ªá quy ƒë·ªÉ l∆∞u c·∫£ replies)
        parent_type: "chapter" ho·∫∑c "story"
        """
        if not self.mongo_collections:
            return
        
        comments_col = self.mongo_collections["comments"]
        
        for comment in comments:
            comment_data = {
                "comment_id": comment.get("comment_id", ""),
                "story_id": story_id,
                "parent_id": parent_id,
                "parent_type": parent_type,
                "username": comment.get("username", ""),
                "comment_text": comment.get("comment_text", ""),
                "time": comment.get("time", "")
            }
            
            comment_id = comment_data["comment_id"]
            if comment_id:
                # Ki·ªÉm tra xem ƒë√£ c√≥ comment n√†y ch∆∞a (th√™m parent_type ƒë·ªÉ ch·∫Øc ch·∫Øn)
                existing = comments_col.find_one({
                    "comment_id": comment_id,
                    "story_id": story_id,
                    "parent_id": parent_id,
                    "parent_type": parent_type
                })
                
                if existing:
                    comments_col.update_one(
                        {"comment_id": comment_id, "story_id": story_id, "parent_id": parent_id, "parent_type": parent_type},
                        {"$set": comment_data}
                    )
                else:
                    comments_col.insert_one(comment_data)
                
                # L∆∞u user t·ª´ comment
                username = comment_data.get("username", "")
                if username:
                    self._save_user_to_mongo(username)
                
                # L∆∞u replies (ƒë·ªá quy)
                replies = comment.get("replies", [])
                if replies:
                    self._save_comments_to_mongo(replies, story_id, comment_id, "comment")
    
    def _save_user_to_mongo(self, username):
        """
        L∆∞u user v√†o collection "users" (ch·ªâ l∆∞u username, c√≥ th·ªÉ m·ªü r·ªông sau)
        """
        if not self.mongo_collections or not username or username == "[Unknown]":
            return
        
        users_col = self.mongo_collections["users"]
        
        # Ki·ªÉm tra xem user ƒë√£ t·ªìn t·∫°i ch∆∞a
        existing_user = users_col.find_one({"username": username})
        if not existing_user:
            user_data = {
                "username": username,
                "created_at": time.time()  # Timestamp khi t·∫°o
            }
            users_col.insert_one(user_data)