import time
import json
import os
import re
import sys
from concurrent.futures import ThreadPoolExecutor, as_completed
from playwright.sync_api import sync_playwright
from src import config, utils

# Import MongoDB
try:
    from pymongo import MongoClient
    MONGODB_AVAILABLE = True
except ImportError:
    MONGODB_AVAILABLE = False

# Helper function ƒë·ªÉ print an to√†n v·ªõi encoding UTF-8
def safe_print(*args, **kwargs):
    """Print function an to√†n v·ªõi encoding UTF-8 tr√™n Windows"""
    try:
        # Th·ª≠ print b√¨nh th∆∞·ªùng
        print(*args, **kwargs)
    except UnicodeEncodeError:
        # N·∫øu l·ªói encoding, encode l·∫°i th√†nh ASCII-safe
        message = ' '.join(str(arg) for arg in args)
        # Thay th·∫ø emoji v√† k√Ω t·ª± ƒë·∫∑c bi·ªát
        message = message.encode('ascii', 'replace').decode('ascii')
        print(message, **kwargs)

class RoyalRoadScraper:
    def __init__(self, max_workers=None):
        self.browser = None
        self.context = None
        self.page = None
        self.playwright = None
        self.max_workers = max_workers or config.MAX_WORKERS
        
        # Kh·ªüi t·∫°o MongoDB client n·∫øu ƒë∆∞·ª£c b·∫≠t
        self.mongo_client = None
        self.mongo_db = None
        self.mongo_collection_stories = None
        self.mongo_collection_chapters = None
        self.mongo_collection_comments = None
        self.mongo_collection_reviews = None
        self.mongo_collection_users = None
        self.mongo_collection_scores = None
        if config.MONGODB_ENABLED and MONGODB_AVAILABLE:
            try:
                self.mongo_client = MongoClient(config.MONGODB_URI)
                self.mongo_db = self.mongo_client[config.MONGODB_DB_NAME]
                self.mongo_collection_stories = self.mongo_db[config.MONGODB_COLLECTION_STORIES]
                self.mongo_collection_chapters = self.mongo_db["chapters"]
                self.mongo_collection_comments = self.mongo_db["comments"]
                self.mongo_collection_reviews = self.mongo_db["reviews"]
                self.mongo_collection_users = self.mongo_db["users"]
                self.mongo_collection_scores = self.mongo_db["scores"]
                safe_print("‚úÖ ƒê√£ k·∫øt n·ªëi MongoDB v·ªõi 6 collections")
            except Exception as e:
                safe_print(f"‚ö†Ô∏è Kh√¥ng th·ªÉ k·∫øt n·ªëi MongoDB: {e}")
                safe_print("   Ti·∫øp t·ª•c l∆∞u v√†o file JSON...")
                self.mongo_client = None

    def start(self):
        """Kh·ªüi ƒë·ªông tr√¨nh duy·ªát"""
        self.playwright = sync_playwright().start()
        self.browser = self.playwright.chromium.launch(headless=config.HEADLESS)
        self.context = self.browser.new_context()
        self.page = self.context.new_page()
        safe_print("‚úÖ Bot ƒë√£ kh·ªüi ƒë·ªông!")

    def stop(self):
        """ƒê√≥ng tr√¨nh duy·ªát v√† MongoDB connection"""
        if self.browser:
            self.browser.close()
        if self.playwright:
            self.playwright.stop()
        if self.mongo_client:
            self.mongo_client.close()
            safe_print("‚úÖ ƒê√£ ƒë√≥ng k·∫øt n·ªëi MongoDB")
        safe_print("zzz Bot ƒë√£ t·∫Øt.")

    def scrape_best_rated_stories(self, best_rated_url, num_stories=10, start_from=0):
        """
        C√†o nhi·ªÅu b·ªô truy·ªán t·ª´ trang best-rated
        Args:
            best_rated_url: URL trang best-rated
            num_stories: S·ªë l∆∞·ª£ng b·ªô truy·ªán mu·ªën c√†o (m·∫∑c ƒë·ªãnh 10)
            start_from: B·∫Øt ƒë·∫ßu t·ª´ v·ªã tr√≠ th·ª© m·∫•y (0 = b·ªô ƒë·∫ßu ti√™n, 5 = b·ªè qua 5 b·ªô ƒë·∫ßu)
        """
        safe_print(f"üìö ƒêang truy c·∫≠p trang best-rated: {best_rated_url}")
        self.page.goto(best_rated_url, timeout=config.TIMEOUT)
        time.sleep(2)
        
        # L·∫•y danh s√°ch c√°c b·ªô truy·ªán t·ª´ trang best-rated
        if start_from > 0:
            safe_print(f"üîç ƒêang l·∫•y danh s√°ch {num_stories} b·ªô truy·ªán (b·∫Øt ƒë·∫ßu t·ª´ v·ªã tr√≠ {start_from + 1})...")
        else:
            safe_print(f"üîç ƒêang l·∫•y danh s√°ch {num_stories} b·ªô truy·ªán ƒë·∫ßu ti√™n...")
        story_urls = self._get_story_urls_from_best_rated(num_stories, start_from)
        
        if not story_urls:
            safe_print("‚ùå Kh√¥ng t√¨m th·∫•y b·ªô truy·ªán n√†o!")
            return
        
        safe_print(f"‚úÖ ƒê√£ t√¨m th·∫•y {len(story_urls)} b·ªô truy·ªán:")
        for i, url in enumerate(story_urls, 1):
            safe_print(f"   {i}. {url}")
        
        # C√†o t·ª´ng b·ªô truy·ªán tu·∫ßn t·ª±
        for index, story_url in enumerate(story_urls, 1):
            safe_print(f"\n{'='*60}")
            safe_print(f"üìñ B·∫Øt ƒë·∫ßu c√†o b·ªô truy·ªán {index}/{len(story_urls)}")
            safe_print(f"{'='*60}")
            try:
                self.scrape_story(story_url)
                safe_print(f"‚úÖ Ho√†n th√†nh b·ªô truy·ªán {index}/{len(story_urls)}")
            except Exception as e:
                safe_print(f"‚ùå L·ªói khi c√†o b·ªô truy·ªán {index}: {e}")
                continue
            
            # Delay gi·ªØa c√°c b·ªô truy·ªán
            if index < len(story_urls):
                safe_print(f"‚è≥ Ngh·ªâ {config.DELAY_BETWEEN_CHAPTERS * 2} gi√¢y tr∆∞·ªõc khi c√†o b·ªô ti·∫øp theo...")
                time.sleep(config.DELAY_BETWEEN_CHAPTERS * 2)
        
        safe_print(f"\n{'='*60}")
        safe_print(f"üéâ ƒê√£ ho√†n th√†nh c√†o {len(story_urls)} b·ªô truy·ªán!")
        safe_print(f"{'='*60}")

    def _get_story_urls_from_best_rated(self, num_stories=10, start_from=0):
        """
        L·∫•y danh s√°ch URL c·ªßa c√°c b·ªô truy·ªán t·ª´ trang best-rated
        Selector: h2.fiction-title a
        Args:
            num_stories: S·ªë l∆∞·ª£ng b·ªô truy·ªán mu·ªën l·∫•y
            start_from: B·∫Øt ƒë·∫ßu t·ª´ v·ªã tr√≠ th·ª© m·∫•y (0 = b·ªô ƒë·∫ßu ti√™n)
        """
        story_urls = []
        
        try:
            # Scroll xu·ªëng ƒë·ªÉ load th√™m n·ªôi dung n·∫øu c·∫ßn
            self.page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
            time.sleep(2)
            
            # L·∫•y t·∫•t c·∫£ c√°c link truy·ªán t·ª´ th·∫ª h2.fiction-title a
            fiction_links = self.page.locator("h2.fiction-title a").all()
            
            # T√≠nh to√°n v·ªã tr√≠ b·∫Øt ƒë·∫ßu v√† k·∫øt th√∫c
            start_index = start_from
            end_index = start_from + num_stories
            
            # L·∫•y c√°c link t·ª´ v·ªã tr√≠ start_from ƒë·∫øn end_index
            for link in fiction_links[start_index:end_index]:
                try:
                    href = link.get_attribute("href")
                    if href:
                        # T·∫°o full URL
                        if href.startswith("/"):
                            full_url = config.BASE_URL + href
                        elif href.startswith("http"):
                            full_url = href
                        else:
                            full_url = config.BASE_URL + "/" + href
                        
                        if full_url not in story_urls:
                            story_urls.append(full_url)
                except Exception as e:
                    safe_print(f"‚ö†Ô∏è L·ªói khi l·∫•y URL truy·ªán: {e}")
                    continue
            
            return story_urls
            
        except Exception as e:
            safe_print(f"‚ö†Ô∏è L·ªói khi l·∫•y danh s√°ch truy·ªán t·ª´ best-rated: {e}")
            return []

    def scrape_story(self, story_url):
        """
        H√†m ch√≠nh ƒë·ªÉ c√†o to√†n b·ªô 1 b·ªô truy·ªán.
        Lu·ªìng ƒëi: V√†o trang truy·ªán -> L·∫•y Info -> L·∫•y List Chapter -> V√†o t·ª´ng Chapter -> L·∫•y Content.
        """
        safe_print(f"üåç ƒêang truy c·∫≠p truy·ªán: {story_url}")
        self.page.goto(story_url, timeout=config.TIMEOUT)

        # 1. L·∫•y ID truy·ªán t·ª´ URL (V√≠ d·ª•: 21220)
        story_id = story_url.split("/")[4]

        # 2. L·∫•y th√¥ng tin t·ªïng quan (Metadata)
        safe_print("... ƒêang l·∫•y th√¥ng tin chung")
        
        # L·∫•y title
        title = self.page.locator("h1").first.inner_text()
        
        # L·∫•y URL ·∫£nh b√¨a r·ªìi t·∫£i v·ªÅ lu√¥n
        img_url_raw = self.page.locator(".cover-art-container img").get_attribute("src")
        local_img_path = utils.download_image(img_url_raw, story_id)

        # L·∫•y author (user_id t·ª´ profile URL)
        author_id = self.page.locator(".fic-title h4 a").first.get_attribute("href").split("/")[2]
        author_name = self.page.locator(".fic-title h4 a").first.inner_text()
        
        # L∆∞u user (author) ngay v√†o MongoDB
        if author_id and author_name:
            self._save_user_to_mongo(author_id, author_name)

        # L·∫•y category
        category = self.page.locator(".fiction-info span").first.inner_text()

        # L·∫•y status
        status = self.page.locator(".fiction-info span:nth-child(2)").first.inner_text()

        #L·∫•y tags
        tags = self.page.locator(".tags a").all_inner_texts()

        #L·∫•y description - gi·ªØ nguy√™n ƒë·ªãnh d·∫°ng nh∆∞ trong UI
        description = ""
        try:
            desc_container = self.page.locator(".description").first
            if desc_container.count() > 0:
                # L·∫•y HTML ƒë·ªÉ gi·ªØ ƒë·ªãnh d·∫°ng
                html_content = desc_container.inner_html()
                # Chuy·ªÉn HTML sang text v·ªõi ƒë·ªãnh d·∫°ng ƒë√∫ng
                description = self._convert_html_to_formatted_text(html_content)
        except Exception as e:
            safe_print(f"‚ö†Ô∏è L·ªói khi l·∫•y description: {e}")
            description = ""

        #L·∫•y stats
        # stats = self.page.locator(".stats-content .list-item").all()
        # Container ch√≠nh: .stats-content ul.list-unstyled
        base_locator = ".stats-content ul.list-unstyled li:nth-child({}) span"

        # 1. Overall Score (N·∫±m ·ªü v·ªã tr√≠ con th·ª© 2)
        overall_score = self.page.locator(base_locator.format(2)).inner_text()

        # 2. Style Score (V·ªã tr√≠ con th·ª© 4)
        style_score = self.page.locator(base_locator.format(4)).inner_text()

        # 3. Story Score (V·ªã tr√≠ con th·ª© 6)
        story_score = self.page.locator(base_locator.format(6)).inner_text()

        # 4. Grammar Score (V·ªã tr√≠ con th·ª© 8)
        grammar_score = self.page.locator(base_locator.format(8)).inner_text()

        # 5. Character Score (V·ªã tr√≠ con th·ª© 10)
        character_score = self.page.locator(base_locator.format(10)).inner_text()

        # 1. ƒê·ªãnh v·ªã t·∫•t c·∫£ c√°c th·∫ª <li> ch·ª©a GI√Å TR·ªä s·ªë li·ªáu
        # S·ª≠ d·ª•ng class ƒë·∫∑c tr∆∞ng (.font-red-sunglo) v√† gi·ªõi h·∫°n trong kh·ªëi stats b√™n ph·∫£i (.col-sm-6)
        stats_values_locator = self.page.locator("div.col-sm-6 li.font-red-sunglo")
        
        # 2. L·∫•y gi√° tr·ªã b·∫±ng c√°ch d√πng ch·ªâ m·ª•c (index)
        
        # L·∫•y total_views (Index 0)
        total_views = stats_values_locator.nth(0).inner_text()
        
        # L·∫•y average_views (Index 1)
        average_views = stats_values_locator.nth(1).inner_text()
        
        # L·∫•y followers (Index 2)
        followers = stats_values_locator.nth(2).inner_text()
        
        # L·∫•y favorites (Index 3)
        favorites = stats_values_locator.nth(3).inner_text()
        
        # L·∫•y ratings (Index 4)
        ratings = stats_values_locator.nth(4).inner_text()
        
        # L·∫•y pages/words (Index 5 - Gi√° tr·ªã cu·ªëi c√πng)
        pages = stats_values_locator.nth(5).inner_text()

        # T·∫°o c·∫•u tr√∫c d·ªØ li·ªáu t·ªïng quan theo schema
        # Schema: story id, story name, story url, cover image, category, status, tags, description, 
        # total views, average views, followers, favorites, ratings, page views
        # Score: overall_score, style_score, story_score, grammar_score, character_score
        story_data = {
            "id": story_id,  # Schema: story id
            "name": title,  # Schema: story name
            "url": story_url,  # Schema: story url
            "cover_image": local_img_path,  # Schema: cover image
            "category": category,  # Schema: category
            "status": status,  # Schema: status
            "tags": tags,  # Schema: tags
            "description": description,  # Schema: description
            "total_views": total_views,  # Schema: total views
            "average_views": average_views,  # Schema: average views
            "followers": followers,  # Schema: followers
            "favorites": favorites,  # Schema: favorites
            "ratings": ratings,  # Schema: ratings
            "page_views": pages,  # Schema: page views
            "overall_score": overall_score,  # Schema: overall score
            "style_score": style_score,  # Schema: style score
            "story_score": story_score,  # Schema: story score
            "grammar_score": grammar_score,  # Schema: grammar score
            "character_score": character_score,  # Schema: character score
            "reviews": [],  # S·∫Ω ƒë∆∞·ª£c ƒëi·ªÅn sau
            "chapters": []     # Chu·∫©n b·ªã c√°i m·∫£ng r·ªóng ƒë·ªÉ ch·ª©a c√°c ch∆∞∆°ng
        }
        
        # L∆∞u score v√†o collection scores (t·ª´ story)
        score_id = f"{story_id}_score"
        self._save_score_to_mongo(score_id, overall_score, style_score, story_score, grammar_score, character_score)
        
        # L∆∞u story ngay khi c√†o xong metadata (ch∆∞a c√≥ chapters v√† reviews)
        self._save_story_to_mongo(story_data)

        # 3. L·∫•y danh s√°ch link ch∆∞∆°ng t·ª´ T·∫§T C·∫¢ c√°c trang ph√¢n trang
        safe_print("... ƒêang l·∫•y danh s√°ch ch∆∞∆°ng t·ª´ t·∫•t c·∫£ c√°c trang")
        chapter_urls = self._get_all_chapters_from_pagination(story_url)
        
        safe_print(f"--> T·ªïng c·ªông t√¨m th·∫•y {len(chapter_urls)} ch∆∞∆°ng t·ª´ t·∫•t c·∫£ c√°c trang.")

        # 3.5. L·∫•y reviews cho to√†n b·ªô truy·ªán
        safe_print("... ƒêang l·∫•y reviews cho to√†n b·ªô truy·ªán")
        reviews = self._scrape_reviews(story_url, story_id)
        story_data["reviews"] = reviews
        safe_print(f"‚úÖ ƒê√£ l·∫•y ƒë∆∞·ª£c {len(reviews)} reviews")

        # 4. C√†o c√°c ch∆∞∆°ng song song v·ªõi ThreadPoolExecutor (GI·ªÆ ƒê√öNG TH·ª® T·ª∞)
        safe_print(f"üöÄ B·∫Øt ƒë·∫ßu c√†o {len(chapter_urls)} ch∆∞∆°ng v·ªõi {self.max_workers} thread...")
        
        # T·∫°o list k·∫øt qu·∫£ c·ªë ƒë·ªãnh theo index - m·ªói index = 1 ch∆∞∆°ng
        chapter_results = [None] * len(chapter_urls)
        
        # Dictionary ƒë·ªÉ map future -> index ƒë·ªÉ bi·∫øt ch∆∞∆°ng n√†o
        future_to_index = {}
        
        # S·ª≠ d·ª•ng ThreadPoolExecutor - N√ì T·ª∞ ƒê·ªòNG PH√ÇN PH·ªêI c√¥ng vi·ªác!
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            # Submit T·∫§T C·∫¢ chapters v√†o pool - m·ªói ch∆∞∆°ng ch·ªâ submit 1 L·∫¶N
            for index, chap_url in enumerate(chapter_urls):
                future = executor.submit(self._scrape_single_chapter_worker, chap_url, index, story_id)
                future_to_index[future] = index
            
            # Thu th·∫≠p k·∫øt qu·∫£ - c√°c thread c√≥ th·ªÉ ho√†n th√†nh b·∫•t k·ª≥ l√∫c n√†o
            completed = 0
            for future in as_completed(future_to_index):
                index = future_to_index[future]  # L·∫•y index c·ªßa ch∆∞∆°ng n√†y
                try:
                    chapter_data = future.result()
                    # L∆ØU V√ÄO ƒê√öNG V·ªä TR√ç INDEX - kh√¥ng ph·∫£i append!
                    chapter_results[index] = chapter_data
                    completed += 1
                    status = "‚úÖ" if chapter_data else "‚ö†Ô∏è"
                    safe_print(f"    {status} Ho√†n th√†nh ch∆∞∆°ng {index + 1}/{len(chapter_urls)} (ƒë√£ xong {completed}/{len(chapter_urls)})")
                except Exception as e:
                    safe_print(f"    ‚ùå L·ªói khi c√†o ch∆∞∆°ng {index + 1}: {e}")
                    chapter_results[index] = None

        # SAU KHI T·∫§T C·∫¢ XONG: Th√™m v√†o story_data THEO ƒê√öNG TH·ª® T·ª∞
        safe_print(f"üìù S·∫Øp x·∫øp k·∫øt qu·∫£ theo ƒë√∫ng th·ª© t·ª±...")
        for index in range(len(chapter_results)):
            chapter_data = chapter_results[index]
            if chapter_data:
                story_data["chapters"].append(chapter_data)
            else:
                safe_print(f"    ‚ö†Ô∏è B·ªè qua ch∆∞∆°ng {index + 1} (l·ªói ho·∫∑c kh√¥ng c√≥ d·ªØ li·ªáu)")

        safe_print(f"‚úÖ ƒê√£ ho√†n th√†nh {len(story_data['chapters'])}/{len(chapter_urls)} ch∆∞∆°ng (theo ƒë√∫ng th·ª© t·ª±)")

        # 5. C·∫≠p nh·∫≠t story trong MongoDB v·ªõi ƒë·∫ßy ƒë·ªß chapters v√† reviews
        self._save_story_to_mongo(story_data)
        
        # 6. L∆∞u k·∫øt qu·∫£ ra JSON (backup)
        self._save_to_json(story_data)

    def _get_all_chapters_from_pagination(self, story_url):
        """
        L·∫•y t·∫•t c·∫£ chapters t·ª´ t·∫•t c·∫£ c√°c trang ph√¢n trang
        Pagination s·ª≠ d·ª•ng JavaScript (AJAX), kh√¥ng ƒë·ªïi URL
        Tr·∫£ v·ªÅ danh s√°ch URL c·ªßa t·∫•t c·∫£ chapters
        """
        all_chapter_urls = []
        
        try:
            # Trang ƒë·∫ßu ti√™n: L·∫•y t·ª´ trang story ch√≠nh
            safe_print(f"    üìÑ ƒêang l·∫•y chapters t·ª´ trang 1 (trang story ch√≠nh)...")
            self.page.goto(story_url, timeout=config.TIMEOUT)
            time.sleep(2)
            
            # L·∫•y chapters t·ª´ trang story ch√≠nh
            page_chapters = self._get_chapters_from_current_page()
            all_chapter_urls.extend(page_chapters)
            safe_print(f"    ‚úÖ Trang 1: L·∫•y ƒë∆∞·ª£c {len(page_chapters)} chapters")
            
            # T√¨m s·ªë trang t·ªëi ƒëa cho chapters t·ª´ pagination tr√™n trang story ch√≠nh
            max_page = self._get_max_chapter_page()
            
            # N·∫øu ch·ªâ c√≥ 1 trang, return lu√¥n
            if max_page <= 1:
                safe_print(f"    üìö Ch·ªâ c√≥ 1 trang chapters")
                return all_chapter_urls
            
            safe_print(f"    üìö T√¨m th·∫•y {max_page} trang chapters (trang 1 ƒë√£ l·∫•y, c√≤n {max_page - 1} trang n·ªØa)")
            
            # Loop qua t·ª´ng trang c√≤n l·∫°i (t·ª´ trang 2 tr·ªü ƒëi)
            # S·ª≠ d·ª•ng click v√†o pagination ƒë·ªÉ load th√™m chapters (AJAX, kh√¥ng ƒë·ªïi URL)
            for page_num in range(2, max_page + 1):
                safe_print(f"    üìÑ ƒêang l·∫•y chapters t·ª´ trang {page_num}/{max_page}...")
                
                # Click v√†o n√∫t pagination ƒë·ªÉ chuy·ªÉn trang (AJAX load, kh√¥ng ƒë·ªïi URL)
                if not self._go_to_chapter_page(page_num):
                    safe_print(f"    ‚ö†Ô∏è Kh√¥ng th·ªÉ chuy·ªÉn ƒë·∫øn trang {page_num}, d·ª´ng l·∫°i")
                    break
                
                # ƒê·ª£i AJAX load xong
                time.sleep(2)
                
                # L·∫•y chapters t·ª´ trang hi·ªán t·∫°i
                page_chapters = self._get_chapters_from_current_page()
                all_chapter_urls.extend(page_chapters)
                
                safe_print(f"    ‚úÖ Trang {page_num}: L·∫•y ƒë∆∞·ª£c {len(page_chapters)} chapters")
                
                # Delay gi·ªØa c√°c trang
                if page_num < max_page:
                    time.sleep(1)
            
            return all_chapter_urls
            
        except Exception as e:
            safe_print(f"    ‚ö†Ô∏è L·ªói khi l·∫•y chapters t·ª´ pagination: {e}")
            # Fallback: L·∫•y t·ª´ trang ƒë·∫ßu ti√™n (trang story ch√≠nh)
            try:
                self.page.goto(story_url, timeout=config.TIMEOUT)
                time.sleep(2)
                return self._get_chapters_from_current_page()
            except:
                return []

    def _get_max_chapter_page(self):
        """L·∫•y s·ªë trang chapters t·ªëi ƒëa t·ª´ pagination"""
        try:
            # Scroll xu·ªëng ƒë·ªÉ load pagination
            self.page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
            time.sleep(2)
            
            max_page = 1  # M·∫∑c ƒë·ªãnh l√† 1 trang
            
            # T√¨m pagination element - c√≥ th·ªÉ l√† pagination-small ho·∫∑c pagination
            pagination_selectors = [
                "ul.pagination-small",
                "ul.pagination",
                ".pagination-small",
                ".pagination"
            ]
            
            pagination = None
            for selector in pagination_selectors:
                try:
                    pagination = self.page.locator(selector).first
                    if pagination.count() > 0:
                        break
                except:
                    continue
            
            if pagination and pagination.count() > 0:
                # L·∫•y t·∫•t c·∫£ c√°c link c√≥ data-page attribute
                page_links = pagination.locator("a[data-page]").all()
                
                page_numbers = []
                for link in page_links:
                    try:
                        page_num_str = link.get_attribute("data-page")
                        if page_num_str:
                            page_num = int(page_num_str)
                            page_numbers.append(page_num)
                    except:
                        continue
                
                # N·∫øu kh√¥ng c√≥ data-page, th·ª≠ l·∫•y t·ª´ text content
                if not page_numbers:
                    try:
                        all_links = pagination.locator("a").all()
                        for link in all_links:
                            try:
                                link_text = link.inner_text().strip()
                                # B·ªè qua c√°c n√∫t navigation (Next, Previous) v√† icon
                                if link_text.isdigit():
                                    page_num = int(link_text)
                                    page_numbers.append(page_num)
                            except:
                                continue
                    except:
                        pass
                
                if page_numbers:
                    max_page = max(page_numbers)
                    safe_print(f"        üìÑ T√¨m th·∫•y {max_page} trang chapters")
                else:
                    # N·∫øu kh√¥ng t√¨m th·∫•y s·ªë trang, c√≥ th·ªÉ ch·ªâ c√≥ 1 trang
                    safe_print(f"        üìÑ Kh√¥ng t√¨m th·∫•y pagination, gi·∫£ s·ª≠ c√≥ 1 trang")
            
            return max_page
        except Exception as e:
            safe_print(f"        ‚ö†Ô∏è L·ªói khi l·∫•y s·ªë trang chapters: {e}")
            return 1

    def _get_chapter_page_urls(self, base_url, max_page):
        """L·∫•y t·∫•t c·∫£ URL c·ªßa c√°c trang chapters t·ª´ pagination"""
        page_urls = [base_url]  # Trang 1 l√† base_url
        
        try:
            # T√¨m pagination
            pagination_selectors = [
                "ul.pagination-small",
                "ul.pagination",
                ".pagination-small",
                ".pagination"
            ]
            
            pagination = None
            for selector in pagination_selectors:
                try:
                    pagination = self.page.locator(selector).first
                    if pagination.count() > 0:
                        break
                except:
                    continue
            
            if pagination and pagination.count() > 0:
                # L·∫•y t·∫•t c·∫£ c√°c link c√≥ data-page attribute
                page_links = pagination.locator("a[data-page]").all()
                
                url_map = {}  # {page_num: url}
                for link in page_links:
                    try:
                        page_num_str = link.get_attribute("data-page")
                        if page_num_str:
                            page_num = int(page_num_str)
                            href = link.get_attribute("href")
                            if href:
                                # T·∫°o full URL
                                if href.startswith("/"):
                                    full_url = config.BASE_URL + href
                                elif href.startswith("http"):
                                    full_url = href
                                else:
                                    full_url = config.BASE_URL + "/" + href
                                url_map[page_num] = full_url
                    except:
                        continue
                
                # S·∫Øp x·∫øp v√† th√™m v√†o list
                for page_num in sorted(url_map.keys()):
                    if page_num <= max_page:
                        page_urls.append(url_map[page_num])
            
        except Exception as e:
            safe_print(f"        ‚ö†Ô∏è L·ªói khi l·∫•y URLs t·ª´ pagination: {e}")
        
        return page_urls

    def _go_to_chapter_page(self, page_num):
        """
        Chuy·ªÉn ƒë·∫øn trang chapters c·ª• th·ªÉ b·∫±ng c√°ch click v√†o link ho·∫∑c n√∫t Next
        Tr·∫£ v·ªÅ True n·∫øu th√†nh c√¥ng, False n·∫øu th·∫•t b·∫°i
        """
        try:
            # T√¨m pagination
            pagination_selectors = [
                "ul.pagination-small",
                "ul.pagination",
                ".pagination-small",
                ".pagination"
            ]
            
            pagination = None
            for selector in pagination_selectors:
                try:
                    pagination = self.page.locator(selector).first
                    if pagination.count() > 0:
                        break
                except:
                    continue
            
            if not pagination or pagination.count() == 0:
                return False
            
            # C√°ch 1: Th·ª≠ t√¨m link c√≥ data-page = page_num
            try:
                page_link = pagination.locator(f'a[data-page="{page_num}"]').first
                if page_link.count() > 0:
                    page_link.click()
                    time.sleep(2)
                    return True
            except:
                pass
            
            # C√°ch 2: N·∫øu kh√¥ng c√≥ data-page, th·ª≠ t√¨m link c√≥ text = page_num
            # L·∫•y t·∫•t c·∫£ c√°c link trong pagination v√† t√¨m link c√≥ text = page_num
            try:
                all_links = pagination.locator("a").all()
                for link in all_links:
                    try:
                        link_text = link.inner_text().strip()
                        # Ki·ªÉm tra xem text c√≥ ph·∫£i l√† s·ªë v√† b·∫±ng page_num kh√¥ng
                        if link_text.isdigit() and int(link_text) == page_num:
                            # Ki·ªÉm tra xem kh√¥ng ph·∫£i l√† n√∫t navigation (kh√¥ng c√≥ class nav-arrow)
                            parent_class = link.evaluate("el => el.closest('li')?.className || ''")
                            if "nav-arrow" not in parent_class:
                                link.click()
                                time.sleep(2)
                                return True
                    except:
                        continue
            except:
                pass
            
            # C√°ch 3: Click n√∫t "Next" nhi·ªÅu l·∫ßn (ch·ªâ d√πng n·∫øu page_num nh·ªè)
            # T√¨m n√∫t Next (c√≥ class nav-arrow ho·∫∑c ch·ª©a icon chevron-right)
            if page_num <= 10:  # Gi·ªõi h·∫°n ƒë·ªÉ tr√°nh click qu√° nhi·ªÅu
                # T√¨m trang hi·ªán t·∫°i
                current_page = 1
                try:
                    active_page = pagination.locator("li.page-active a").first
                    if active_page.count() > 0:
                        active_text = active_page.inner_text().strip()
                        if active_text.isdigit():
                            current_page = int(active_text)
                except:
                    pass
                
                # Click Next cho ƒë·∫øn khi ƒë·∫øn trang c·∫ßn
                while current_page < page_num:
                    # T√¨m n√∫t Next (c√≥ th·ªÉ l√† .nav-arrow v·ªõi icon chevron-right)
                    next_selectors = [
                        'a.pagination-button:has(i.fa-chevron-right)',
                        '.nav-arrow a:has(i.fa-chevron-right)',
                        'a:has(i.fa-chevron-right)',
                        '.nav-arrow a',
                        'a.pagination-button'
                    ]
                    
                    next_button = None
                    for selector in next_selectors:
                        try:
                            next_button = pagination.locator(selector).last  # L·∫•y n√∫t cu·ªëi (Next)
                            if next_button.count() > 0:
                                # Ki·ªÉm tra xem c√≥ ph·∫£i n√∫t Next kh√¥ng (kh√¥ng ph·∫£i Previous)
                                href = next_button.get_attribute("href") or ""
                                if "page" in href.lower() or "next" in href.lower() or not href:
                                    break
                        except:
                            continue
                    
                    if next_button and next_button.count() > 0:
                        try:
                            next_button.click()
                            time.sleep(2)
                            current_page += 1
                        except:
                            return False
                    else:
                        return False
                
                return True
            
            return False
            
        except Exception as e:
            safe_print(f"        ‚ö†Ô∏è L·ªói khi chuy·ªÉn ƒë·∫øn trang {page_num}: {e}")
            return False

    def _get_chapters_from_current_page(self):
        """L·∫•y danh s√°ch chapters t·ª´ trang hi·ªán t·∫°i"""
        chapter_urls = []
        
        try:
            # L·∫•y t·∫•t c·∫£ c√°c rows trong table chapters
            chapter_rows = self.page.locator("table#chapters tbody tr").all()
            
            for row in chapter_rows:
                try:
                    link_el = row.locator("td").first.locator("a")
                    if link_el.count() > 0:
                        url = link_el.get_attribute("href")
                        if url:
                            # T·∫°o full URL
                            if url.startswith("/"):
                                full_url = config.BASE_URL + url
                            elif url.startswith("http"):
                                full_url = url
                            else:
                                full_url = config.BASE_URL + "/" + url
                            
                            # Tr√°nh duplicate
                            if full_url not in chapter_urls:
                                chapter_urls.append(full_url)
                except:
                    continue
            
            return chapter_urls
            
        except Exception as e:
            safe_print(f"        ‚ö†Ô∏è L·ªói khi l·∫•y chapters t·ª´ trang hi·ªán t·∫°i: {e}")
            return []

    def _convert_html_to_formatted_text(self, html_content):
        """
        Chuy·ªÉn ƒë·ªïi HTML sang text v·ªõi ƒë·ªãnh d·∫°ng ƒë√∫ng (gi·ªØ nguy√™n xu·ªëng d√≤ng nh∆∞ trong UI)
        - M·ªói th·∫ª <p> = m·ªôt ƒëo·∫°n vƒÉn, c√°c ƒëo·∫°n c√°ch nhau b·∫±ng m·ªôt d√≤ng tr·ªëng
        - Th·∫ª <br> = xu·ªëng d√≤ng
        - Gi·ªØ nguy√™n c·∫•u tr√∫c nh∆∞ trong UI
        """
        if not html_content:
            return ""
        
        import html as html_module
        
        # Decode HTML entities tr∆∞·ªõc
        html_content = html_module.unescape(html_content)
        
        # X·ª≠ l√Ω theo th·ª© t·ª± ƒë·ªÉ ƒë·∫£m b·∫£o ƒë·ªãnh d·∫°ng ƒë√∫ng
        text = html_content
        
        # 1. X·ª≠ l√Ω <br> v√† <br/> tr∆∞·ªõc - xu·ªëng d√≤ng ngay l·∫≠p t·ª©c
        text = re.sub(r'<br\s*/?>', '\n', text, flags=re.IGNORECASE)
        
        # 2. X·ª≠ l√Ω c√°c th·∫ª block: <p> - m·ªói ƒëo·∫°n vƒÉn c√°ch nhau 1 d√≤ng tr·ªëng
        # Thay th·∫ø </p> th√†nh d·∫•u ph√¢n c√°ch ƒëo·∫°n (2 d√≤ng xu·ªëng)
        text = re.sub(r'</p>', '\n\n', text, flags=re.IGNORECASE)
        # X√≥a th·∫ª m·ªü <p>
        text = re.sub(r'<p[^>]*>', '', text, flags=re.IGNORECASE)
        
        # 3. X·ª≠ l√Ω c√°c th·∫ª block kh√°c: <div> - xu·ªëng d√≤ng
        text = re.sub(r'</div>', '\n', text, flags=re.IGNORECASE)
        text = re.sub(r'<div[^>]*>', '', text, flags=re.IGNORECASE)
        
        # 4. X·ª≠ l√Ω c√°c th·∫ª heading (h1, h2, h3, ...) - xu·ªëng d√≤ng tr∆∞·ªõc v√† sau
        text = re.sub(r'</h[1-6]>', '\n\n', text, flags=re.IGNORECASE)
        text = re.sub(r'<h[1-6][^>]*>', '\n', text, flags=re.IGNORECASE)
        
        # 5. X√≥a t·∫•t c·∫£ c√°c th·∫ª HTML c√≤n l·∫°i (gi·ªØ l·∫°i text)
        text = re.sub(r'<[^>]+>', '', text)
        
        # 6. L√†m s·∫°ch: x·ª≠ l√Ω c√°c d√≤ng tr·ªëng v√† kho·∫£ng tr·∫Øng th·ª´a
        lines = text.split('\n')
        cleaned_lines = []
        
        prev_empty = False
        for line in lines:
            # Strip c·∫£ 2 b√™n ƒë·ªÉ lo·∫°i b·ªè kho·∫£ng tr·∫Øng th·ª´a (t·ª´ HTML indentation)
            stripped_line = line.strip()
            
            # X·ª≠ l√Ω d√≤ng tr·ªëng
            if not stripped_line:
                # Ch·ªâ th√™m 1 d√≤ng tr·ªëng gi·ªØa c√°c ƒëo·∫°n (kh√¥ng th√™m nhi·ªÅu d√≤ng tr·ªëng li√™n ti·∫øp)
                if not prev_empty:
                    cleaned_lines.append('')
                prev_empty = True
            else:
                # Gi·ªØ nguy√™n d√≤ng c√≥ n·ªôi dung (ƒë√£ strip kho·∫£ng tr·∫Øng th·ª´a)
                cleaned_lines.append(stripped_line)
                prev_empty = False
        
        # Lo·∫°i b·ªè d√≤ng tr·ªëng ·ªü ƒë·∫ßu v√† cu·ªëi (nh∆∞ng gi·ªØ d√≤ng tr·ªëng gi·ªØa c√°c ƒëo·∫°n)
        while cleaned_lines and not cleaned_lines[0].strip():
            cleaned_lines.pop(0)
        while cleaned_lines and not cleaned_lines[-1].strip():
            cleaned_lines.pop()
        
        result = '\n'.join(cleaned_lines)
        
        # Lo·∫°i b·ªè kho·∫£ng tr·∫Øng th·ª´a ·ªü ƒë·∫ßu v√† cu·ªëi to√†n b·ªô text
        # Nh∆∞ng v·∫´n gi·ªØ nguy√™n c·∫•u tr√∫c b√™n trong (c√°c d√≤ng tr·ªëng gi·ªØa ƒëo·∫°n)
        result = result.strip()
        
        # ƒê·∫£m b·∫£o kh√¥ng c√≥ kho·∫£ng tr·∫Øng th·ª´a ·ªü ƒë·∫ßu m·ªói d√≤ng (t·ª´ HTML indentation)
        # Normalize l·∫°i ƒë·ªÉ ch·∫Øc ch·∫Øn
        if result:
            lines = result.split('\n')
            final_lines = []
            for line in lines:
                # Strip t·ª´ng d√≤ng ƒë·ªÉ lo·∫°i b·ªè kho·∫£ng tr·∫Øng th·ª´a
                clean_line = line.strip()
                # Gi·ªØ d√≤ng tr·ªëng n·∫øu l√† d√≤ng tr·ªëng th·∫≠t
                if not clean_line:
                    final_lines.append('')
                else:
                    final_lines.append(clean_line)
            result = '\n'.join(final_lines).strip()
        
        return result

    def _scrape_single_chapter(self, url):
        """H√†m con: Ch·ªâ ch·ªãu tr√°ch nhi·ªám v√†o 1 link ch∆∞∆°ng v√† tr·∫£ v·ªÅ c·ª•c data c·ªßa ch∆∞∆°ng ƒë√≥"""
        try:
            self.page.goto(url, timeout=config.TIMEOUT)
            self.page.wait_for_selector(".chapter-inner", timeout=10000)

            title = self.page.locator("h1").first.inner_text()
            
            # L·∫•y content v·ªõi ƒë·ªãnh d·∫°ng ƒë√∫ng (gi·ªØ nguy√™n xu·ªëng d√≤ng nh∆∞ trong UI)
            content = ""
            try:
                content_container = self.page.locator(".chapter-inner").first
                if content_container.count() > 0:
                    # L·∫•y HTML ƒë·ªÉ gi·ªØ ƒë·ªãnh d·∫°ng
                    html_content = content_container.inner_html()
                    # Chuy·ªÉn HTML sang text v·ªõi ƒë·ªãnh d·∫°ng ƒë√∫ng
                    content = self._convert_html_to_formatted_text(html_content)
                else:
                    # Fallback: d√πng inner_text n·∫øu kh√¥ng t√¨m th·∫•y
                    content = self.page.locator(".chapter-inner").inner_text()
            except Exception as e:
                safe_print(f"      ‚ö†Ô∏è L·ªói khi l·∫•y content: {e}")
                content = self.page.locator(".chapter-inner").inner_text()

            # L·∫•y published_time
            published_time = ""
            try:
                time_elem = self.page.locator("time, .timestamp, [class*='time'], [class*='date'], [datetime]").first
                if time_elem.count() > 0:
                    published_time = time_elem.get_attribute("datetime") or time_elem.inner_text().strip()
            except:
                pass
            
            # L·∫•y chapter_id t·ª´ URL (v√≠ d·ª•: /chapter/123456/ -> 123456)
            chapter_id = ""
            try:
                url_parts = url.split("/chapter/")
                if len(url_parts) > 1:
                    chapter_id = url_parts[1].split("/")[0]
            except:
                chapter_id = ""
            
            # L·∫•y comments cho chapter n√†y
            safe_print(f"      ... ƒêang l·∫•y comments cho ch∆∞∆°ng")
            chapter_comments = self._scrape_comments(url, "chapter", chapter_id)

            return {
                "id": chapter_id,  # Schema: chapter id
                "name": title,  # Schema: chapter name
                "url": url,  # Schema: chapter url
                "content": content,  # Schema: content
                "published_time": published_time,  # Schema: published time
                "story_id": "",  # S·∫Ω ƒë∆∞·ª£c ƒëi·ªÅn sau n·∫øu c·∫ßn
                "comments": chapter_comments
            }
        except Exception as e:
            safe_print(f"‚ö†Ô∏è L·ªói c√†o ch∆∞∆°ng {url}: {e}")
            return None

    def _scrape_single_chapter_worker(self, url, index, story_id):
        """
        Worker function ƒë·ªÉ c√†o M·ªòT ch∆∞∆°ng - m·ªói worker c√≥ browser instance ri√™ng
        Thread-safe: M·ªói worker c√≥ browser instance ri√™ng
        
        Args:
            url: URL c·ªßa ch∆∞∆°ng c·∫ßn c√†o (DUY NH·∫§T - kh√¥ng tr√πng l·∫∑p)
            index: Th·ª© t·ª± ch∆∞∆°ng trong list (DUY NH·∫§T - kh√¥ng tr√πng l·∫∑p)
            story_id: ID c·ªßa story (FK)
        """
        worker_playwright = None
        worker_browser = None
        
        try:
            # Delay ƒë·ªÉ stagger c√°c thread - tr√°nh t·∫•t c·∫£ thread b·∫Øt ƒë·∫ßu c√πng l√∫c
            time.sleep(index * config.DELAY_THREAD_START)
            
            # T·∫°o browser instance ri√™ng cho worker n√†y
            worker_playwright = sync_playwright().start()
            worker_browser = worker_playwright.chromium.launch(headless=config.HEADLESS)
            worker_context = worker_browser.new_context()
            worker_page = worker_context.new_page()
            
            safe_print(f"    üîÑ Thread-{index}: ƒêang c√†o ch∆∞∆°ng {index + 1}")
            
            # Delay tr∆∞·ªõc khi request ƒë·ªÉ tr√°nh ban IP
            time.sleep(config.DELAY_BETWEEN_REQUESTS)
            
            # C√†o ch∆∞∆°ng
            worker_page.goto(url, timeout=config.TIMEOUT)
            worker_page.wait_for_selector(".chapter-inner", timeout=10000)
            
            # Delay sau khi load page
            time.sleep(config.DELAY_BETWEEN_REQUESTS)

            title = worker_page.locator("h1").first.inner_text()
            
            # L·∫•y published_time
            published_time = ""
            try:
                time_elem = worker_page.locator("time, .timestamp, [class*='time'], [class*='date'], [datetime]").first
                if time_elem.count() > 0:
                    published_time = time_elem.get_attribute("datetime") or time_elem.inner_text().strip()
            except:
                pass
            
            # L·∫•y content v·ªõi ƒë·ªãnh d·∫°ng ƒë√∫ng
            content = ""
            try:
                content_container = worker_page.locator(".chapter-inner").first
                if content_container.count() > 0:
                    html_content = content_container.inner_html()
                    content = self._convert_html_to_formatted_text(html_content)
                else:
                    content = worker_page.locator(".chapter-inner").inner_text()
            except Exception as e:
                safe_print(f"      ‚ö†Ô∏è Thread-{index}: L·ªói khi l·∫•y content: {e}")
                content = worker_page.locator(".chapter-inner").inner_text()

            # Delay tr∆∞·ªõc khi l·∫•y comments
            time.sleep(config.DELAY_BETWEEN_REQUESTS)
            
            # L·∫•y chapter_id t·ª´ URL (v√≠ d·ª•: /chapter/123456/ -> 123456)
            chapter_id = ""
            try:
                url_parts = url.split("/chapter/")
                if len(url_parts) > 1:
                    chapter_id = url_parts[1].split("/")[0]
            except:
                chapter_id = ""
            
            # L·∫•y comments cho chapter n√†y (c·∫ßn chapter_id ƒë·ªÉ th√™m v√†o m·ªói comment)
            safe_print(f"      üí¨ Thread-{index}: ƒêang l·∫•y comments cho ch∆∞∆°ng")
            chapter_comments = self._scrape_comments_worker(worker_page, url, "chapter", chapter_id)

            # Delay sau khi ho√†n th√†nh ch∆∞∆°ng
            time.sleep(config.DELAY_BETWEEN_CHAPTERS)

            chapter_data = {
                "id": chapter_id,  # Schema: chapter id
                "name": title,  # Schema: chapter name
                "url": url,  # Schema: chapter url
                "content": content,  # Schema: content
                "published_time": published_time,  # Schema: published time
                "story_id": story_id,  # Schema: story id (FK)
                "comments": chapter_comments
            }
            
            # L∆∞u chapter ngay v√†o MongoDB (sau khi ƒë√£ c√†o xong chapter v√† comments)
            self._save_chapter_to_mongo(chapter_data)
            
            return chapter_data
            
        except Exception as e:
            safe_print(f"‚ö†Ô∏è Thread-{index}: L·ªói c√†o ch∆∞∆°ng {index + 1}: {e}")
            return None
        finally:
            # ƒê√≥ng browser c·ªßa worker
            if worker_browser:
                worker_browser.close()
            if worker_playwright:
                worker_playwright.stop()

    def _get_max_comment_page(self, url):
        """L·∫•y s·ªë trang comments t·ªëi ƒëa t·ª´ pagination"""
        try:
            # ƒê·∫£m b·∫£o ƒëang ·ªü ƒë√∫ng trang (trang 1 - kh√¥ng c√≥ query comments)
            base_url = url.split('?')[0]
            current_url = self.page.url.split('?')[0]
            
            if base_url not in current_url:
                self.page.goto(base_url, timeout=config.TIMEOUT)
                time.sleep(2)
            
            # Scroll xu·ªëng ƒë·ªÉ load pagination
            self.page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
            time.sleep(2)
            
            max_page = 1  # M·∫∑c ƒë·ªãnh l√† 1 trang
            
            # T√¨m pagination element - c√≥ th·ªÉ trong .chapter-nav ho·∫∑c tr·ª±c ti·∫øp
            pagination_selectors = [
                "ul.pagination",
                ".chapter-nav ul.pagination",
                ".pagination"
            ]
            
            pagination = None
            for selector in pagination_selectors:
                try:
                    pagination = self.page.locator(selector).first
                    if pagination.count() > 0:
                        break
                except:
                    continue
            
            if pagination and pagination.count() > 0:
                # L·∫•y t·∫•t c·∫£ c√°c link c√≥ data-page attribute
                page_links = pagination.locator("a[data-page]").all()
                
                page_numbers = []
                for link in page_links:
                    try:
                        page_num_str = link.get_attribute("data-page")
                        if page_num_str:
                            page_num = int(page_num_str)
                            page_numbers.append(page_num)
                    except:
                        continue
                
                # C≈©ng th·ª≠ l·∫•y t·ª´ text content (n·∫øu kh√¥ng c√≥ data-page)
                if not page_numbers:
                    try:
                        all_links = pagination.locator("a").all()
                        for link in all_links:
                            try:
                                link_text = link.inner_text().strip()
                                # Th·ª≠ parse s·ªë t·ª´ text (v√≠ d·ª•: "31", "Next >" s·∫Ω b·ªã skip)
                                if link_text.isdigit():
                                    page_num = int(link_text)
                                    page_numbers.append(page_num)
                            except:
                                continue
                    except:
                        pass
                
                if page_numbers:
                    max_page = max(page_numbers)
                    safe_print(f"        üìÑ T√¨m th·∫•y {max_page} trang comments")
                else:
                    # N·∫øu kh√¥ng t√¨m th·∫•y s·ªë trang, c√≥ th·ªÉ ch·ªâ c√≥ 1 trang ho·∫∑c ch∆∞a load
                    safe_print(f"        üìÑ Kh√¥ng t√¨m th·∫•y pagination, gi·∫£ s·ª≠ c√≥ 1 trang")
            
            return max_page
        except Exception as e:
            safe_print(f"        ‚ö†Ô∏è L·ªói khi l·∫•y s·ªë trang: {e}")
            return 1  # N·∫øu l·ªói, m·∫∑c ƒë·ªãnh ch·ªâ c√≥ 1 trang

    def _scrape_comments_from_page(self, page_url, chapter_id=""):
        """L·∫•y comments t·ª´ m·ªôt trang c·ª• th·ªÉ, tr·∫£ v·ªÅ danh s√°ch ph·∫≥ng (flat)"""
        comments = []
        
        try:
            self.page.goto(page_url, timeout=config.TIMEOUT)
            time.sleep(2)  # Ch·ªù page load
            
            # Scroll xu·ªëng ƒë·ªÉ load comments (lazy load)
            self.page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
            time.sleep(2)
            
            # L·∫•y t·∫•t c·∫£ div.comment v√† filter nh·ªØng c√°i kh√¥ng n·∫±m trong ul.subcomments
            all_comments = self.page.locator("div.comment").all()
            
            for comment_elem in all_comments:
                try:
                    # Ki·ªÉm tra xem comment n√†y c√≥ n·∫±m trong ul.subcomments kh√¥ng
                    is_in_subcomments = comment_elem.evaluate("""
                        el => {
                            let parent = el.parentElement;
                            while (parent) {
                                if (parent.tagName === 'UL' && parent.classList.contains('subcomments')) {
                                    return true;
                                }
                                parent = parent.parentElement;
                            }
                            return false;
                        }
                    """)
                    
                    # N·∫øu n·∫±m trong subcomments th√¨ skip (ƒë√¢y l√† reply, s·∫Ω ƒë∆∞·ª£c l·∫•y ƒë·ªá quy)
                    if is_in_subcomments:
                        continue
                    
                    # ƒê√¢y l√† comment g·ªëc, l·∫•y n√≥ v√† t·∫•t c·∫£ replies (flatten)
                    comment_list = self._scrape_single_comment_recursive(comment_elem, chapter_id, parent_id=None)
                    if comment_list:
                        comments.extend(comment_list)
                except Exception as e:
                    continue
            
            return comments
            
        except Exception as e:
            safe_print(f"        ‚ö†Ô∏è L·ªói khi l·∫•y comments t·ª´ trang: {e}")
            return []

    def _scrape_comments(self, url, comment_type="chapter", chapter_id=""):
        """
        L·∫•y t·∫•t c·∫£ comments t·ª´ T·∫§T C·∫¢ c√°c trang ph√¢n trang
        Tr·∫£ v·ªÅ danh s√°ch comments ph·∫≥ng (flat) v·ªõi parent_id thay v√¨ nested
        """
        try:
            # ƒê·∫£m b·∫£o ƒëang ·ªü ƒë√∫ng trang ƒë·ªÉ ki·ªÉm tra pagination
            current_url = self.page.url
            if url not in current_url:
                self.page.goto(url, timeout=config.TIMEOUT)
                time.sleep(2)
            
            safe_print(f"      üí¨ ƒêang l·∫•y comments ({comment_type}-level)...")
            
            # B∆∞·ªõc 1: T√¨m s·ªë trang t·ªëi ƒëa
            max_page = self._get_max_comment_page(url)
            
            all_comments = []
            
            # B∆∞·ªõc 2: L·∫•y comments t·ª´ t·∫•t c·∫£ c√°c trang
            for page_num in range(1, max_page + 1):
                safe_print(f"        üìÑ ƒêang l·∫•y trang {page_num}/{max_page}...")
                
                # T·∫°o URL cho trang n√†y
                if page_num == 1:
                    # Trang 1: Lo·∫°i b·ªè query parameter comments n·∫øu c√≥
                    base_url = url.split('?')[0]  # L·∫•y URL g·ªëc kh√¥ng c√≥ query
                    page_url = base_url
                else:
                    # Trang kh√°c: Th√™m query parameter comments=N
                    base_url = url.split('?')[0]  # L·∫•y URL g·ªëc
                    # T√¨m c√°c query parameter hi·ªán c√≥ (tr·ª´ comments)
                    if '?' in url:
                        existing_params = url.split('?', 1)[1]
                        # Lo·∫°i b·ªè comments parameter n·∫øu c√≥
                        params_list = []
                        for param in existing_params.split('&'):
                            if not param.startswith('comments='):
                                params_list.append(param)
                        if params_list:
                            other_params = '&'.join(params_list)
                            page_url = f"{base_url}?{other_params}&comments={page_num}"
                        else:
                            page_url = f"{base_url}?comments={page_num}"
                    else:
                        page_url = f"{base_url}?comments={page_num}"
                
                # L·∫•y comments t·ª´ trang n√†y
                page_comments = self._scrape_comments_from_page(page_url, chapter_id)
                all_comments.extend(page_comments)
                
                safe_print(f"        ‚úÖ Trang {page_num}: L·∫•y ƒë∆∞·ª£c {len(page_comments)} comments")
                
                # Delay gi·ªØa c√°c trang ƒë·ªÉ tr√°nh b·ªã ban
                if page_num < max_page:
                    time.sleep(1)
            
            safe_print(f"      ‚úÖ T·ªïng c·ªông l·∫•y ƒë∆∞·ª£c {len(all_comments)} comments t·ª´ {max_page} trang ({comment_type}-level)")
            return all_comments
            
        except Exception as e:
            safe_print(f"      ‚ö†Ô∏è L·ªói khi l·∫•y comments: {e}")
            return []

    def _scrape_comments_worker(self, page, url, comment_type="chapter", chapter_id=""):
        """
        Worker function ƒë·ªÉ l·∫•y comments - d√πng page t·ª´ worker thay v√¨ self.page
        """
        try:
            current_url = page.url
            if url not in current_url:
                # Delay tr∆∞·ªõc khi request comments
                time.sleep(config.DELAY_BETWEEN_REQUESTS)
                page.goto(url, timeout=config.TIMEOUT)
                time.sleep(2)
            
            safe_print(f"      üí¨ ƒêang l·∫•y comments ({comment_type}-level)...")
            
            # Delay tr∆∞·ªõc khi l·∫•y s·ªë trang
            time.sleep(config.DELAY_BETWEEN_REQUESTS)
            
            # T√¨m s·ªë trang t·ªëi ƒëa
            max_page = self._get_max_comment_page_worker(page, url)
            
            all_comments = []
            
            # L·∫•y comments t·ª´ t·∫•t c·∫£ c√°c trang
            for page_num in range(1, max_page + 1):
                safe_print(f"        üìÑ ƒêang l·∫•y trang {page_num}/{max_page}...")
                
                # T·∫°o URL cho trang n√†y
                if page_num == 1:
                    base_url = url.split('?')[0]
                    page_url = base_url
                else:
                    base_url = url.split('?')[0]
                    if '?' in url:
                        existing_params = url.split('?', 1)[1]
                        params_list = []
                        for param in existing_params.split('&'):
                            if not param.startswith('comments='):
                                params_list.append(param)
                        if params_list:
                            other_params = '&'.join(params_list)
                            page_url = f"{base_url}?{other_params}&comments={page_num}"
                        else:
                            page_url = f"{base_url}?comments={page_num}"
                    else:
                        page_url = f"{base_url}?comments={page_num}"
                
                # Delay tr∆∞·ªõc khi request trang comments
                if page_num > 1:
                    time.sleep(config.DELAY_BETWEEN_REQUESTS)
                
                # L·∫•y comments t·ª´ trang n√†y
                page_comments = self._scrape_comments_from_page_worker(page, page_url, chapter_id)
                all_comments.extend(page_comments)
                
                safe_print(f"        ‚úÖ Trang {page_num}: L·∫•y ƒë∆∞·ª£c {len(page_comments)} comments")
                
                # Delay gi·ªØa c√°c trang comments
                if page_num < max_page:
                    time.sleep(config.DELAY_BETWEEN_REQUESTS)
            
            safe_print(f"      ‚úÖ T·ªïng c·ªông l·∫•y ƒë∆∞·ª£c {len(all_comments)} comments t·ª´ {max_page} trang ({comment_type}-level)")
            return all_comments
            
        except Exception as e:
            safe_print(f"      ‚ö†Ô∏è L·ªói khi l·∫•y comments: {e}")
            return []

    def _get_max_comment_page_worker(self, page, url):
        """L·∫•y s·ªë trang comments t·ªëi ƒëa t·ª´ pagination - d√πng page t·ª´ worker"""
        try:
            base_url = url.split('?')[0]
            current_url = page.url.split('?')[0]
            
            if base_url not in current_url:
                page.goto(base_url, timeout=config.TIMEOUT)
                time.sleep(2)
            
            page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
            time.sleep(2)
            
            max_page = 1
            
            pagination_selectors = [
                "ul.pagination",
                ".chapter-nav ul.pagination",
                ".pagination"
            ]
            
            pagination = None
            for selector in pagination_selectors:
                try:
                    pagination = page.locator(selector).first
                    if pagination.count() > 0:
                        break
                except:
                    continue
            
            if pagination and pagination.count() > 0:
                page_links = pagination.locator("a[data-page]").all()
                
                page_numbers = []
                for link in page_links:
                    try:
                        page_num_str = link.get_attribute("data-page")
                        if page_num_str:
                            page_num = int(page_num_str)
                            page_numbers.append(page_num)
                    except:
                        continue
                
                if not page_numbers:
                    try:
                        all_links = pagination.locator("a").all()
                        for link in all_links:
                            try:
                                link_text = link.inner_text().strip()
                                if link_text.isdigit():
                                    page_num = int(link_text)
                                    page_numbers.append(page_num)
                            except:
                                continue
                    except:
                        pass
                
                if page_numbers:
                    max_page = max(page_numbers)
            
            return max_page
        except Exception as e:
            safe_print(f"        ‚ö†Ô∏è L·ªói khi l·∫•y s·ªë trang: {e}")
            return 1

    def _scrape_comments_from_page_worker(self, page, page_url, chapter_id=""):
        """L·∫•y comments t·ª´ m·ªôt trang c·ª• th·ªÉ - d√πng page t·ª´ worker, tr·∫£ v·ªÅ danh s√°ch ph·∫≥ng"""
        comments = []
        
        try:
            # Delay tr∆∞·ªõc khi request
            time.sleep(config.DELAY_BETWEEN_REQUESTS)
            page.goto(page_url, timeout=config.TIMEOUT)
            time.sleep(2)
            
            page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
            time.sleep(2)
            
            all_comments = page.locator("div.comment").all()
            
            for comment_elem in all_comments:
                try:
                    is_in_subcomments = comment_elem.evaluate("""
                        el => {
                            let parent = el.parentElement;
                            while (parent) {
                                if (parent.tagName === 'UL' && parent.classList.contains('subcomments')) {
                                    return true;
                                }
                                parent = parent.parentElement;
                            }
                            return false;
                        }
                    """)
                    
                    if is_in_subcomments:
                        continue
                    
                    comment_list = self._scrape_single_comment_recursive(comment_elem, chapter_id, parent_id=None)
                    if comment_list:
                        comments.extend(comment_list)
                except Exception as e:
                    continue
            
            return comments
            
        except Exception as e:
            safe_print(f"        ‚ö†Ô∏è L·ªói khi l·∫•y comments t·ª´ trang: {e}")
            return []

    def _scrape_single_comment_recursive(self, comment_elem, chapter_id="", parent_id=None):
        """
        H√†m ƒë·ªá quy ƒë·ªÉ l·∫•y m·ªôt comment v√† t·∫•t c·∫£ replies c·ªßa n√≥, tr·∫£ v·ªÅ danh s√°ch ph·∫≥ng (flat)
        Schema: comment id, comment text, time, chapter id (FK), parent id (recursive FK), user id (FK)
        """
        result_list = []
        
        try:
            # L·∫•y comment container (div.media.media-v2)
            media_elem = comment_elem.locator("div.media.media-v2").first
            if media_elem.count() == 0:
                return []
            
            # L·∫•y comment ID t·ª´ id attribute
            comment_id = media_elem.get_attribute("id") or ""
            if comment_id.startswith("comment-container-"):
                comment_id = comment_id.replace("comment-container-", "")
            
            # L·∫•y user_id t·ª´ profile URL
            user_id = ""
            username = ""
            try:
                # C·∫•u tr√∫c: h4.media-heading > span.name > a[href*='/profile/']
                username_selectors = [
                    "h4.media-heading span.name a",
                    "h4.media-heading .name a",
                    ".media-heading span.name a",
                    ".media-heading .name a[href*='/profile/']",
                    "h4.media-heading a[href*='/profile/']",
                    ".media-heading a[href*='/profile/']"
                ]
                
                for selector in username_selectors:
                    try:
                        username_elem = media_elem.locator(selector).first
                        if username_elem.count() > 0:
                            username = username_elem.inner_text().strip()
                            # L·∫•y user_id t·ª´ href
                            href = username_elem.get_attribute("href") or ""
                            if "/profile/" in href:
                                user_id = href.split("/profile/")[1].split("/")[0] if "/profile/" in href else ""
                            if username:
                                break
                    except:
                        continue
                
                # N·∫øu v·∫´n kh√¥ng t√¨m th·∫•y, th·ª≠ l·∫•y t·ª´ b·∫•t k·ª≥ link profile n√†o trong media-heading
                if not username:
                    try:
                        username_elem = media_elem.locator(".media-heading a[href*='/profile/']").first
                        if username_elem.count() > 0:
                            username = username_elem.inner_text().strip()
                            href = username_elem.get_attribute("href") or ""
                            if "/profile/" in href:
                                user_id = href.split("/profile/")[1].split("/")[0] if "/profile/" in href else ""
                    except:
                        pass
                        
                if not username:
                    username = "[Unknown]"
            except:
                username = "[Unknown]"
            
            # L·∫•y comment text/content - l·∫•y t·∫•t c·∫£ c√°c ƒëo·∫°n vƒÉn ƒë·ªÉ gi·ªØ format
            comment_text = ""
            try:
                media_body = media_elem.locator(".media-body").first
                if media_body.count() > 0:
                    # L·∫•y t·∫•t c·∫£ c√°c ƒëo·∫°n vƒÉn trong comment
                    paragraphs = media_body.locator("p").all()
                    
                    if paragraphs:
                        # N·∫øu c√≥ nhi·ªÅu ƒëo·∫°n vƒÉn, n·ªëi l·∫°i v·ªõi xu·ªëng d√≤ng
                        text_parts = []
                        for para in paragraphs:
                            try:
                                para_text = para.inner_text().strip()
                                if para_text:
                                    text_parts.append(para_text)
                            except:
                                continue
                        comment_text = "\n\n".join(text_parts)
                    else:
                        # N·∫øu kh√¥ng c√≥ th·∫ª p, l·∫•y to√†n b·ªô text t·ª´ media-body
                        full_text = media_body.inner_text().strip()
                        
                        # Lo·∫°i b·ªè username n·∫øu c√≥ ·ªü ƒë·∫ßu
                        if username and full_text.startswith(username):
                            comment_text = full_text[len(username):].strip()
                        else:
                            comment_text = full_text
                        
                        # Lo·∫°i b·ªè c√°c ph·∫ßn kh√¥ng ph·∫£i n·ªôi dung (nh∆∞ timestamp, rep count)
                        # C√°c ph·∫ßn n√†y th∆∞·ªùng ·ªü cu·ªëi, c√≥ th·ªÉ c√≥ format nh∆∞ "7 years ago" ho·∫∑c "Rep (63)"
                        lines = comment_text.split('\n')
                        cleaned_lines = []
                        for line in lines:
                            line = line.strip()
                            if not line:
                                continue
                            # B·ªè qua d√≤ng ch·ª©a "years ago", "Rep (", "Reply", "Report"
                            if any(x in line.lower() for x in ['years ago', 'months ago', 'days ago', 'hours ago', 
                                                                'rep (', 'reply', 'report']):
                                continue
                            cleaned_lines.append(line)
                        comment_text = '\n'.join(cleaned_lines).strip()
            except Exception as e:
                comment_text = ""
            
            # L·∫•y timestamp
            timestamp = ""
            try:
                time_elem = media_elem.locator("time, .timestamp, [class*='time'], [class*='date']").first
                if time_elem.count() > 0:
                    timestamp = time_elem.get_attribute("datetime") or time_elem.inner_text().strip()
            except:
                pass
            
            # T·∫°o c·∫•u tr√∫c comment theo schema (flat structure)
            comment_data = {
                "comment_id": comment_id,  # Schema: comment id
                "comment_text": comment_text,  # Schema: comment text
                "time": timestamp,  # Schema: time
                "chapter_id": chapter_id,  # Schema: chapter id (FK)
                "parent_id": parent_id,  # Schema: parent id (recursive FK, None n·∫øu l√† comment g·ªëc)
                "user_id": user_id  # Schema: user id (FK)
            }
            
            # L∆∞u user n·∫øu c√≥ user_id v√† username
            if user_id and username:
                self._save_user_to_mongo(user_id, username)
            
            # L∆∞u comment ngay v√†o MongoDB (t·ª´ c·∫•p th·∫•p nh·∫•t)
            self._save_comment_to_mongo(comment_data)
            
            # Th√™m comment n√†y v√†o danh s√°ch
            result_list.append(comment_data)
            
            # L·∫•y replies (subcomments) - ƒê·ªÜ QUY (flatten)
            try:
                subcomments_list = comment_elem.locator("ul.subcomments").first
                if subcomments_list.count() > 0:
                    # L·∫•y t·∫•t c·∫£ c√°c comment con trong ul.subcomments
                    reply_comments = subcomments_list.locator("div.comment").all()
                    
                    for reply_elem in reply_comments:
                        # G·ªçi ƒë·ªá quy v·ªõi parent_id = comment_id c·ªßa comment hi·ªán t·∫°i
                        reply_list = self._scrape_single_comment_recursive(reply_elem, chapter_id, parent_id=comment_id)
                        if reply_list:
                            result_list.extend(reply_list)
            except Exception as e:
                # Kh√¥ng c√≥ replies ho·∫∑c l·ªói khi l·∫•y
                pass
            
            return result_list
            
        except Exception as e:
            safe_print(f"        ‚ö†Ô∏è L·ªói khi parse comment: {e}")
            return []

    def _scrape_reviews(self, story_url, story_id):
        """
        L·∫•y t·∫•t c·∫£ reviews t·ª´ trang story
        Schema: review id, title, time, content, user id (FK), chapter id (FK), story id (FK), score id (FK)
        """
        reviews = []
        try:
            safe_print("      üìù ƒêang l·∫•y reviews t·ª´ trang story...")
            
            # ƒê·∫£m b·∫£o ƒëang ·ªü trang story
            self.page.goto(story_url, timeout=config.TIMEOUT)
            time.sleep(2)
            
            # Scroll xu·ªëng ƒë·ªÉ load reviews section
            self.page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
            time.sleep(2)
            
            # T√¨m reviews section - c√≥ th·ªÉ l√† tab "Reviews" ho·∫∑c section ri√™ng
            # Th·ª≠ t√¨m c√°c selector ph·ªï bi·∫øn cho reviews
            review_selectors = [
                ".review",
                ".review-item",
                ".review-container",
                "[class*='review']",
                ".rating-review"
            ]
            
            review_elements = []
            for selector in review_selectors:
                try:
                    elements = self.page.locator(selector).all()
                    if elements:
                        review_elements = elements
                        safe_print(f"      ‚úÖ T√¨m th·∫•y {len(elements)} reviews v·ªõi selector: {selector}")
                        break
                except:
                    continue
            
            # N·∫øu kh√¥ng t√¨m th·∫•y v·ªõi selector th√¥ng th∆∞·ªùng, th·ª≠ t√¨m trong tabs
            if not review_elements:
                try:
                    # Th·ª≠ click v√†o tab "Reviews" n·∫øu c√≥
                    reviews_tab = self.page.locator("a[href*='reviews'], button:has-text('Reviews'), .nav-tabs a:has-text('Reviews')").first
                    if reviews_tab.count() > 0:
                        reviews_tab.click()
                        time.sleep(3)
                        # Th·ª≠ l·∫°i v·ªõi c√°c selector
                        for selector in review_selectors:
                            try:
                                elements = self.page.locator(selector).all()
                                if elements:
                                    review_elements = elements
                                    break
                            except:
                                continue
                except:
                    pass
            
            # Parse t·ª´ng review v√† l∆∞u ngay
            for review_elem in review_elements:
                try:
                    review_data = self._parse_single_review(review_elem, story_id)
                    if review_data:
                        reviews.append(review_data)
                        # L∆∞u review ngay v√†o MongoDB
                        self._save_review_to_mongo(review_data)
                except Exception as e:
                    safe_print(f"        ‚ö†Ô∏è L·ªói khi parse review: {e}")
                    continue
            
            safe_print(f"      ‚úÖ ƒê√£ l·∫•y ƒë∆∞·ª£c {len(reviews)} reviews")
            return reviews
            
        except Exception as e:
            safe_print(f"      ‚ö†Ô∏è L·ªói khi l·∫•y reviews: {e}")
            return []

    def _parse_single_review(self, review_elem, story_id):
        """
        Parse m·ªôt review element th√†nh dictionary theo schema
        Schema: review id, title, time, content, user id (FK), chapter id (FK), story id (FK), score id (FK)
        """
        try:
            # L·∫•y review ID
            review_id = ""
            try:
                review_id = review_elem.get_attribute("id") or review_elem.get_attribute("data-id") or ""
                if review_id.startswith("review-"):
                    review_id = review_id.replace("review-", "")
            except:
                pass
            
            # L·∫•y title
            title = ""
            try:
                title_elem = review_elem.locator("h3, h4, .review-title, [class*='title']").first
                if title_elem.count() > 0:
                    title = title_elem.inner_text().strip()
            except:
                pass
            
            # L·∫•y user_id t·ª´ profile URL
            user_id = ""
            try:
                username_elem = review_elem.locator("a[href*='/profile/'], .username, .reviewer-name, [class*='username']").first
                if username_elem.count() > 0:
                    href = username_elem.get_attribute("href") or ""
                    if "/profile/" in href:
                        user_id = href.split("/profile/")[1].split("/")[0] if "/profile/" in href else ""
            except:
                pass
            
            # L·∫•y chapter_id t·ª´ chapter link
            chapter_id = ""
            try:
                chapter_elem = review_elem.locator("a[href*='/chapter/'], .chapter-link, [class*='chapter']").first
                if chapter_elem.count() > 0:
                    href = chapter_elem.get_attribute("href") or ""
                    if "/chapter/" in href:
                        chapter_id = href.split("/chapter/")[1].split("/")[0]
            except:
                pass
            
            # L·∫•y time
            time_str = ""
            try:
                time_elem = review_elem.locator("time, .timestamp, [class*='time'], [class*='date']").first
                if time_elem.count() > 0:
                    time_str = time_elem.get_attribute("datetime") or time_elem.inner_text().strip()
            except:
                pass
            
            # L·∫•y content
            content = ""
            try:
                content_elem = review_elem.locator(".review-content, .review-text, [class*='content'], [class*='text']").first
                if content_elem.count() > 0:
                    content = content_elem.inner_text().strip()
            except:
                pass
            
            # L·∫•y scores ƒë·ªÉ t·∫°o score_id (t·∫°o unique ID t·ª´ scores)
            scores = {
                "overall_score": "",
                "style_score": "",
                "story_score": "",
                "grammar_score": "",
                "character_score": ""
            }
            
            try:
                # T√¨m c√°c score elements
                score_elements = review_elem.locator(".score, .rating, [class*='score'], [class*='rating']").all()
                for score_elem in score_elements:
                    try:
                        score_text = score_elem.inner_text().strip()
                        score_label = score_elem.get_attribute("data-label") or ""
                        # C√≥ th·ªÉ parse t·ª´ text ho·∫∑c t·ª´ data attributes
                        if "overall" in score_label.lower() or "overall" in score_text.lower():
                            scores["overall_score"] = score_text
                        elif "style" in score_label.lower() or "style" in score_text.lower():
                            scores["style_score"] = score_text
                        elif "story" in score_label.lower() or "story" in score_text.lower():
                            scores["story_score"] = score_text
                        elif "grammar" in score_label.lower() or "grammar" in score_text.lower():
                            scores["grammar_score"] = score_text
                        elif "character" in score_label.lower() or "character" in score_text.lower():
                            scores["character_score"] = score_text
                    except:
                        continue
            except:
                pass
            
            # T·∫°o score_id t·ª´ scores (hash ho·∫∑c unique identifier)
            score_id = f"{review_id}_score" if review_id else ""
            
            # T·∫°o review data theo schema
            review_data = {
                "review_id": review_id,  # Schema: review id
                "title": title,  # Schema: title
                "time": time_str,  # Schema: time
                "content": content,  # Schema: content
                "user_id": user_id,  # Schema: user id (FK)
                "chapter_id": chapter_id,  # Schema: chapter id (FK)
                "story_id": story_id,  # Schema: story id (FK)
                "score_id": score_id  # Schema: score id (FK)
            }
            
            # L∆∞u score v√†o collection scores (t·ª´ review)
            if score_id and any(scores.values()):
                self._save_score_to_mongo(
                    score_id,
                    scores.get("overall_score", ""),
                    scores.get("style_score", ""),
                    scores.get("story_score", ""),
                    scores.get("grammar_score", ""),
                    scores.get("character_score", "")
                )
            
            # L∆∞u user n·∫øu c√≥ user_id
            if user_id:
                # Username c√≥ th·ªÉ l·∫•y t·ª´ review element n·∫øu c·∫ßn
                username_elem = review_elem.locator("a[href*='/profile/'], .username, .reviewer-name, [class*='username']").first
                if username_elem.count() > 0:
                    username = username_elem.inner_text().strip()
                    if username:
                        self._save_user_to_mongo(user_id, username)
            
            # Note: Review s·∫Ω ƒë∆∞·ª£c l∆∞u trong _scrape_reviews sau khi parse
            
            return review_data
            
        except Exception as e:
            safe_print(f"        ‚ö†Ô∏è L·ªói khi parse review: {e}")
            return None

    def _save_comment_to_mongo(self, comment_data):
        """L∆∞u comment v√†o MongoDB ngay khi c√†o xong"""
        if not comment_data or not self.mongo_collection_comments:
            return
        
        try:
            existing = self.mongo_collection_comments.find_one({"comment_id": comment_data.get("comment_id")})
            if existing:
                self.mongo_collection_comments.update_one(
                    {"comment_id": comment_data.get("comment_id")},
                    {"$set": comment_data}
                )
            else:
                self.mongo_collection_comments.insert_one(comment_data)
        except Exception as e:
            safe_print(f"        ‚ö†Ô∏è L·ªói khi l∆∞u comment v√†o MongoDB: {e}")
    
    def _save_chapter_to_mongo(self, chapter_data):
        """L∆∞u chapter v√†o MongoDB ngay khi c√†o xong chapter v√† comments"""
        if not chapter_data or not self.mongo_collection_chapters:
            return
        
        try:
            existing = self.mongo_collection_chapters.find_one({"id": chapter_data.get("id")})
            if existing:
                self.mongo_collection_chapters.update_one(
                    {"id": chapter_data.get("id")},
                    {"$set": chapter_data}
                )
                safe_print(f"      üîÑ ƒê√£ c·∫≠p nh·∫≠t chapter {chapter_data.get('id')} trong MongoDB")
            else:
                self.mongo_collection_chapters.insert_one(chapter_data)
                safe_print(f"      ‚úÖ ƒê√£ l∆∞u chapter {chapter_data.get('id')} v√†o MongoDB")
        except Exception as e:
            safe_print(f"      ‚ö†Ô∏è L·ªói khi l∆∞u chapter v√†o MongoDB: {e}")
    
    def _save_review_to_mongo(self, review_data):
        """L∆∞u review v√†o MongoDB ngay khi c√†o xong"""
        if not review_data or not self.mongo_collection_reviews:
            return
        
        try:
            existing = self.mongo_collection_reviews.find_one({"review_id": review_data.get("review_id")})
            if existing:
                self.mongo_collection_reviews.update_one(
                    {"review_id": review_data.get("review_id")},
                    {"$set": review_data}
                )
            else:
                self.mongo_collection_reviews.insert_one(review_data)
        except Exception as e:
            safe_print(f"        ‚ö†Ô∏è L·ªói khi l∆∞u review v√†o MongoDB: {e}")
    
    def _save_user_to_mongo(self, user_id, username):
        """L∆∞u user v√†o MongoDB ngay khi g·∫∑p user_id v√† username"""
        if not user_id or not username or not self.mongo_collection_users:
            return
        
        try:
            existing = self.mongo_collection_users.find_one({"user_id": user_id})
            if existing:
                # Update n·∫øu username thay ƒë·ªïi
                if existing.get("username") != username:
                    self.mongo_collection_users.update_one(
                        {"user_id": user_id},
                        {"$set": {"username": username}}
                    )
            else:
                user_data = {
                    "user_id": user_id,  # Schema: user id
                    "username": username  # Schema: username
                }
                self.mongo_collection_users.insert_one(user_data)
        except Exception as e:
            safe_print(f"        ‚ö†Ô∏è L·ªói khi l∆∞u user v√†o MongoDB: {e}")
    
    def _save_score_to_mongo(self, score_id, overall_score, style_score, story_score, grammar_score, character_score):
        """L∆∞u score v√†o MongoDB"""
        if not score_id or not self.mongo_collection_scores:
            return
        
        try:
            score_data = {
                "score_id": score_id,  # Schema: score id
                "overall_score": overall_score,  # Schema: overall score
                "style_score": style_score,  # Schema: style score
                "story_score": story_score,  # Schema: story score
                "grammar_score": grammar_score,  # Schema: grammar score
                "character_score": character_score  # Schema: character score
            }
            
            existing = self.mongo_collection_scores.find_one({"score_id": score_id})
            if existing:
                self.mongo_collection_scores.update_one(
                    {"score_id": score_id},
                    {"$set": score_data}
                )
            else:
                self.mongo_collection_scores.insert_one(score_data)
        except Exception as e:
            safe_print(f"        ‚ö†Ô∏è L·ªói khi l∆∞u score v√†o MongoDB: {e}")
    
    def _save_story_to_mongo(self, story_data):
        """L∆∞u story v√†o MongoDB (c√≥ th·ªÉ update nhi·ªÅu l·∫ßn khi c√≥ th√™m chapters/reviews)"""
        if not story_data or not self.mongo_collection_stories:
            return
        
        try:
            existing = self.mongo_collection_stories.find_one({"id": story_data.get("id")})
            if existing:
                self.mongo_collection_stories.update_one(
                    {"id": story_data.get("id")},
                    {"$set": story_data}
                )
            else:
                self.mongo_collection_stories.insert_one(story_data)
        except Exception as e:
            safe_print(f"‚ö†Ô∏è L·ªói khi l∆∞u story v√†o MongoDB: {e}")
    
    def _save_to_json(self, data):
        """
        L∆∞u d·ªØ li·ªáu v√†o file JSON (MongoDB ƒë√£ ƒë∆∞·ª£c l∆∞u t·ª´ng ph·∫ßn ri√™ng)
        """
        filename = f"{data['id']}_{utils.clean_text(data.get('name', data.get('title', 'unknown')))}.json"
        save_path = os.path.join(config.JSON_DIR, filename)
        
        with open(save_path, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=4)
        safe_print(f"üíæ ƒê√£ l∆∞u d·ªØ li·ªáu v√†o file: {save_path}")