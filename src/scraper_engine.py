import json
import os
import sys
import re
import uuid
import requests
import time
import threading
import random
from collections import deque
from concurrent.futures import ThreadPoolExecutor, as_completed
from bs4 import BeautifulSoup
from datetime import datetime, timedelta
from src import config, utils
from src.login_service import WattpadLoginService, login_if_needed

# Import MongoDB
try:
    from pymongo import MongoClient
    from typing import Optional
    MONGODB_AVAILABLE = True
except ImportError:
    MongoClient = None  # type: ignore
    Optional = None  # type: ignore
    MONGODB_AVAILABLE = False

# Import scrapers
from src.scrapers import (
    StoryScraper, ChapterScraper, CommentScraper, 
    UserScraper, ChapterContentScraper, WebsiteScraper, safe_print
)

# Import duplicate checker
from src.utils.duplicate_checker import DuplicateChecker

# Import playwright-stealth if available
try:
    from playwright_stealth import stealth_sync  # type: ignore[import-not-found]
    STEALTH_AVAILABLE = True
except ImportError:
    stealth_sync = None  # type: ignore
    STEALTH_AVAILABLE = False


class RateLimiter:
    """Thread-safe rate limiter ƒë·ªÉ tr√°nh ban IP"""
    
    def __init__(self, max_requests=None, time_window=60):
        self.max_requests = max_requests or config.MAX_REQUESTS_PER_MINUTE
        self.time_window = time_window  # seconds
        self.request_times = deque()
        self._lock = threading.Lock()  # Thread-safe
    
    def wait_if_needed(self):
        """Wait n·∫øu v∆∞·ª£t qu√° rate limit (thread-safe)"""
        # ‚úÖ FIX: T√°ch logic t√≠nh to√°n (trong lock) v√† sleep (ngo√†i lock)
        sleep_time = 0
        
        with self._lock:
            now = datetime.now()
            
            # Remove requests ngo√†i time window
            while self.request_times and self.request_times[0] < now - timedelta(seconds=self.time_window):
                self.request_times.popleft()
            
            # N·∫øu ƒë√£ max requests, t√≠nh sleep time
            if len(self.request_times) >= self.max_requests:
                sleep_time = (self.request_times[0] + timedelta(seconds=self.time_window) - now).total_seconds()
            
            # Record this request TR∆Ø·ªöC khi sleep (reserve slot)
            self.request_times.append(datetime.now())
        
        # ‚úÖ Sleep NGO√ÄI lock ƒë·ªÉ kh√¥ng block threads kh√°c
        if sleep_time > 0:
            safe_print(f"‚è≥ [Thread {threading.current_thread().name}] Rate limit reached. Waiting {sleep_time:.1f}s...")
            time.sleep(sleep_time)
        
        # Random delay c≈©ng ngo√†i lock
        delay = random.uniform(0.5, 2.0)
        time.sleep(delay)


def retry_request(func, max_retries=None, backoff=None):
    """
    Decorator for retry logic v·ªõi exponential backoff
    
    Args:
        func: Function to retry
        max_retries: Number of retries (from config if None)
        backoff: Backoff multiplier (from config if None)
    
    Returns:
        Result ho·∫∑c None n·∫øu t·∫•t c·∫£ retries th·∫•t b·∫°i
    """
    max_retries = max_retries or config.MAX_RETRIES
    backoff = backoff or config.RETRY_BACKOFF
    
    for attempt in range(max_retries + 1):
        try:
            return func()
        except requests.exceptions.Timeout:
            if attempt < max_retries:
                wait_time = backoff ** attempt
                safe_print(f"‚ö†Ô∏è Timeout (attempt {attempt+1}/{max_retries}). Retry in {wait_time}s...")
                time.sleep(wait_time)
            else:
                safe_print(f"‚ùå Failed after {max_retries} retries (Timeout)")
                return None
        except requests.exceptions.ConnectionError:
            if attempt < max_retries:
                wait_time = backoff ** attempt
                safe_print(f"‚ö†Ô∏è Connection error (attempt {attempt+1}/{max_retries}). Retry in {wait_time}s...")
                time.sleep(wait_time)
            else:
                safe_print(f"‚ùå Failed after {max_retries} retries (Connection error)")
                return None
        except requests.exceptions.HTTPError as e:
            if e.response.status_code in [429, 503]:  # Rate limit or service unavailable
                if attempt < max_retries:
                    wait_time = backoff ** attempt * 5  # Longer wait for rate limit
                    safe_print(f"‚ö†Ô∏è Server error {e.response.status_code} (attempt {attempt+1}/{max_retries}). Retry in {wait_time}s...")
                    time.sleep(wait_time)
                else:
                    safe_print(f"‚ùå Failed after {max_retries} retries (HTTP {e.response.status_code})")
                    return None
            else:
                safe_print(f"‚ùå HTTP error {e.response.status_code}: {e}")
                return None
        except Exception as e:
            safe_print(f"‚ùå Unexpected error: {e}")
            return None
    
    return None


class WattpadScraper:
    """Wattpad API-based scraper using modular components"""
    
    def __init__(self, max_workers=None):
        self.context = None
        self.page = None
        self.playwright = None
        self.max_workers = max_workers or config.MAX_WORKERS
        
        # Rate limiter
        self.rate_limiter = RateLimiter()
        
        # Reusable HTTP session for connection pooling + default headers
        self.http = requests.Session()
        self.http.headers.update({
            "User-Agent": config.DEFAULT_USER_AGENT,
            "Accept": "application/json, text/javascript, */*; q=0.01",
        })
        # Configure proxies if set in config
        if config.HTTP_PROXY or config.HTTPS_PROXY:
            proxies = {}
            if config.HTTP_PROXY:
                proxies["http"] = config.HTTP_PROXY
            if config.HTTPS_PROXY:
                proxies["https"] = config.HTTPS_PROXY
            self.http.proxies.update(proxies)
            safe_print(f"üåê Proxy configured: {config.HTTPS_PROXY or config.HTTP_PROXY}")
        
        # Track current proxy used for Playwright context
        self._current_proxy = None
        
        # Initialize login service
        self.login_service = WattpadLoginService()
        
        # Initialize scrapers (will be set in start())
        self.story_scraper = None
        self.chapter_scraper = None
        self.comment_scraper = None
        self.user_scraper = None
        self.chapter_content_scraper = None
        
        # Legacy collections (backward compatibility)
        self.mongo_collection_stories = None
        self.mongo_collection_chapters = None
        self.mongo_collection_comments = None
        self.mongo_collection_users = None
        
        # Kh·ªüi t·∫°o MongoDB client n·∫øu ƒë∆∞·ª£c b·∫≠t
        self.mongo_client = None
        self.mongo_db = None
        
        if config.MONGODB_ENABLED and MONGODB_AVAILABLE:
            try:
                if MongoClient is not None:
                    self.mongo_client = MongoClient(config.MONGODB_URI)
                    self.mongo_db = self.mongo_client[config.MONGODB_DB_NAME]
                    # Create all required collections
                    self.mongo_collection_stories = self.mongo_db[config.MONGODB_COLLECTION_STORIES]
                    self.mongo_collection_story_info = self.mongo_db["story_info"]
                    self.mongo_collection_chapters = self.mongo_db["chapters"]
                    self.mongo_collection_chapter_contents = self.mongo_db["chapter_contents"]
                    self.mongo_collection_comments = self.mongo_db["comments"]
                    self.mongo_collection_users = self.mongo_db["users"]
                    self.mongo_collection_websites = self.mongo_db["websites"]
                    safe_print("‚úÖ ƒê√£ k·∫øt n·ªëi MongoDB v·ªõi 7 collections (stories, story_info, chapters, chapter_contents, comments, users, websites)")
            except Exception as e:
                safe_print(f"‚ö†Ô∏è Kh√¥ng th·ªÉ k·∫øt n·ªëi MongoDB: {e}")
                safe_print("   Ti·∫øp t·ª•c l∆∞u v√†o file JSON...")
                self.mongo_client = None

    def _build_proxy_dict(self, proxy_server=None):
        """Helper to build proxy dict for Playwright from proxy server string or config."""
        if not proxy_server:
            proxies_list = getattr(config, 'PROXIES', []) or []
            if proxies_list:
                proxy_server = random.choice(proxies_list)
                self._current_proxy = proxy_server
            elif config.HTTPS_PROXY or config.HTTP_PROXY:
                proxy_server = config.HTTPS_PROXY or config.HTTP_PROXY
                self._current_proxy = proxy_server
        
        if proxy_server:
            return {'server': proxy_server}
        return None

    def _simulate_human_behavior(self, page):
        """Simulate human-like behavior to avoid bot detection"""
        try:
            import random
            # Random delay
            page.wait_for_timeout(random.randint(1000, 3000))
            # Random mouse movement
            page.mouse.move(random.randint(100, 400), random.randint(100, 400))
            # Random scroll
            page.mouse.wheel(0, random.randint(300, 800))
            # Another delay
            page.wait_for_timeout(random.randint(1000, 2000))
        except Exception as e:
            safe_print(f"‚ö†Ô∏è Human behavior simulation failed: {e}")

    def start(self, username=None, password=None, worker_id=None):
        """Kh·ªüi ƒë·ªông scrapers, Playwright browser, v√† login
        
        Args:
            username: Wattpad username (optional)
            password: Wattpad password (optional)
            worker_id: Worker ID for parallel crawling (to create unique profile dir)
        """
        try:
            # Kh·ªüi t·∫°o Playwright persistent context (simulate real browser)
            from playwright.sync_api import sync_playwright
            import random

            self.playwright = sync_playwright().start()

            # Ensure profile dir exists - UNIQUE per worker to avoid conflicts
            profile_dir = getattr(config, 'PLAYWRIGHT_PROFILE_DIR', None)
            if not profile_dir:
                profile_dir = os.path.join(os.getcwd(), '.pw-profile')
            
            # Add worker_id suffix if provided (for parallel crawling)
            if worker_id is not None:
                profile_dir = f"{profile_dir}_worker_{worker_id}"
            
            os.makedirs(profile_dir, exist_ok=True)

            # Setup proxy using helper
            proxy_dict = self._build_proxy_dict()

            ua = getattr(config, 'PLAYWRIGHT_USER_AGENT', config.DEFAULT_USER_AGENT)

            # Extra headers to bypass bot detection
            extra_headers = {
                "Accept-Language": "en-US,en;q=0.9",
                "DNT": "1",
                "Upgrade-Insecure-Requests": "1",
                "Sec-Fetch-Site": "same-origin",
                "Sec-Fetch-Mode": "navigate",
                "Sec-Fetch-User": "?1",
                "Sec-Fetch-Dest": "document",
            }

            # Launch persistent context (headful recommended)
            pw = self.playwright
            # playwright runtime object may be typed as Optional in static analysis; ignore for attribute access
            self.context = pw.chromium.launch_persistent_context(
                profile_dir,
                headless=False if not config.HEADLESS else True,
                user_agent=ua,
                proxy=proxy_dict,  # type: ignore[arg-type]
                extra_http_headers=extra_headers,
                viewport={"width": 1280, "height": 800},
                java_script_enabled=True,
                args=[
                    '--no-sandbox',
                    '--disable-blink-features=AutomationControlled'
                ]
            )  # type: ignore[attr-defined]

            # Get page (existing or new)
            pages = self.context.pages
            self.page = pages[0] if pages else self.context.new_page()

            # Apply stealth to page (bypass bot detection)
            if STEALTH_AVAILABLE:
                try:
                    stealth_sync(self.page)  # type: ignore[misc]
                    safe_print("‚úÖ Stealth mode activated")
                except Exception as e:
                    safe_print(f"‚ö†Ô∏è Stealth activation failed: {e}")
            else:
                safe_print("‚ö†Ô∏è playwright-stealth not installed, skipping stealth mode")
                safe_print("   Install with: pip install playwright-stealth")

            safe_print("‚úÖ Playwright persistent context initialized")

            # ƒêƒÉng nh·∫≠p v√†o Wattpad n·∫øu c√≥ credentials
            if username and password:
                safe_print("\n" + "="*60)
                safe_print("üîë WATTPAD LOGIN")
                safe_print("="*60)
                self.login_service.login_with_playwright(self.page, username, password)
            else:
                # Load cookies t·ª´ file n·∫øu c√≥
                if self.login_service.load_cookies_from_file():
                    self.login_service.apply_cookies_to_browser(self.page)
                    safe_print("‚úÖ ƒê√£ load cookies t·ª´ file")
                else:
                    safe_print("‚ö†Ô∏è Kh√¥ng c√≥ credentials, scrape m√† kh√¥ng ƒëƒÉng nh·∫≠p")
                    safe_print("   M·ªôt s·ªë trang c√≥ th·ªÉ c·∫ßn ƒëƒÉng nh·∫≠p ƒë·ªÉ xem")

        except Exception as e:
            safe_print(f"‚ö†Ô∏è L·ªói kh·ªüi t·∫°o Playwright: {e}")
            safe_print("   Ti·∫øp t·ª•c m√† kh√¥ng c√≥ Playwright (ch·ªâ d√πng API)")

        # Kh·ªüi t·∫°o scrapers (d√π c√≥ Playwright hay kh√¥ng)
        from src.scrapers.story_info import StoryInfoScraper
        self.story_scraper = StoryScraper(self.page, self.mongo_db)
        self.story_info_scraper = StoryInfoScraper(self.page, self.mongo_db)
        self.chapter_scraper = ChapterScraper(self.page, self.mongo_db)
        self.comment_scraper = CommentScraper(self.page, self.mongo_db)
        self.user_scraper = UserScraper(self.page, self.mongo_db)
        self.chapter_content_scraper = ChapterContentScraper(self.page, self.mongo_db)
        self.website_scraper = WebsiteScraper(self.page, self.mongo_db)
        
        # T·∫°o Wattpad website entry n·∫øu ch∆∞a c√≥ (1 l·∫ßn duy nh·∫•t)
        if self.mongo_collection_websites is not None:
            self.wattpad_website = WebsiteScraper.get_or_create_wattpad_website(
                self.mongo_collection_websites
            )
        else:
            self.wattpad_website = None
        
        safe_print("‚úÖ Bot ƒë√£ kh·ªüi ƒë·ªông! (Wattpad API crawler + Playwright + Login)")

    def _rotate_proxy_and_restart(self):
        """Choose a different proxy from config.PROXIES and restart the Playwright context."""
        try:
            proxies_list = getattr(config, 'PROXIES', []) or []
            if not proxies_list:
                safe_print("‚ÑπÔ∏è No proxies configured to rotate")
                return False

            # Choose a new proxy different from current
            candidates = [p for p in proxies_list if p != self._current_proxy]
            if not candidates:
                candidates = proxies_list
            new_proxy = random.choice(candidates)
            self._current_proxy = new_proxy
            safe_print(f"üîÅ Rotating proxy: {new_proxy}")

            # Close existing context
            try:
                if self.context is not None:
                    try:
                        self.context.close()
                    except Exception:
                        pass
            except Exception:
                pass

            # Launch new persistent context with new proxy
            profile_dir = getattr(config, 'PLAYWRIGHT_PROFILE_DIR', None)
            if not profile_dir:
                profile_dir = os.path.join(os.getcwd(), '.pw-profile')
            os.makedirs(profile_dir, exist_ok=True)

            proxy_dict = self._build_proxy_dict(new_proxy)
            ua = getattr(config, 'PLAYWRIGHT_USER_AGENT', config.DEFAULT_USER_AGENT)

            # Extra headers to bypass bot detection (same as start())
            extra_headers = {
                "Accept-Language": "en-US,en;q=0.9",
                "DNT": "1",
                "Upgrade-Insecure-Requests": "1",
                "Sec-Fetch-Site": "same-origin",
                "Sec-Fetch-Mode": "navigate",
                "Sec-Fetch-User": "?1",
                "Sec-Fetch-Dest": "document",
            }

            pw = self.playwright
            if pw is None:
                return False
            self.context = pw.chromium.launch_persistent_context(
                profile_dir,
                headless=False if not config.HEADLESS else True,
                user_agent=ua,
                proxy=proxy_dict,  # type: ignore[arg-type]
                extra_http_headers=extra_headers,
                viewport={"width": 1280, "height": 800},
                java_script_enabled=True,
                args=[
                    '--no-sandbox',
                    '--disable-blink-features=AutomationControlled'
                ]
            )  # type: ignore[attr-defined]
            pages = self.context.pages
            self.page = pages[0] if pages else self.context.new_page()
            
            # Apply stealth to new page
            if STEALTH_AVAILABLE:
                try:
                    stealth_sync(self.page)  # type: ignore[misc]
                except Exception:
                    pass
            
            self._current_proxy = new_proxy
            safe_print("‚úÖ Restarted Playwright context with new proxy")
            return True
        except Exception as e:
            safe_print(f"‚ö†Ô∏è Failed to rotate proxy and restart context: {e}")
            return False


    def stop(self):
        """ƒê√≥ng MongoDB connection v√† Playwright"""
        if self.context:
            try:
                self.context.close()
            except Exception:
                pass
        if self.playwright:
            try:
                self.playwright.stop()
            except Exception:
                pass
        if self.mongo_client:
            self.mongo_client.close()
            safe_print("‚úÖ ƒê√£ ƒë√≥ng k·∫øt n·ªëi MongoDB")
        safe_print("zzz Bot ƒë√£ t·∫Øt.")

    # ==================== WATTPAD API METHODS ====================
    
    def scrape_stories_from_api(self, story_ids, fields=None):
        """
        C√†o nhi·ªÅu b·ªô truy·ªán t·ª´ Wattpad API
        
        Args:
            story_ids: List of story IDs to scrape
            fields: API fields to retrieve (default: all standard fields)
        
        Returns:
            List of processed story data
        """
        if not story_ids:
            safe_print("‚ùå Kh√¥ng c√≥ story ID n√†o ƒë∆∞·ª£c cung c·∫•p!")
            return []
        
        if fields is None:
            # Include tags and categories so story mapping can use API-provided values
            fields = "id,title,voteCount,readCount,createDate,lastPublishedPart,user(name,avatar),cover,url,numParts,isPaywalled,paidModel,completed,mature,description,tags,categories"
        
        safe_print(f"üìö ƒêang c√†o {len(story_ids)} b·ªô truy·ªán t·ª´ Wattpad API...")
        
        stories_data = []
        for idx, story_id in enumerate(story_ids, 1):
            try:
                story_data = self.fetch_story_from_api(story_id, fields)
                if story_data:
                    if self.story_scraper is None:
                        safe_print(f"‚ö†Ô∏è [{idx}/{len(story_ids)}] Story scraper ch∆∞a ƒë∆∞·ª£c kh·ªüi t·∫°o")
                        continue
                    processed = self.story_scraper.scrape_story_metadata(story_data)
                    if processed:
                        stories_data.append(processed)
                        safe_print(f"‚úÖ [{idx}/{len(story_ids)}] C√†o: {story_data.get('title')}")
                else:
                    safe_print(f"‚ö†Ô∏è [{idx}/{len(story_ids)}] L·ªói khi l·∫•y story ID {story_id}")
            except Exception as e:
                safe_print(f"‚ùå [{idx}/{len(story_ids)}] L·ªói: {e}")
                continue
        
        safe_print(f"\nüéâ Ho√†n th√†nh c√†o {len(stories_data)}/{len(story_ids)} b·ªô truy·ªán")
        return stories_data

    def check_paid_content(self, story_id):
        """
        Ki·ªÉm tra xem story c√≥ y√™u c·∫ßu tr·∫£ ph√≠ hay kh√¥ng
        S·ª≠ d·ª•ng API: /v5/story/{story_id}/paid-content/metadata
        
        Args:
            story_id: Story ID
        
        Returns:
            dict with 'has_full_access' and 'is_paid' or None if error
        """
        if not self.page:
            safe_print("‚ö†Ô∏è Playwright page not available for paid content check")
            return None
        
        try:
            # Use Playwright to call API (c·∫ßn cookies/auth)
            url = f"{config.BASE_URL}/v5/story/{story_id}/paid-content/metadata"
            
            self.rate_limiter.wait_if_needed()
            response = self.page.request.get(url)
            
            if response.status == 200:
                data = response.json()
                story_info = data.get("story", {})
                has_full_access = story_info.get("has_full_access", True)
                is_paid = bool(story_info.get("price"))
                
                return {
                    "has_full_access": has_full_access,
                    "is_paid": is_paid,
                    "price": story_info.get("price", [])
                }
            else:
                safe_print(f"‚ö†Ô∏è Paid content API returned status {response.status}")
                return None
                
        except Exception as e:
            safe_print(f"‚ö†Ô∏è Error checking paid content: {e}")
            return None

    def fetch_story_from_api(self, story_id, fields=None):
        """
        L·∫•y d·ªØ li·ªáu 1 b·ªô truy·ªán t·ª´ Wattpad API
        V·ªõi rate limiting v√† retry logic
        
        Args:
            story_id: Story ID
            fields: API fields to retrieve
        
        Returns:
            API response dict or None
        """
        if fields is None:
            # Include tags and categories in default fields
            fields = "id,title,voteCount,readCount,createDate,lastPublishedPart,user(name,avatar),cover,url,numParts,isPaywalled,paidModel,completed,mature,description,tags,categories"
        
        url = f"{config.BASE_URL}/api/v3/stories/{story_id}"
        params = {"fields": fields}
        
        # Apply rate limiting
        self.rate_limiter.wait_if_needed()
        
        # Retry with backoff
        def make_request():
            response = self.http.get(url, params=params, timeout=config.REQUEST_TIMEOUT)
            response.raise_for_status()
            return response.json()
        
        try:
            return retry_request(make_request)
        except Exception as e:
            safe_print(f"‚ö†Ô∏è L·ªói khi fetch story {story_id}: {e}")
            return None

    def fetch_comments_from_api_v5(self, chapter_id):
        """
        L·∫•y comments t·ª´ Wattpad API v5 (endpoint m·ªõi) - d√πng Playwright t·ª´ main thread
        
        Args:
            chapter_id: Chapter/Part ID
        
        Returns:
            List of comments (limited by MAX_COMMENTS_PER_CHAPTER)
        """
        # Sequential processing using Playwright (thread-safe, main thread only)
        self._v5_saved_flag = False
        collected = []
        seen = set()

        def process_page(resource_id, namespace, cursor=None):
            """Fetch and process a single page using Playwright"""
            key = (resource_id, namespace, cursor)
            if key in seen:
                return
            seen.add(key)

            try:
                # Use Playwright to fetch (browser cookies/token)
                if not self.page:
                    safe_print("‚ö†Ô∏è No Playwright page available")
                    return
                
                # Log pagination info
                cursor_display = f"cursor={cursor[:30]}..." if cursor and len(cursor) > 30 else f"cursor={cursor}"
                safe_print(f"      üìÑ Fetching {namespace} page: {cursor_display if cursor else 'page 1'}")
                
                self.rate_limiter.wait_if_needed()
                data = CommentScraper.fetch_v5_page_via_playwright(self.page, resource_id, namespace, cursor)

                # If no data and we have proxies configured, try rotating proxy and retry once
                if (not data or "comments" not in data) and getattr(config, 'PROXIES', []):
                    safe_print("‚ö†Ô∏è No data from v5 endpoint; attempting proxy rotation and retry")
                    try:
                        rotated = self._rotate_proxy_and_restart()
                        if rotated:
                            self.rate_limiter.wait_if_needed()
                            data = CommentScraper.fetch_v5_page_via_playwright(self.page, resource_id, namespace, cursor)
                    except Exception as e:
                        safe_print(f"‚ö†Ô∏è Proxy rotation attempt failed: {e}")

                if not data or "comments" not in data:
                    return

                # Delegate mapping & saving to CommentScraper
                try:
                    # Pass parent_comment_id when fetching replies (namespace='comments')
                    parent_id = resource_id if namespace == 'comments' else None
                    mapped_list, parents, next_cursor = CommentScraper.process_v5_comments_page(
                        data, chapter_id, namespace, 
                        comment_scraper=self.comment_scraper,
                        parent_comment_id=parent_id
                    )
                except Exception as e:
                    safe_print(f"      ‚ö†Ô∏è Error processing v5 page: {e}")
                    mapped_list, parents, next_cursor = [], [], None

                if mapped_list:
                    # respect MAX_COMMENTS_PER_CHAPTER
                    added_count = 0
                    for m in mapped_list:
                        if config.MAX_COMMENTS_PER_CHAPTER and len(collected) >= config.MAX_COMMENTS_PER_CHAPTER:
                            break
                        collected.append(m)
                        added_count += 1
                    # mark that comments were saved by process_v5_comments_page
                    self._v5_saved_flag = True
                    safe_print(f"      ‚úÖ Processed {added_count} comments (total: {len(collected)}/{config.MAX_COMMENTS_PER_CHAPTER or 'unlimited'})")

                # Recursively fetch replies (only for 'parts' namespace with replyCount > 0)
                if namespace == 'parts' and parents:
                    safe_print(f"      üí¨ Found {len(parents)} comments with replies, fetching...")
                    for p in parents:
                        process_page(p, 'comments', None)

                # Pagination
                if next_cursor and (not config.MAX_COMMENTS_PER_CHAPTER or len(collected) < config.MAX_COMMENTS_PER_CHAPTER):
                    cursor_short = next_cursor[:40] + '...' if len(next_cursor) > 40 else next_cursor
                    safe_print(f"      ‚û°Ô∏è Next page available, continuing pagination... (cursor: {cursor_short})")
                    process_page(resource_id, namespace, next_cursor)
                elif not next_cursor:
                    safe_print(f"      ‚ÑπÔ∏è No more pages (reached end)")

            except Exception as e:
                safe_print(f"      ‚ö†Ô∏è L·ªói fetch comments v5 ({resource_id} {namespace}): {e}")
                return

        # Start from main thread (no ThreadPoolExecutor - Playwright sequential)
        # Use 'parts' namespace for chapter-level comments
        process_page(chapter_id, 'parts', None)

        return collected

    def fetch_comments_from_api(self, story_id, part_id):
        """
        L·∫•y comments t·ª´ Wattpad API
        V·ªõi rate limiting, retry logic, v√† limit comments
        
        Args:
            story_id: Story ID
            part_id: Chapter/Part ID
        
        Returns:
            List of comments (limited by MAX_COMMENTS_PER_CHAPTER)
        """
        url = f"{config.BASE_URL}/api/v3/stories/{story_id}/parts/{part_id}/comments"
        
        all_comments = []
        pagination_cursor = None
        
        try:
            while True:
                # Check limit
                if config.MAX_COMMENTS_PER_CHAPTER and len(all_comments) >= config.MAX_COMMENTS_PER_CHAPTER:
                    safe_print(f"   ‚è∏Ô∏è ƒê√£ reach limit {config.MAX_COMMENTS_PER_CHAPTER} comments")
                    break
                
                # Apply rate limiting
                self.rate_limiter.wait_if_needed()
                
                params = {}
                if pagination_cursor:
                    params["after"] = pagination_cursor
                
                def make_request():
                    response = self.http.get(url, params=params, timeout=config.REQUEST_TIMEOUT)
                    response.raise_for_status()
                    return response.json()
                
                data = retry_request(make_request)
                if not data:
                    break
                
                if "comments" in data:
                    comments_to_add = data["comments"]
                    
                    # Trim if would exceed limit
                    if config.MAX_COMMENTS_PER_CHAPTER:
                        remaining = config.MAX_COMMENTS_PER_CHAPTER - len(all_comments)
                        comments_to_add = comments_to_add[:remaining]
                    
                    all_comments.extend(comments_to_add)
                
                # Check for next page and limit
                if config.MAX_COMMENTS_PER_CHAPTER and len(all_comments) >= config.MAX_COMMENTS_PER_CHAPTER:
                    break
                
                if "pagination" in data and "after" in data["pagination"]:
                    pagination_cursor = data["pagination"]["after"]
                else:
                    break
            
            return all_comments
        except Exception as e:
            safe_print(f"‚ö†Ô∏è L·ªói khi l·∫•y comments: {e}")
            return []

    def fetch_chapters_from_api(self, story_id):
        """
        L·∫•y danh s√°ch t·∫•t c·∫£ chapters t·ª´ Wattpad API
        
        L∆ØU √ù: Wattpad API /parts endpoint y√™u c·∫ßu authorization header
        Fallback: S·ª≠ d·ª•ng prefetched data ho·∫∑c HTML parsing
        
        Args:
            story_id: Story ID
        
        Returns:
            List of chapter data (limited by MAX_CHAPTERS_PER_STORY)
        """
        # Hi·ªán t·∫°i endpoint /parts kh√¥ng work public
        # S·∫Ω fallback t·ªõi prefetched data trong scrape_story()
        
        url = f"{config.BASE_URL}/api/v3/stories/{story_id}/parts"
        
        try:
            # Apply rate limiting
            self.rate_limiter.wait_if_needed()
            
            def make_request():
                response = self.http.get(url, timeout=config.REQUEST_TIMEOUT)
                response.raise_for_status()
                return response.json()
            
            data = retry_request(make_request)
            if data and "parts" in data:
                return data["parts"]
            else:
                return []
        except Exception as e:
            safe_print(f"‚ö†Ô∏è API /parts kh√¥ng kh·∫£ d·ª•ng: {e}")
            return []

    def fetch_categories(self):
        """
        L·∫•y danh s√°ch categories t·ª´ Wattpad API
        V·ªõi rate limiting v√† retry logic
        
        Returns:
            List of categories with mapping
        """
        url = f"{config.BASE_URL}/api/v3/categories"
        
        # Apply rate limiting
        self.rate_limiter.wait_if_needed()
        
        def make_request():
            response = self.http.get(url, timeout=config.REQUEST_TIMEOUT)
            response.raise_for_status()
            return response.json()
        
        try:
            categories = retry_request(make_request)
            if not categories:
                return {}
            
            # T·∫°o mapping id ‚Üí name_english
            category_map = {cat["id"]: cat["name_english"] for cat in categories}
            safe_print(f"‚úÖ ƒê√£ l·∫•y {len(category_map)} categories")
            return category_map
        except Exception as e:
            safe_print(f"‚ö†Ô∏è L·ªói khi l·∫•y categories: {e}")
            return {}

    def scrape_story(self, story_id, fetch_chapters=True, fetch_comments=True, story_url=None):
        """
        C√†o to√†n b·ªô th√¥ng tin b·ªô truy·ªán:
        1. Metadata t·ª´ API /api/v3/stories/{id}
        2. Tags + Categories t·ª´ HTML window.prefetched
        3. Chapters t·ª´ HTML window.prefetched
        4. Comments t·ª´ HTML window.prefetched
        
        Args:
            story_id: Story ID ho·∫∑c full URL to scrape
            fetch_chapters: Whether to fetch chapter list
            fetch_comments: Whether to fetch comments
            story_url: Story URL (optional, will be extracted from story_id if URL is provided)
        
        Returns:
            Complete story data dict
        """
        import re
        
        # Handle n·∫øu story_id l√† URL
        if isinstance(story_id, str) and story_id.startswith('http'):
            # Extract ID t·ª´ URL: https://www.wattpad.com/STORYID-title
            story_url = story_id
            match = re.search(r'wattpad\.com/(\d+)', story_id)
            if match:
                story_id = match.group(1)
            else:
                safe_print(f"‚ùå Kh√¥ng th·ªÉ extract story ID t·ª´ URL: {story_id}")
                return None
        
        safe_print(f"\n{'='*60}")
        safe_print(f"üìñ B·∫Øt ƒë·∫ßu c√†o story ID: {story_id}")
        safe_print(f"{'='*60}")
        
        # ‚úÖ CHECK: Story ƒë√£ ƒë∆∞·ª£c c√†o hay ch∆∞a
        dup_checker = DuplicateChecker()
        story_status = dup_checker.check_story(story_id)
        if story_status:
            dup_checker.close()
            return story_status  # Return existing data instead of None
        dup_checker.close()
        
        # 1. Fetch story metadata t·ª´ API
        story_data = self.fetch_story_from_api(story_id)
        if not story_data:
            safe_print(f"‚ùå Kh√¥ng th·ªÉ l·∫•y metadata cho story {story_id}")
            return None
        
        # L·∫•y story URL t·ª´ API response n·∫øu kh√¥ng ƒë∆∞·ª£c provide
        if not story_url and story_data.get("url"):
            story_url = story_data["url"]
            if not story_url.startswith("http"):
                story_url = config.BASE_URL + story_url
        
        if story_url:
            safe_print(f"   URL: {story_url}")
        
        # Try to fetch from HTML prefetched data (tags, categories, chapters)
        extra_info = None
        prefetched_data = None
        
        # ƒê·ªÉ fetch prefetched, c·∫ßn URL c·ªßa CHAPTER, kh√¥ng ph·∫£i story overview
        # N·∫øu c√≥ lastPublishedPart, d√πng chapter URL ƒë√≥
        chapter_url_for_prefetch = None
        if story_data.get("lastPublishedPart"):
            last_part = story_data["lastPublishedPart"]
            # C√°ch 1: N·∫øu c√≥ url field
            if last_part.get("url"):
                chapter_url_for_prefetch = last_part["url"]
                if not chapter_url_for_prefetch.startswith("http"):
                    chapter_url_for_prefetch = config.BASE_URL + chapter_url_for_prefetch
            # C√°ch 2: Build URL t·ª´ part ID
            elif last_part.get("id"):
                part_id = last_part["id"]
                chapter_url_for_prefetch = f"{config.BASE_URL}/{part_id}"
        else:
            safe_print(f"   ‚ÑπÔ∏è Kh√¥ng c√≥ lastPublishedPart, b·ªè qua prefetched data")
        
        if chapter_url_for_prefetch:
            safe_print(f"   üåê ƒêang fetch HTML prefetched data...")
            prefetched_data = self.fetch_html_prefetched_data(chapter_url_for_prefetch)
            if prefetched_data:
                extra_info = StoryScraper.extract_story_info_from_prefetched(prefetched_data, story_id)
                
                # 2a. Extract author from prefetched data (cho truy·ªán free/premium c√≥ prefetched)
                user_info_from_prefetch = UserScraper.extract_user_info_from_prefetched(prefetched_data)
                if user_info_from_prefetch and user_info_from_prefetch.get("userName"):
                    if self.user_scraper:
                        self.user_scraper.save_user_to_mongo(
                            user_info_from_prefetch["userName"],
                            user_info_from_prefetch["userName"],
                            user_info_from_prefetch.get("avatar")
                        )
                        safe_print(f"   ‚úÖ Saved author from prefetched: {user_info_from_prefetch['userName']}")
        
        # 2b. Extract and save author/user info from API response (fallback + b·ªï sung)
        if story_data.get("user"):
            user_data = story_data["user"]
            user_name = user_data.get("name")
            avatar = user_data.get("avatar")
            
            if user_name and self.user_scraper:
                self.user_scraper.save_user_to_mongo(
                    user_name,  # Use username as userId
                    user_name,
                    avatar
                )
                safe_print(f"   ‚úÖ Saved author from API: {user_name}")
        
        # 3. Process story metadata (k√®m tags + categories)
        if self.story_scraper is None:
            safe_print(f"‚ùå Story scraper ch∆∞a ƒë∆∞·ª£c kh·ªüi t·∫°o")
            return None
        processed_story = StoryScraper.map_api_to_story(story_data, extra_info)
        
        if not processed_story:
            safe_print(f"‚ùå L·ªói khi x·ª≠ l√Ω story metadata")
            return None
        
        # 4. CHECK PAID CONTENT tr∆∞·ªõc khi fetch chapters
        if fetch_chapters:
            safe_print(f"   üí∞ ƒêang ki·ªÉm tra paid content...")
            paid_info = self.check_paid_content(story_id)
            
            if paid_info and not paid_info.get("has_full_access", True):
                safe_print(f"   ‚õî TRUY·ªÜN N√ÄY C·∫¶N TR·∫¢ PH√ç ƒê·ªÇ XEM - B·ªè qua t·∫•t c·∫£ chapters")
                safe_print(f"      üíµ Price: {paid_info.get('price', 'Unknown')}")
                safe_print(f"      üîí Has full access: False")
                # V·∫´n l∆∞u story metadata nh∆∞ng kh√¥ng c√†o chapters
                fetch_chapters = False
        
        # 5. Optionally fetch chapters
        if fetch_chapters:
            safe_print(f"   üìö ƒêang l·∫•y danh s√°ch chapters...")
            chapters = []
            chapter_urls = []

            # Prefer `parts` from story_data as authoritative chapter list (if available)
            parts_from_api = story_data.get("parts") or []
            if parts_from_api and isinstance(parts_from_api, list):
                safe_print(f"   ‚ÑπÔ∏è S·ª≠ d·ª•ng `parts[]` t·ª´ API l√†m ngu·ªìn chapters ({len(parts_from_api)})")
                for idx_p, p in enumerate(parts_from_api, 1):
                    web_chapter_id = str(p.get("id"))
                    chapter_id = WebsiteScraper.generate_chapter_id(web_chapter_id, prefix="wp")
                    
                    chapter_obj = {
                        "chapterId": chapter_id,              # wp_uuid_v7
                        "webChapterId": web_chapter_id,      # Original Wattpad ID
                        "order": idx_p - 1,
                        "chapterName": p.get("title"),
                        "chapterUrl": p.get("url") if p.get("url") and p.get("url").startswith("http") else (config.BASE_URL + str(p.get("url")) if p.get("url") else f"{config.BASE_URL}/{p.get('id')}"),
                        "publishedTime": p.get("createDate"),
                        "storyId": story_id,                 # Parent wp_uuid_v7
                        "voted": p.get("voteCount", 0),
                        "views": p.get("readCount", 0),
                        "totalComments": p.get("commentCount", 0),
                    }
                    chapters.append(chapter_obj)

            # If we already populated chapters from parts[], skip HTML/API discovery
            if not chapters:
            
                # Step 1: Extract chapter URLs from story overview page
                if self.page:
                    try:
                        # Navigate to story overview page to get table of contents
                        story_overview_url = f"{config.BASE_URL}/story/{story_id}"
                        safe_print(f"   üîç ƒêang extract danh s√°ch chapters t·ª´ story overview...")
                        
                        self.rate_limiter.wait_if_needed()
                        self.page.goto(story_overview_url, wait_until="load", timeout=config.REQUEST_TIMEOUT * 1000)
                        self.page.wait_for_timeout(2000)
                        self._simulate_human_behavior(self.page)
                        
                        page_html = self.page.content()
                        chapter_urls = ChapterScraper.extract_chapter_urls_from_html(page_html, story_id, config.MAX_CHAPTERS_PER_STORY)
                        
                        if chapter_urls:
                            safe_print(f"   ‚úÖ T√¨m ƒë∆∞·ª£c {len(chapter_urls)} chapters t·ª´ HTML")
                            for i, url in enumerate(chapter_urls, 1):
                                safe_print(f"      [{i}] {url}")
                        
                    except Exception as e:
                        safe_print(f"   ‚ö†Ô∏è L·ªói khi extract t·ª´ story page: {e}")
                
                # Step 2: If no URLs found, fallback to API or prefetched
                if not chapter_urls:
                    chapters = self.fetch_chapters_from_api(story_id)
                    if not chapters and prefetched_data:
                        chapters = ChapterScraper.extract_chapters_from_prefetched(prefetched_data, story_id)
                else:
                    # Build basic chapter objects t·ª´ URLs
                    for url in chapter_urls:
                        # Extract chapter ID - should be at start after domain
                        # URL format: https://www.wattpad.com/1234567-chapter-name
                        chapter_id_match = re.search(r'/([0-9]+)(?:-|/|$)', url)
                        if chapter_id_match:
                            web_chapter_id = chapter_id_match.group(1)
                            chapter_id = WebsiteScraper.generate_chapter_id(web_chapter_id, prefix="wp")
                            
                            chapter_obj = {
                                "chapterId": chapter_id,         # wp_uuid_v7
                                "webChapterId": web_chapter_id,  # Original Wattpad ID
                                "storyId": story_id,             # Parent wp_uuid_v7
                                "chapterUrl": url,
                                "chapterName": f"Chapter {len(chapters) + 1}",
                            }
                            chapters.append(chapter_obj)
            
            if chapters:
                # Step 3: Scrape t·ª´ng chapter - FOR EACH CHAPTER: content + comments + metadata
                safe_print(f"   üìñ B·∫Øt ƒë·∫ßu c√†o {min(len(chapters), config.MAX_CHAPTERS_PER_STORY or len(chapters))} chapters...")
                
                max_to_fetch = config.MAX_CHAPTERS_PER_STORY or len(chapters)
                for idx, chapter in enumerate(chapters, 1):
                    if idx > max_to_fetch:
                        break
                    
                    chapter_id = chapter.get("chapterId")
                    if not chapter_id:
                        continue
                    
                    # Build chapter URL if not available
                    chapter_url = chapter.get("chapterUrl")
                    if not chapter_url:
                        chapter_url = f"{config.BASE_URL}/{chapter_id}"
                    
                    # Skip content extraction if no page available
                    if not self.page or not self.chapter_content_scraper:
                        safe_print(f"\n   üìñ [{idx}/{max_to_fetch}] Chapter: {chapter.get('chapterName')} (kh√¥ng c√≥ Playwright page)")
                        continue
                    
                    try:
                        safe_print(f"\n   üìñ [{idx}/{max_to_fetch}] C√†o chapter: {chapter.get('chapterName')}")
                        
                        # ‚úÖ CHECK: Chapter ƒë√£ ƒë∆∞·ª£c c√†o hay ch∆∞a
                        dup_checker = DuplicateChecker()
                        if dup_checker.check_chapter(chapter_id, chapter.get('chapterName')):
                            dup_checker.close()
                            continue  # Skip to next chapter
                        dup_checker.close()
                        
                        # Step 3a: Navigate t·ªõi chapter URL
                        self.rate_limiter.wait_if_needed()
                        self.page.goto(chapter_url, wait_until="load", timeout=config.REQUEST_TIMEOUT * 1000)
                        self.page.wait_for_timeout(2000)
                        self._simulate_human_behavior(self.page)
                        
                        # Step 3b: Fetch window.prefetched data c·ªßa chapter n√†y
                        chapter_prefetched_data = self.page.evaluate("() => window.prefetched")
                        
                        if chapter_prefetched_data:
                            # Extract metadata t·ª´ prefetched (NEW SCHEMA)
                            for key, value in chapter_prefetched_data.items():
                                if key.startswith("part.") and "metadata" in key:
                                    if "data" in value:
                                        chapter_meta = value["data"]
                                        chapter["chapterName"] = chapter_meta.get("title", chapter.get("chapterName"))
                                        chapter["views"] = chapter_meta.get("readCount", 0)
                                        chapter["voted"] = chapter_meta.get("voteCount", 0)
                                        chapter["order"] = chapter_meta.get("order", idx - 1)
                                        chapter["totalComments"] = chapter_meta.get("commentCount", 0)
                                        chapter["publishedTime"] = chapter_meta.get("createDate")
                                        chapter["webChapterId"] = None
                                        safe_print(f"      ‚úÖ Metadata: {chapter['chapterName']}")
                                        break
                        else:
                            # N·∫øu kh√¥ng c√≥ prefetched data, d√πng placeholder name
                            chapter["chapterName"] = f"Chapter {idx}: {chapter.get('chapterName', 'Unknown')}"
                            chapter["order"] = idx - 1
                        
                        # Step 3c: Extract chapter content t·ª´ HTML
                        page_html = self.page.content()
                        chapter_content = ChapterContentScraper.extract_and_map_chapter_content(page_html, chapter_id)
                        
                        if chapter_content and chapter_content.get("content"):
                            # ‚úÖ L∆ØU√ç: Kh√¥ng l∆∞u content trong chapter object
                            # Content s·∫Ω ƒë∆∞·ª£c l∆∞u ri√™ng v√†o chapter_content collection
                            content_len = len(chapter_content.get('content', ''))
                            safe_print(f"      ‚úÖ Content: {content_len} bytes")
                            
                            # Save chapter_content to MongoDB (collection ri√™ng)
                            if self.chapter_content_scraper:
                                self.chapter_content_scraper.save_chapter_content_to_mongo(chapter_content)
                        else:
                            safe_print(f"      ‚ö†Ô∏è Kh√¥ng extract ƒë∆∞·ª£c content")
                        
                        # Step 3d: Extract comments cho CHAPTER N√ÄY (kh√¥ng ph·∫£i chapter cu·ªëi c√πng)
                        chapter_comments = None
                        if fetch_comments:
                            safe_print(f"      üí¨ ƒêang l·∫•y comments...")
                            
                            # Use API v5 with Playwright (only method)
                            chapter_comments = None
                            try:
                                chapter_comments = self.fetch_comments_from_api_v5(chapter_id)
                            except Exception as e:
                                safe_print(f"      ‚ö†Ô∏è L·ªói API v5: {e}")
                            
                            if chapter_comments:
                                # ‚úÖ L∆ØU√ç: Kh√¥ng l∆∞u comments trong chapter object
                                # Comments s·∫Ω ƒë∆∞·ª£c l∆∞u ri√™ng v√†o comments collection
                                safe_print(f"      ‚úÖ Comments: {len(chapter_comments)} comments")
                        
                        # Step 3e: Save chapter to MongoDB (NGAY SAU KHI HO√ÄN TH√ÄNH)
                        if self.chapter_scraper:
                            self.chapter_scraper.save_chapter_to_mongo(chapter)
                        
                        # Step 3f: Save comments to MongoDB (n·∫øu c√≥)
                        if fetch_comments and chapter_comments and self.comment_scraper:
                            # If v5 fetcher already saved comments to DB, skip double-saving
                            if getattr(self, '_v5_saved_flag', False):
                                safe_print(f"      ‚ÑπÔ∏è Comments already saved by v5 fetcher; skipping duplicate save")
                            else:
                                for comment in chapter_comments:
                                    user_name = comment.get("userName")  # Extract userName from comment (display name)
                                    self.comment_scraper.save_comment_to_mongo(comment, user_name=user_name)
                        
                    except Exception as e:
                        safe_print(f"      ‚ö†Ô∏è L·ªói: {e}")
                        import traceback
                        traceback.print_exc()
                
                # Don't add chapters to story document - they're stored separately
                # processed_story["chapters"] = chapters  # REMOVED - chapters in separate collection
                safe_print(f"\n   ‚úÖ Ho√†n th√†nh c√†o {len(chapters)} chapters")
        
        # Save story to MongoDB (WITHOUT chapters array)
        if self.story_scraper:
            safe_print(f"   üíæ ƒêang l∆∞u story v√†o MongoDB...")
            # Make sure no chapters array in story document
            if "chapters" in processed_story:
                del processed_story["chapters"]
            self.story_scraper.save_story_to_mongo(processed_story)
            
            # Also save story info (stats/metrics)
            if self.story_info_scraper:
                safe_print(f"   üíæ ƒêang l∆∞u story info v√†o MongoDB...")
                story_info = self.story_info_scraper.map_api_to_story_info(story_data)
                if story_info:
                    self.story_info_scraper.save_story_info(story_info)
        
        safe_print(f"‚úÖ Ho√†n th√†nh c√†o story: {processed_story.get('storyName')}")
        return processed_story

    # ==================== PAGE SCRAPING METHODS ====================
    
    @staticmethod
    def fetch_story_links_from_page(page_url, max_stories=None):
        """
        Qu√©t trang Wattpad ƒë·ªÉ l·∫•y danh s√°ch story links (STATIC - kh√¥ng c·∫ßn rate limiter)
        
        Args:
            page_url: URL trang danh s√°ch stories
            max_stories: Max s·ªë stories ƒë·ªÉ l·∫•y (None = t·∫•t c·∫£, ho·∫∑c d√πng config.MAX_STORIES_PER_BATCH)
        
        Returns:
            List of story URLs
        """
        if max_stories is None:
            max_stories = config.MAX_STORIES_PER_BATCH
        
        def make_request():
            response = requests.get(
                page_url,
                timeout=config.REQUEST_TIMEOUT,
                headers={"User-Agent": config.DEFAULT_USER_AGENT}
            )
            response.raise_for_status()
            return response.content
        
        try:
            content = retry_request(make_request)
            if not content:
                return []
            
            # Parse HTML
            soup = BeautifulSoup(content, 'html.parser')
            story_links = []
            
            # T√¨m story links - c√°c c√°ch kh√°c nhau t√πy v√†o layout trang
            # C√°ch 1: Links c√≥ d·∫°ng /story/{id}-{title}
            for link in soup.find_all('a', href=True):
                href_raw = link.get('href')
                if href_raw is None:
                    continue
                
                # Convert to string if needed
                href = str(href_raw) if href_raw else ''
                if not href:
                    continue
                
                # Match story URLs
                if re.search(r'/story/\d+', href) or re.search(r'/\d+\-', href):
                    # Ensure full URL
                    if not href.startswith('http'):
                        href = config.BASE_URL + href
                    
                    # Extract story ID ƒë·ªÉ check duplicate
                    story_id_match = re.search(r'/(\d+)', href)
                    if story_id_match:
                        story_id = story_id_match.group(1)
                        
                        # Check if already added
                        existing = [url for url in story_links if story_id in url]
                        if not existing:
                            story_links.append(href)
                            
                            # Check limit
                            if max_stories and len(story_links) >= max_stories:
                                break
            
            safe_print(f"‚úÖ T√¨m ƒë∆∞·ª£c {len(story_links)} stories tr√™n trang")
            return story_links
            
        except Exception as e:
            safe_print(f"‚ö†Ô∏è L·ªói khi qu√©t trang: {e}")
            return []

    def scrape_stories_from_page(self, page_url, fetch_chapters=True, fetch_comments=True):
        """
        Qu√©t t·∫•t c·∫£ stories t·ª´ 1 trang
        
        Args:
            page_url: URL trang danh s√°ch
            fetch_chapters: C√≥ l·∫•y chapters kh√¥ng
            fetch_comments: C√≥ l·∫•y comments kh√¥ng
        
        Returns:
            List of scraped story data
        """
        safe_print(f"\n{'='*60}")
        safe_print(f"üìÑ Qu√©t trang: {page_url}")
        safe_print(f"{'='*60}")
        
        # 1. Get story links from page
        story_links = self.fetch_story_links_from_page(page_url)
        if not story_links:
            safe_print(f"‚ùå Kh√¥ng t√¨m ƒë∆∞·ª£c stories tr√™n trang")
            return []
        
        # 2. Scrape t·ª´ng story
        results = []
        for idx, story_url in enumerate(story_links, 1):
            safe_print(f"\n[{idx}/{len(story_links)}] {story_url}")
            
            # Extract story ID
            story_id_match = re.search(r'/(\d+)', story_url)
            if not story_id_match:
                safe_print(f"  ‚ö†Ô∏è Kh√¥ng extract ƒë∆∞·ª£c story ID")
                continue
            
            story_id = story_id_match.group(1)
            
            # Scrape story
            result = self.scrape_story(
                story_id=story_id,
                story_url=story_url,
                fetch_chapters=fetch_chapters,
                fetch_comments=fetch_comments
            )
            
            if result:
                results.append(result)
        
        safe_print(f"\n{'='*60}")
        safe_print(f"‚úÖ Qu√©t xong {len(results)}/{len(story_links)} stories")
        safe_print(f"{'='*60}")
        
        return results

    # ==================== HTML SCRAPING METHODS ====================
    
    def fetch_html_prefetched_data(self, story_url):
        """
        L·∫•y d·ªØ li·ªáu t·ª´ window.prefetched trong HTML page
        D√πng Playwright ƒë·ªÉ execute JavaScript (window.prefetched ƒë∆∞·ª£c render b·ªüi JS)
        
        Args:
            story_url: Full URL to story chapter
        
        Returns:
            dict with prefetched data or None
        """
        # N·∫øu kh√¥ng c√≥ page object (Playwright ch∆∞a init), tr·∫£ v·ªÅ None
        if self.page is None:
            safe_print(f"‚ö†Ô∏è Playwright page ch∆∞a init, b·ªè qua prefetched data")
            return None
        
        try:
            # Apply rate limiting
            self.rate_limiter.wait_if_needed()
            
            safe_print(f"   üåê ƒêang fetch HTML v·ªõi Playwright (execute JS)...")
            safe_print(f"   üìç URL: {story_url}")
            
            # Navigate to page (Playwright s·∫Ω execute t·∫•t c·∫£ JS)
            # Use wait_until="load" ƒë·ªÉ page load xong, kh√¥ng ch·ªù networkidle
            self.page.goto(story_url, wait_until="load", timeout=config.REQUEST_TIMEOUT * 1000)
            
            # Ch·ªù m·ªôt ch√∫t ƒë·ªÉ JS render xong
            self.page.wait_for_timeout(3000)
            
            # L·∫•y window.prefetched object t·ª´ browser context
            prefetched_data = self.page.evaluate("() => window.prefetched")
            
            if prefetched_data:
                safe_print(f"‚úÖ ƒê√£ l·∫•y prefetched data t·ª´ browser (Playwright)")
                safe_print(f"   Keys: {list(prefetched_data.keys())}")
                return prefetched_data
            else:
                safe_print(f"‚ö†Ô∏è window.prefetched kh√¥ng c√≥ trong page")
                # Debug: Check window object
                try:
                    window_keys = self.page.evaluate("() => Object.keys(window).slice(0, 20)")
                    safe_print(f"   Window keys sample: {window_keys}")
                except:
                    pass
                return None
                
        except Exception as e:
            safe_print(f"‚ö†Ô∏è L·ªói khi fetch HTML v·ªõi Playwright: {e}")
            import traceback
            traceback.print_exc()
            return None
            return None

    # ==================== UTILITY METHODS ====================

    def _save_to_json(self, data):
        """
        L∆∞u d·ªØ li·ªáu v√†o file JSON (WITHOUT chapters array)
        """
        # Remove chapters array if present (chapters stored separately in DB)
        data_to_save = data.copy()
        if "chapters" in data_to_save:
            del data_to_save["chapters"]
        
        # Sanitize filename
        story_name = data_to_save.get('storyName', 'unknown')
        safe_name = story_name.replace('/', '_').replace('\\', '_').replace('|', '_').replace('?', '_').replace('*', '_').replace('"', '_').replace('<', '_').replace('>', '_').replace(':', '_')[:50]
        # Use webStoryId for filename (original Wattpad ID is more readable)
        web_story_id = data_to_save.get('webStoryId', data_to_save.get('storyId', 'unknown'))
        filename = f"{web_story_id}_{safe_name}.json"
        save_path = os.path.join(config.JSON_DIR, filename)
        
        try:
            with open(save_path, "w", encoding="utf-8") as f:
                json.dump(data_to_save, f, ensure_ascii=False, indent=4)
            safe_print(f"üíæ ƒê√£ l∆∞u d·ªØ li·ªáu v√†o file: {save_path}")

        except Exception as e:
            safe_print(f"‚ö†Ô∏è L·ªói khi l∆∞u file JSON: {e}")
    

# For backward compatibility
RoyalRoadScraper = WattpadScraper
