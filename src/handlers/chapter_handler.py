"""
Chapter handler - x·ª≠ l√Ω chapter content scraping
‚úÖ C√ÅCH T·ªêI ∆ØU: D√πng browser ch√≠nh ƒë√£ m·ªü (ƒë√£ v∆∞·ª£t Cloudflare) thay v√¨ t·∫°o m·ªõi
"""
import time
import random
import re
from playwright.sync_api import sync_playwright
from src import config
from src.utils import safe_print, generate_id, convert_html_to_formatted_text
from src.utils.requests_helper import get_session_from_context, scrape_chapter_with_requests


class ChapterHandler:
    """Handler cho chapter content scraping"""
    
    def __init__(self, mongo_handler, comment_handler, context=None):
        """
        Args:
            mongo_handler: MongoHandler instance
            comment_handler: CommentHandler instance
            context: Playwright context (ƒë·ªÉ l·∫•y cookies cho requests)
        """
        self.mongo = mongo_handler
        self.comment_handler = comment_handler
        self.context = context  # L∆∞u context ƒë·ªÉ d√πng cho requests
    
    def scrape_single_chapter_using_browser(self, page, url, index, story_id, order, published_time_from_table):
        """
        ‚úÖ C√ÅCH T·ªêI ∆ØU: Scrape chapter b·∫±ng browser ch√≠nh ƒë√£ m·ªü (ƒë√£ v∆∞·ª£t Cloudflare)
        ‚Üí Kh√¥ng b·ªã 403 Forbidden (v√¨ d√πng browser ƒë√£ verify)
        ‚Üí Kh√¥ng b·ªã l·ªói Playwright Sync API (v√¨ kh√¥ng t·∫°o browser m·ªõi)
        ‚Üí ·ªîn ƒë·ªãnh nh·∫•t, reliable nh·∫•t
        
        Args:
            page: Playwright page object (browser ch√≠nh ƒë√£ m·ªü)
            url: URL c·ªßa ch∆∞∆°ng c·∫ßn c√†o
            index: Th·ª© t·ª± ch∆∞∆°ng trong list
            story_id: ID c·ªßa story (FK)
            order: S·ªë th·ª© t·ª± c·ªßa chapter (t·ª´ 1)
            published_time_from_table: published_time l·∫•y t·ª´ table row
        """
        try:
            safe_print(f"    üîÑ ƒêang c√†o ch∆∞∆°ng {index + 1} b·∫±ng Browser ch√≠nh...")
            
            # 1. Goto URL b·∫±ng browser ƒëang m·ªü (ƒë√£ v∆∞·ª£t Cloudflare)
            page.goto(url, timeout=config.TIMEOUT, wait_until="domcontentloaded")
            
            # 2. Random delay ƒë·ªÉ gi·ªëng ng∆∞·ªùi th·∫≠t
            time.sleep(random.uniform(2.0, 4.0))
            
            # 3. X·ª≠ l√Ω Cloudflare n·∫øu v√¥ t√¨nh g·∫∑p l·∫°i (Scroll nh·∫π)
            page_content = page.content().lower()
            if any(x in page_content for x in ["challenges.cloudflare.com", "please unblock", "checking your browser"]):
                safe_print("      ‚ö†Ô∏è G·∫∑p l·∫°i Cloudflare, ƒë·ª£i 5s...")
                time.sleep(5)
            
            # 4. L·∫•y n·ªôi dung t·ª´ div.chp_raw (gi·ªØ ƒë√∫ng format nh∆∞ UI)
            try:
                # Th·ª≠ l·∫•y t·ª´ #chp_raw ho·∫∑c .chp_raw
                page.wait_for_selector("#chp_raw, .chp_raw", timeout=10000)
            except:
                safe_print(f"      ‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y #chp_raw/.chp_raw (Timeout), th·ª≠ fallback...")
            
            # L·∫•y chapter_name t·ª´ .chapter-title
            # HTML: <div class="chapter-title">Chapter 77: Instant KO Salamence</div>
            chapter_name = ""
            try:
                title_elem = page.locator(".chapter-title").first
                if title_elem.count() > 0:
                    chapter_name = title_elem.inner_text().strip()
                else:
                    # Fallback: th·ª≠ h1
                    title_elem = page.locator("h1").first
                    if title_elem.count() > 0:
                        chapter_name = title_elem.inner_text().strip()
            except Exception as e:
                safe_print(f"      ‚ö†Ô∏è L·ªói l·∫•y chapter_name: {e}")
            
            content = ""
            try:
                # ‚úÖ L·∫•y t·ª´ div.chp_raw (c√≥ nhi·ªÅu th·∫ª p, gi·ªØ ƒë√∫ng format nh∆∞ UI)
                content_container = page.locator("#chp_raw, .chp_raw").first
                if content_container.count() > 0:
                    html_content = content_container.inner_html()
                    # convert_html_to_formatted_text s·∫Ω gi·ªØ ƒë√∫ng format:
                    # - M·ªói <p> = m·ªôt ƒëo·∫°n vƒÉn, c√°c ƒëo·∫°n c√°ch nhau b·∫±ng m·ªôt d√≤ng tr·ªëng
                    # - <br> = xu·ªëng d√≤ng
                    # - Gi·ªØ nguy√™n c·∫•u tr√∫c nh∆∞ trong UI
                    content = convert_html_to_formatted_text(html_content)
                    safe_print(f"      ‚úÖ ƒê√£ l·∫•y content t·ª´ .chp_raw ({len(content)} k√Ω t·ª±)")
                else:
                    # Fallback 1: Th·ª≠ .chapter-inner
                    try:
                        content_container = page.locator(".chapter-inner").first
                        if content_container.count() > 0:
                            html_content = content_container.inner_html()
                            content = convert_html_to_formatted_text(html_content)
                            safe_print(f"      ‚ö†Ô∏è D√πng fallback .chapter-inner")
                        else:
                            # Fallback 2: L·∫•y text th√¥
                            content = page.locator("body").inner_text()
                            safe_print(f"      ‚ö†Ô∏è D√πng fallback body text")
                    except:
                        pass
            except Exception as e:
                safe_print(f"      ‚ö†Ô∏è L·ªói l·∫•y content: {e}")
                try:
                    # Fallback cu·ªëi c√πng
                    content = page.locator("body").inner_text()
                except:
                    pass
            
            # 5. L·∫•y published_time n·∫øu ch∆∞a c√≥
            published_time = published_time_from_table
            if not published_time:
                try:
                    time_elem = page.locator("time[datetime]").first
                    if time_elem.count() > 0:
                        published_time = time_elem.get_attribute("datetime") or ""
                except:
                    pass
            
            # 6. L·∫•y web_chapter_id t·ª´ URL
            web_chapter_id = ""
            try:
                match = re.search(r'/chapter/(\d+)', url)
                if match:
                    web_chapter_id = match.group(1)
                else:
                    if "/chapter/" in url:
                        web_chapter_id = url.split("/chapter/")[1].split("/")[0]
            except Exception as e:
                safe_print(f"      ‚ö†Ô∏è L·ªói khi l·∫•y web_chapter_id: {e}")
            
            # 7. Ki·ªÉm tra ƒë√£ c√≥ ch∆∞a
            if web_chapter_id and self.mongo.is_chapter_scraped(web_chapter_id):
                safe_print(f"      ‚è≠Ô∏è  B·ªè qua (ƒê√£ t·ªìn t·∫°i): {web_chapter_id}")
                return None
            
            chapter_id = generate_id()
            
            # 8. L∆∞u Content
            if content and chapter_id:
                if not self.mongo.is_chapter_content_scraped(chapter_id):
                    content_id = generate_id()
                    self.mongo.save_chapter_content(content_id, content, chapter_id)
            
            # 9. Scrape comments (d√πng page hi·ªán t·∫°i)
            total_comments = 0
            try:
                comments_list = self.comment_handler.scrape_comments_worker(page, url, "chapter", chapter_id)
                total_comments = len(comments_list) if comments_list else 0
            except Exception as e:
                safe_print(f"      ‚ö†Ô∏è L·ªói khi scrape comments: {e}")
            
            # 10. L∆∞u Info
            chapter_data = {
                "id": chapter_id,
                "web_chapter_id": web_chapter_id,
                "order": order,
                "name": title,
                "url": url,
                "published_time": published_time,
                "story_id": story_id,
                "voted": "",
                "views": "",
                "total_comments": str(total_comments)
            }
            
            self.mongo.save_chapter(chapter_data)
            safe_print(f"      ‚úÖ ƒê√£ c√†o ch∆∞∆°ng {index + 1} b·∫±ng Browser ch√≠nh!")
            
            return chapter_data
            
        except Exception as e:
            safe_print(f"‚ùå L·ªói c√†o ch∆∞∆°ng {index + 1}: {e}")
            return None
    
    def scrape_single_chapter_with_requests(self, url, index, story_id, order, published_time_from_table, session=None):
        """
        ‚úÖ C√ÅCH 5: Scrape chapter b·∫±ng requests (kh√¥ng d√πng Playwright)
        ‚Üí Kh√¥ng b·ªã detect nh∆∞ bot headless
        ‚Üí Nhanh h∆°n, ·ªïn ƒë·ªãnh h∆°n
        
        Args:
            url: URL c·ªßa ch∆∞∆°ng c·∫ßn c√†o
            index: Th·ª© t·ª± ch∆∞∆°ng trong list
            story_id: ID c·ªßa story (FK)
            order: S·ªë th·ª© t·ª± c·ªßa chapter (t·ª´ 1)
            published_time_from_table: published_time l·∫•y t·ª´ table row
            session: requests.Session (n·∫øu None th√¨ t·∫°o m·ªõi t·ª´ context)
        """
        try:
            safe_print(f"    üìÑ ƒêang c√†o ch∆∞∆°ng {index + 1} b·∫±ng requests...")
            
            # ‚úÖ C√ÅCH 4: Random delay nh∆∞ ng∆∞·ªùi th·∫≠t
            delay = random.uniform(2.5, 6.0)  # Random 2.5-6 gi√¢y
            time.sleep(delay)
            
            # T·∫°o session t·ª´ context n·∫øu ch∆∞a c√≥
            if session is None and self.context:
                # L·∫•y user_agent t·ª´ context options (kh√¥ng ph·∫£i dict)
                user_agent = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
                session = get_session_from_context(self.context, user_agent)
            
            if session is None:
                safe_print(f"      ‚ö†Ô∏è Kh√¥ng c√≥ session, d√πng Playwright fallback...")
                return self.scrape_single_chapter_worker(url, index, story_id, order, published_time_from_table)
            
            # Scrape b·∫±ng requests
            chapter_data = scrape_chapter_with_requests(session, url)
            
            if not chapter_data:
                safe_print(f"      ‚ö†Ô∏è Requests failed, d√πng Playwright fallback...")
                return self.scrape_single_chapter_worker(url, index, story_id, order, published_time_from_table)
            
            # L·∫•y web_chapter_id t·ª´ URL
            web_chapter_id = ""
            try:
                match = re.search(r'/chapter/(\d+)', url)
                if match:
                    web_chapter_id = match.group(1)
                else:
                    url_parts = url.split("/chapter/")
                    if len(url_parts) > 1:
                        web_chapter_id = url_parts[1].split("/")[0]
            except Exception as e:
                safe_print(f"      ‚ö†Ô∏è L·ªói khi l·∫•y web_chapter_id: {e}")
            
            # Ki·ªÉm tra ƒë√£ c√≥ ch∆∞a
            if web_chapter_id and self.mongo.is_chapter_scraped(web_chapter_id):
                safe_print(f"      ‚è≠Ô∏è  B·ªè qua chapter {web_chapter_id} (ƒë√£ c√≥ trong DB)")
                return None
            
            chapter_id = generate_id()
            title = chapter_data.get('title', '')
            content = chapter_data.get('content', '')
            published_time = published_time_from_table or chapter_data.get('published_time', '')
            
            # L∆∞u content
            if content and chapter_id:
                if not self.mongo.is_chapter_content_scraped(chapter_id):
                    content_id = generate_id()
                    self.mongo.save_chapter_content(content_id, content, chapter_id)
            
            # Comments - v·∫´n c·∫ßn Playwright cho comments (c√≥ th·ªÉ c·∫£i thi·ªán sau)
            # T·∫°m th·ªùi b·ªè qua comments khi d√πng requests
            total_comments = 0
            
            # L·∫•y views v√† voted t·ª´ requests (c·∫ßn parse HTML)
            views = ""
            voted = ""
            try:
                from bs4 import BeautifulSoup
                soup = BeautifulSoup(session.get(url).text, 'html.parser')
                stats_container = soup.select_one(".chapter_stats")
                if stats_container:
                    stats_items = stats_container.select(".chp_stats_feature")
                    for item in stats_items:
                        icon = item.select_one("i")
                        if icon and "fa-eye" in icon.get("class", []):
                            text = item.get_text(strip=True)
                            numbers = re.findall(r'\d+', text)
                            if numbers:
                                views = numbers[0]
                        elif icon and "fa-heart" in icon.get("class", []):
                            heart_cnt = item.select_one("#heart_cnt")
                            if heart_cnt:
                                voted = heart_cnt.get_text(strip=True)
                            else:
                                text = item.get_text(strip=True)
                                numbers = re.findall(r'\d+', text)
                                if numbers:
                                    voted = numbers[0]
            except:
                pass
            
            chapter_data_dict = {
                "chapter_id": chapter_id,  # Kh√≥a ch√≠nh (kh√¥ng ph·∫£i "id")
                "web_chapter_id": web_chapter_id,
                "order": order,
                "chapter_name": title,  # Kh√¥ng ph·∫£i "name"
                "chapter_url": url,  # Kh√¥ng ph·∫£i "url"
                "published_time": published_time,
                "story_id": story_id,
                "voted": voted,
                "views": views,
                "total_comments": str(total_comments)
            }
            
            self.mongo.save_chapter(chapter_data_dict)
            safe_print(f"      ‚úÖ ƒê√£ c√†o ch∆∞∆°ng {index + 1} b·∫±ng requests!")
            
            return chapter_data_dict
            
        except Exception as e:
            safe_print(f"      ‚ö†Ô∏è L·ªói khi scrape chapter b·∫±ng requests: {e}")
            # Fallback v·ªÅ Playwright
            return self.scrape_single_chapter_worker(url, index, story_id, order, published_time_from_table)
    
    def scrape_single_chapter_worker(self, url, index, story_id, order, published_time_from_table):
        """
        Worker function ƒë·ªÉ c√†o M·ªòT ch∆∞∆°ng b·∫±ng Playwright (fallback)
        """
        worker_playwright = None
        worker_browser = None
        
        try:
            # ‚úÖ C√ÅCH 4: Random delay
            delay = random.uniform(2.5, 6.0)
            time.sleep(delay)
            
            worker_playwright = sync_playwright().start()
            worker_browser = worker_playwright.chromium.launch(headless=config.HEADLESS)
            worker_context = worker_browser.new_context()
            worker_page = worker_context.new_page()
            
            safe_print(f"    üîÑ Thread-{index}: ƒêang c√†o ch∆∞∆°ng {index + 1} (Playwright fallback)")
            
            worker_page.goto(url, timeout=config.TIMEOUT, wait_until="domcontentloaded")
            
            # ‚úÖ C√ÅCH 4: Random delay
            time.sleep(random.uniform(1.0, 3.0))
            
            # ‚úÖ L·∫•y t·ª´ div.chp_raw (gi·ªØ ƒë√∫ng format nh∆∞ UI)
            worker_page.wait_for_selector("#chp_raw, .chp_raw", timeout=15000)
            
            title = worker_page.locator("h1").first.inner_text()
            
            published_time = published_time_from_table
            if not published_time:
                try:
                    time_elem = worker_page.locator("time[datetime]").first
                    if time_elem.count() > 0:
                        published_time = time_elem.get_attribute("datetime") or ""
                except:
                    pass
            
            content = ""
            try:
                # ‚úÖ L·∫•y t·ª´ div.chp_raw (c√≥ nhi·ªÅu th·∫ª p, gi·ªØ ƒë√∫ng format nh∆∞ UI)
                content_container = worker_page.locator("#chp_raw, .chp_raw").first
                if content_container.count() > 0:
                    html_content = content_container.inner_html()
                    content = convert_html_to_formatted_text(html_content)
                else:
                    # Fallback: Th·ª≠ .chapter-inner
                    try:
                        content_container = worker_page.locator(".chapter-inner").first
                        if content_container.count() > 0:
                            html_content = content_container.inner_html()
                            content = convert_html_to_formatted_text(html_content)
                        else:
                            content = worker_page.locator("body").inner_text()
                    except:
                        content = worker_page.locator("body").inner_text()
            except Exception as e:
                safe_print(f"      ‚ö†Ô∏è Thread-{index}: L·ªói khi l·∫•y content: {e}")
                try:
                    content = worker_page.locator("#chp_raw, .chp_raw").inner_text()
                except:
                    content = worker_page.locator("body").inner_text()
            
            # ‚úÖ C√ÅCH 4: Random delay
            time.sleep(random.uniform(1.0, 3.0))
            
            # L·∫•y web_chapter_id t·ª´ URL
            web_chapter_id = ""
            try:
                match = re.search(r'/chapter/(\d+)', url)
                if match:
                    web_chapter_id = match.group(1)
                else:
                    url_parts = url.split("/chapter/")
                    if len(url_parts) > 1:
                        web_chapter_id = url_parts[1].split("/")[0]
            except Exception as e:
                safe_print(f"      ‚ö†Ô∏è Thread-{index}: L·ªói khi l·∫•y web_chapter_id t·ª´ URL: {e}")
                web_chapter_id = ""
            
            if web_chapter_id and self.mongo.is_chapter_scraped(web_chapter_id):
                safe_print(f"      ‚è≠Ô∏è  Thread-{index}: B·ªè qua chapter {web_chapter_id} (ƒë√£ c√≥ trong DB)")
                return None
            
            chapter_id = generate_id()
            
            comments_list = self.comment_handler.scrape_comments_worker(worker_page, url, "chapter", chapter_id)
            total_comments = len(comments_list) if comments_list else 0
            
            if content and chapter_id:
                if not self.mongo.is_chapter_content_scraped(chapter_id):
                    content_id = generate_id()
                    self.mongo.save_chapter_content(content_id, content, chapter_id)
            
            chapter_data = {
                "id": chapter_id,
                "web_chapter_id": web_chapter_id,
                "order": order,
                "name": title,
                "url": url,
                "published_time": published_time,
                "story_id": story_id,
                "voted": "",
                "views": "",
                "total_comments": str(total_comments)
            }
            
            self.mongo.save_chapter(chapter_data)
            
            return chapter_data
            
        except Exception as e:
            safe_print(f"‚ö†Ô∏è Thread-{index}: L·ªói c√†o ch∆∞∆°ng {index + 1}: {e}")
            return None
        finally:
            if worker_browser:
                worker_browser.close()
            if worker_playwright:
                worker_playwright.stop()
